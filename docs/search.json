[
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Python for Geoscience",
    "section": "",
    "text": "Objectives\n\nUnderstand the Python ecosystem and other tools.\n\n\nThis course we will introduce you to foundations of Python programming. We will utilise common geosicence data types (geospatial, temporal, vector, raster, etc) to demonstrate a variety of practical workflows and showcase fundamental capabilities of Python. We will carry out exploratory, analytical, computational and machine learning analyses on these datasets. At the end of the course you will be able to adapt these workflows to your own datasets.\nThe course is presented by the Sydney Informatics Hub on behalf of the Petroleum Exploration Society of Australia.\n\nWhat is Python?\nThe Python programming language was written in the 1980’s. It is an interpreted and high-level language, this makes it easy to use for rapid development with lots of flexibility. Version 2.0 was released in 2000. Version 3.0 was released in 2008 (the current version is 3.10.0).\nI use it everyday to:\n\nAutomate tasks (do things millions of times easily)\nCalculate big numbers (can solve most computational problems)\nFormat and analyse data\nProcess images\n\nThere are many comparable languages (e.g. R, Julia, C++, bash, Matlab, Java). But there are a few reasons to favour Python:\n\nPython is free and open-source.\nThere is a large community of people using it all over the world on different projects, which means there is a lot of help and documentation.\nThere are millions of codes, packages, libraries, and extensions. Some that leverage other programming languages to make your Python tasks fast, efficient and capable of doing whatever you need it to.\n\n\n\nHow do we use Python? Terminals and Notebooks.\nTraditionally one writes a “Python script file”, like a recipe of instructions, and Python executes the script.\nSimply, you can create Python files in a text editor:\nprint(\"Hello World\")\nSave this as hello.py and execute it with python hello.py.\nAs you go deeper into Python, you will see more advanced syntax:\n#!/usr/bin/env python\n\n'''\nA Python program which greets the Earth!\nNathaniel Butterworth\nSIH Python course\n\nusage: python helloworld_advanced.py\n'''\n\ndef main():\n    print(\"Hello World! This is basically the same result, but a different way to get there.\")\n\nif __name__ == '__main__':\n    main()\nOnce again, you can save this in a text editor as hello_advanced.py and execute with python hello_advanced.py.\nYou can also start a Python IDE session, and execute commands one by one.\nA handy tool is the Jupyter Notebook (modelled from Mathematica’s Notebooks), that we will predominately be using throughout this course. They are good for the kind of non-development focused Python tasks you may need.\nThere are also online environments that can host Python code and notebooks for you.\nThroughout the course you will see when and why to use different environments.\nNow let’s get into in the practical session!\n\nKey points\n\nPython is a programming language.\nThere is a rich ecosystem of tools around creating and deploying Python code.\n\n\n\n\n\n\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "JupyterhubOnAWS.html",
    "href": "JupyterhubOnAWS.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "The first steps are adapted from TLJH instuctions here\n\nSelect a Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami- instance (the one maintained by Canonical as per the TLJH screenshots in the documentation)\nChoose a t3.small node type for testing - it’s the smallest that supports TLJH. Note that this does not have a free tier, but running it for a few hours costs approximately $0.10 in my experience.\nFor “production” PESA try m5a.8xlarge in US-Ohio (this gives 32 vCPU and 128 Gb RAM).\n\nIn theory, you are meant to be able to use a custom command to install TLJH in one go, and paste it into the user script box. Unfortunately, none of the flags have worked for me, and the installation script itself sometimes works and sometimes doesn’t - so I recommend doing this over the ssh after the instance has been created.\n#!/bin/bash\ncurl -L https://tljh.jupyter.org/bootstrap.py \\\n  | sudo python3 - \\\n    --admin myusername --show-progress-page --plugin tljh-repo2docker # can also in theory use a requirements.txt file here\n\nWhen selecting storage, request General Purpose SSD (gp2) for most workloads; according to TLJH Provisioned IOPS SSD (io1) is the highest-performant when performance is critical. I have only used gp2 so far.\n[Only the first time you set up] After downloading the AWS .pem file, make sure to chmod 600 it as otherwise you get a permissions error.\n\nchmod 600 ~/Desktop/pesaaws1.pem\n\n\n\n\n\n\nssh into the server via the Public IPv4 DNS provided in the instance details. ubuntu is the admin username for the root ubuntu user, and you’ll need root privileges for the below\n\nssh -i ~/Desktop/pesaaws1.pem ubuntu@ec2-18-220-114-100.us-east-2.compute.amazonaws.com\n\n\n\nimage-20210423141931861\n\n\n\nInstall TLJH using the following command\n\ncurl -L https://tljh.jupyter.org/bootstrap.py \\\n  | sudo python3 - \\\n    --admin myusername #--user-requirements-txt-url https://raw.githubusercontent.com/data-8/materials-sp18/master/requirements.txt\n\n\n\n\n\n\nCreate a folder as recommended in these instructions\nsudo mkdir -p /srv/\n# you may need to chmod it but I can't figure out the optimal permissions\n# sudo chmod 777 data\nOpen a terminal on your local machine and upload the data to the AWS machine with the following command:\nsudo scp -r -i ~/Desktop/pesaaws1.pem * ubuntu@ec2-3-141-195-91.us-east-2.compute.amazonaws.com:/srv/data/\nTo grab from cloudstor (recommended):\ncd /srv/\nsudo wget https://cloudstor.aarnet.edu.au/plus/s/62la9C8dn5vjAaa/download\nsudo apt-get install -y unzip\nsudo unzip download\nThen create a soft link to the data directory with the following command.\nsudo ln -s /srv/geopython/data /etc/skel/data\nsudo ln -s /srv/geopython/notebooks /etc/skel/notebooks\nIt should now be visible for any new user created.\nThe geopython workshop also requires graphviz\nsudo apt install graphviz\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "JupyterhubOnAWS.html#installing-packages",
    "href": "JupyterhubOnAWS.html#installing-packages",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Installing packages",
    "text": "Installing packages\nInstall the packages you need into the base environment.\nWe use the Anaconda package manager for our training, and share installation instructions using a conda environment.yml file. If you instead have a pip requirements.txt file, you could have used the --user-requirements-txt-url flag at the end of the TLJH install above to install all of the packages in one go.\nModify the base conda environment for all users:\nexport PATH=/opt/tljh/user/bin:${PATH}\nsudo -E conda env update -n base -f /srv/geopython/environment.yml"
  },
  {
    "objectID": "JupyterhubOnAWS.html#adding-users",
    "href": "JupyterhubOnAWS.html#adding-users",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Adding users",
    "text": "Adding users\nLaunch the jupyterhub by following the Public IPv4 address indicated for the EC2 instance, changing the https: at the beginning of the url to http:\nIgnoring the unsecurity warnings, log in using the administrator login and a random password (make sure to write this down) to see the jupyter hub.\nGo to the Control Panel -> Admin interface and click the Add Users button.\nManually paste usernames you’d like into the box.\n\nNotes on getting our specific notebooks to work\n#Import dask dataframe modules\nimport dask.dataframe as dd\n#NOTE: to run this example (with diagrams) you will need to \"pip install graphviz\" and donwload graphviz\n#https://graphviz.org/download/\nimport os\n# Nate has the below; replace\n#os.environ[\"PATH\"] += os.pathsep + 'C:/APPS/Graphviz/bin'\nos.chdir('/home/jupyter-user8/')\n# may need to replace data file to dd.read_csv(\"data/ml_data_points.csv\")\nNote: mpi example at end of notebook 4 will not work\n\n\nUsing cloudwatch agent to monitor usage\nNeeded to create an IAM role as describe here once"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python for Geoscience",
    "section": "",
    "text": "This course is aimed at researchers, students, and industry professionals who want to learn about the capabilities of Python and get experience using it applied to real-world problems. This course will introduce you to foundations of Python programming. We will utilise common geoscience data types (geospatial, temporal, vector, raster, etc) to demonstrate a variety of practical workflows and showcase fundamental capabilities of Python. We will carry out exploratory, analytical, computational and machine learning analyses on these datasets. At the end of the course you will be able to adapt these workflows to your own datasets.\nThe course is presented by the Sydney Informatics Hub on behalf of the Petroleum Exploration Society of Australia.\nThe Sydney Informatics Hub (SIH) is a Core Research Facility of the University of Sydney. Core Research Facilities centralise essential research equipment and services that would otherwise be too expensive or impractical for individual Faculties to purchase and maintain. We provide a wide range of research services to aid investigators, such as:\nWe also aim to cultivate a data community, organising monthly Hacky Hours, outside training events (eg NVIDIA, Pawsey Center), and data/coding-related events. Look out for everything happening on our calendar or contact us (at sih.info@sydney.edu.au) to get some digital collaboration going.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Introduction to Python for Geoscience",
    "section": "Trainers",
    "text": "Trainers\n\nNathaniel (Nate) Butterworth (PhD Computational Geophysics), nathaniel.butterworth@sydney.edu.au\nDarya Vanichkina (PhD Bioinformatics, SFHEA)\nKristian Maras (MSc Quant Fin)\nTim White (PhD Asteroseismology)"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Introduction to Python for Geoscience",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nNo previous programming experience is required, but Session 1 is a pre-requisite for the other sessions. Training will be delivered online, so you will need access to a modern computer with a stable internet connection and around 5GB of storage space for data downloaded prior to the course. Participants are encouraged to setup a Python environment on their local computer (as per the Setup Instructions provided), but participation using other platforms/environments can be supported where necessary."
  },
  {
    "objectID": "index.html#venue-online-via-zoom",
    "href": "index.html#venue-online-via-zoom",
    "title": "Introduction to Python for Geoscience",
    "section": "Venue, online via Zoom",
    "text": "Venue, online via Zoom\nParticipants will be provided with a Zoom link. Trainers will be broadcasting from Sydney.\n\nZoom etiquette and how we interact\nSessions will be recorded for attendees only, and it is set up to only record the host shared screen and host audio. We will try and get these uploaded to this site as soon as possible. Please interrupt whenever you want! Ideally, have your camera on and interact as much as possible. There will be someone monitoring the chat-window with any questions you would like to post there. Four hours is a long Zoom session so we have plenty of scheduled breaks combined with a mix of content to be delivered as demos, plus sections as independent exercises, but most of the course will be pretty-hands on with everyone writing their own code. We will use Zoom break-out rooms as needed with the Trainers and participants."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Introduction to Python for Geoscience",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available at https://pages.github.sydney.edu.au/informatics/sih_codeofconduct/"
  },
  {
    "objectID": "index.html#general-session-timings",
    "href": "index.html#general-session-timings",
    "title": "Introduction to Python for Geoscience",
    "section": "General session timings",
    "text": "General session timings\n\nStart at 12:35pm promptly.\n5 min to discuss feedback and summarise previous week.\n1 hour of content.\n10 min break.\n1 hour content\n10 min break\n1 hour content\n10 min break\nUntil 16:25pm summary + hack session until close. This will be an opportunity to get one-on-one support, or we will deviate from lesson plans if interesting questions arise. If there are particular problems or datasets you would be interested in solving/exploring, email us in advanced and we can either do it as a group (if appropriate) or break out into smaller groups."
  },
  {
    "objectID": "index.html#date-time-in-aest-brisbane-time",
    "href": "index.html#date-time-in-aest-brisbane-time",
    "title": "Introduction to Python for Geoscience",
    "section": "Date & Time (in AEST-Brisbane time):",
    "text": "Date & Time (in AEST-Brisbane time):\nWe will be working through the following general content over our 4 sessions together.\nSession 01 Python fundamentals Tues Aug 9, 12:30 pm - 4:30 pm (AEST)\nSession 02 Specialist Python libraries and data analysis for geoscience Tues Aug 16, 12:30 pm - 4:30 pm (AEST)\nSession 03 Pattern recognition and prediction in geoscience Tues Aug 23, 12:30 pm - 4:30 pm (AEST)\nSession 04 Large data and long running workflow strategies Tues Aug 30, 12:30 pm - 4:30 pm (AEST)"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Introduction to Python for Geoscience",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "We generally use and recommend the Miniconda Python distribution: https://docs.conda.io/en/latest/miniconda.html. But feel free to use whatever one works for you (and the course materials). We will be using Miniconda3-py39_4.11.0.\nYou can get this specific version here for:\n\nWindows 64 bit Download\nMac OSX Download\nLinux Download\n\nFollow the prompts (the default recommendations in the installer are generally fine.) Once installed, launch an “Anaconda Prompt” from the Start Menu / Applications Folder to begin your Python adventure.\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html#launching-the-jupyterpython-notebook",
    "href": "setup.html#launching-the-jupyterpython-notebook",
    "title": "Setup",
    "section": "Launching the Jupyter/Python Notebook",
    "text": "Launching the Jupyter/Python Notebook\nNow you have built your environment with all the packages we need, you can launch it. We will be working mostly with Python Notebooks to run Python (as opposed to running an interpreter on the command line/prompt). Each time you restart your work you will have to follow these steps:\n\nLaunch an Anaconda Prompt (or equivalent).\nChange directories to your workspace.\nActivate the geopy environment.\nLaunch the Jupyter/Python Notebook server.\n\ncd C:\\Users\\nbutter\\Desktop\\geopython\nconda activate geopy\njupyter notebook\n\nThis will launch the Notebook server (and may automatically launch a web-browser and take you to the page). If the Notebook does not automatically start, copy the generated link into a browser."
  },
  {
    "objectID": "setup.html#jupyter-hub-in-the-cloud",
    "href": "setup.html#jupyter-hub-in-the-cloud",
    "title": "Setup",
    "section": "Jupyter Hub in the Cloud",
    "text": "Jupyter Hub in the Cloud\nIf the above options do not work for you, you can use an on-demand cloud instance. You will be given a web link, login with provided credentials. Done."
  },
  {
    "objectID": "setup.html#docker",
    "href": "setup.html#docker",
    "title": "Setup",
    "section": "Docker",
    "text": "Docker\nIf you are familiar with Docker you may use our Docker image with something like:\nsudo docker run -it -p 8888:8888 nbutter/geopy:pesa2022 /bin/bash -c \"jupyter notebook --allow-root --ip=0.0.0.0 --no-browser\"\nThis will launch the Python notebook server in the /notebooks folder. Access the notebook by entering the generated link in a web-browser, e.g. http://127.0.0.1:8888/?token=9b16287ab91dc69d6b265e6c9c31a49586a35291bb20d0ab"
  },
  {
    "objectID": "notebooks/01b-dataframes.html",
    "href": "notebooks/01b-dataframes.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "What are libraries and packages?\nHow can I load tabular data into Python?\nHow can I load shapefiles?\nHow can I load segy and las data?\n\n\n\n\n\nLearn how to deal with specialty data types.\nLearn about pandas, pyshp, lasio, obspy.\n\n\nPython can deal with basically any type of data you throw at it. The open source python community has developed many packages that make things easy. Today we will look at pyshp (for dealing with shapefiles), pandas (great for tables and time series), lasio (for las format well log data) and obspy (a highly featured seismic data processing suite) packages.\nData for this exercised was downloaded from http://www.bom.gov.au/water/groundwater/explorer/map.shtml\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01b-dataframes.html#western-seismic-velf-format",
    "href": "notebooks/01b-dataframes.html#western-seismic-velf-format",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Western seismic VELF format",
    "text": "Western seismic VELF format\nSometimes there are no good libraries or data-readers availble for your specific use-case. Very common if you get some specialty instrument with some unique data format. Often the documentation is a good place to start for figuring out how to interpret the data, and more-often-than not, the header of the data can give you all the information you need. Download this VELF file, containing some 3D seismic data. I could not find any good python libraries to handle this dataset, so we can just try out a few things to get the data into a format that is useful for us.\n\n#Imports for plotting\nimport matplotlib.pyplot as plt\n# Load Builtin colormaps, colormap handling utilities, and the ScalarMappable mixin.\nfrom matplotlib import cm\nimport pandas as pd\n\n\n#Open the file for reading\nf = open(\"../data/S3D_Vrms_StkVels_VELF.txt\",'r')\n\n#Read in all the lines in the file and save them to a variable\nmylist = f.readlines()\n\n#Close the file\nf.close()\nprint(\"Done reading file.\")\n\nDone reading file.\n\n\n\n#Print out the first 20 lines\nprint(mylist[0:20])\n\n['Client: XXX \\n', 'Project: YYY\\n', 'Contractor: ZZZ\\n', 'Date:\\n', '\\n', 'Velocity type: RMS Velocity in Time\\n', '\\n', '\\n', 'Datum: GDA94, UTM Zone: UTM53, Central Meridian :  \\n', 'Statics: Two way time corrected to mean sea level: No\\n', '         Gun and Cable statics applied: No\\n', '         Tidal statics applied: No\\n', '\\n', '3D Grid details:\\n', 'inline    crossline      X            Y\\n', '\\n', '1000        5000      599413.78   7382223.37\\n', '1000        5309      595633.30   7375486.63\\n', '1448        5000      609180.96   7376742.28\\n', '1448        5309      605400.48   7370005.55\\n']\n\n\n\n#Set up some empty lists to store each bit of data in\ninline=[]\ncrossline=[]\nX=[]\nY=[]\n\nvelf=[]\n\n#Loop through all the lines in the file\nfor i,line in enumerate(mylist):\n    \n    #First split the line up (by default, split by whitespace)\n    splitline=line.split()\n    \n    #If we encounter certain lines, save some data\n    if i in [16,17,18,19]:  \n        #Print out the lines (check we are doing the right thing)\n        print(splitline)\n        \n        inline.append(int(splitline[0]))\n        crossline.append(int(splitline[1]))\n        X.append(float(splitline[2]))\n        Y.append(float(splitline[3]))\n      \n\n    #This is where the actual data starts\n    #Now depending on the key word at the start of each line\n    #save the data to each particular list/array\n    #Read the data in again, this time with some thought about what we actually want to do\n    if i>49:\n        if splitline[0]=='LINE':\n            LINE = int(splitline[1])\n            \n        if splitline[0]=='SPNT':\n            xline3d=int(splitline[1])\n            binx=float(splitline[2])\n            biny=float(splitline[3])\n            inline3d=int(splitline[4])\n            \n        if splitline[0]=='VELF':\n            \n            for j,val in enumerate(splitline[1:]):\n                #print(j,val)\n                #Counting from the 0th index of splitline[1:end]\n                if j%2==0:\n                    t=int(val)\n                else:\n                    vt=int(val)\n                    velf.append([LINE,xline3d,binx,biny,inline3d,t,vt]) \n\n['1000', '5000', '599413.78', '7382223.37']\n['1000', '5309', '595633.30', '7375486.63']\n['1448', '5000', '609180.96', '7376742.28']\n['1448', '5309', '605400.48', '7370005.55']\n\n\n\n#Convert the python \"list\" type to Pandas dataframe\ndf=pd.DataFrame(velf)\n#Set the names of the columns\ndf.columns=['LINE','xline3d','binx','biny','inline3d','t','vt']\n\ndf\n\n\n\n\n  \n    \n      \n      LINE\n      xline3d\n      binx\n      biny\n      inline3d\n      t\n      vt\n    \n  \n  \n    \n      0\n      1000\n      5080\n      598435.0\n      7380479.0\n      1000\n      0\n      3200\n    \n    \n      1\n      1000\n      5080\n      598435.0\n      7380479.0\n      1000\n      295\n      3300\n    \n    \n      2\n      1000\n      5080\n      598435.0\n      7380479.0\n      1000\n      598\n      4137\n    \n    \n      3\n      1000\n      5080\n      598435.0\n      7380479.0\n      1000\n      738\n      4537\n    \n    \n      4\n      1000\n      5080\n      598435.0\n      7380479.0\n      1000\n      1152\n      4500\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3082\n      1440\n      5280\n      605580.0\n      7370735.0\n      1440\n      2216\n      5259\n    \n    \n      3083\n      1440\n      5280\n      605580.0\n      7370735.0\n      1440\n      2861\n      5791\n    \n    \n      3084\n      1440\n      5280\n      605580.0\n      7370735.0\n      1440\n      3526\n      6294\n    \n    \n      3085\n      1440\n      5280\n      605580.0\n      7370735.0\n      1440\n      4697\n      7077\n    \n    \n      3086\n      1440\n      5280\n      605580.0\n      7370735.0\n      1440\n      5988\n      7748\n    \n  \n\n3087 rows × 7 columns\n\n\n\n\n#Plot the target area\nplt.scatter(df.binx,df.biny,c=df.xline3d)\nplt.colorbar()\nplt.show()\n\n#Plot it in 3d just becasue we can\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(df.binx,df.biny,df.inline3d,c=df.xline3d)\nax.view_init(30, 70)\nplt.show()\n\n\n\n\n\n\n\n\n#Now, make some plots...\n#One way to do this, is to\n#make a 'group' for each unique seismic line\ngroups=df.groupby(['LINE','xline3d','inline3d'])\n\n\n#Make plots by certain groupings\n\n#Add a value to spread out the data nicely\ni=0\nfor name,grp in groups:\n    \n    if name[2]==1280:\n        print(name)\n        plt.plot(grp.vt+i,-grp.t)\n        i+=500\n\n(1280, 5040, 1280)\n(1280, 5060, 1280)\n(1280, 5080, 1280)\n(1280, 5100, 1280)\n(1280, 5120, 1280)\n(1280, 5140, 1280)\n(1280, 5160, 1280)\n(1280, 5180, 1280)\n(1280, 5200, 1280)\n(1280, 5220, 1280)\n(1280, 5240, 1280)\n(1280, 5260, 1280)\n(1280, 5280, 1280)\n\n\n\n\n\n\nfrom scipy.interpolate import interp1d\nimport numpy as np\n\n\n#Normal plots\n%matplotlib inline\n\n#Fancy intereactive plots\n#%matplotlib notebook\n\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\nfor name,grp in groups:\n\n    ##Plot all the data\n    ax.plot(grp.binx+grp.vt,grp.biny,grp.t,'b-')\n    \n    ##Plot all the data with colors\n#     colors=cm.seismic(grp.vt/grp.vt.max())\n#     ax.scatter(grp.binx+grp.vt,grp.biny,grp.t,c=grp.vt/grp.vt.max(),cmap=cm.seismic)\n    \n    #Interpolate the data and plot with colors\n#     x = grp.t\n#     y = grp.vt\n#     f = interp1d(x, y, kind='linear')\n\n#     num=50\n#     xnew = np.linspace(0,max(x),num)\n#     ynew = f(xnew)\n#     binx = np.linspace(min(grp.binx),max(grp.binx),num)\n#     biny = np.linspace(min(grp.biny),max(grp.biny),num)\n#     colours = cm.seismic(ynew/grp.vt.max())\n\n#     ax.scatter(binx+xnew,biny,xnew,c=colours)\n    \nplt.show()"
  },
  {
    "objectID": "notebooks/01b-dataframes.html#bonus-convert-text-to-segy-format",
    "href": "notebooks/01b-dataframes.html#bonus-convert-text-to-segy-format",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Bonus: Convert Text to SegY format",
    "text": "Bonus: Convert Text to SegY format\nA work in progress… but the strategy is to simply match up the correct SEGY values with those in the text file. Grouping everything by each x-y (i-j, lat-lon) surface point, and then the z-axis will be the trace (down to a depth or a TWT etc).\n\nfrom obspy.core.stream import Stream, Trace\nfrom obspy.core.utcdatetime import UTCDateTime\nfrom obspy.io.segy.segy import SEGYTraceHeader\nfrom obspy.core.util.attribdict import AttribDict\nfrom obspy.io.segy.segy import SEGYBinaryFileHeader\nimport sys\n\n# Group all the text traces by their the i-j coordinates\ngroups=df.groupby(['LINE','xline3d'])\nprint(len(groups))\n\n#Make a stream object (flush it out to begin because we have used this variable names for demos)\nstream_out = None \nstream_out = Stream()\n\n#not sure how to group the trace ensembles but can use a counter to keep track of them\nensemble_number = 0\n       \nfor ids,df_trace in groups:\n    #ids are the LINE, xline3d coordinate locations\n    #trc is the subset of the full dataframe for just that i-j location\n\n    #For each LINE-xline3d location, a trace is impdence at all the depth values, i.e.\n    data = df_trace.vt.values\n\n    # Enforce correct byte number and set to the Trace object\n    data = np.require(data, dtype=np.float32)\n    trace = Trace(data=data)\n\n    # Set all the segy header information\n    # Attributes in trace.stats will overwrite everything in trace.stats.segy.trace_header\n    trace.stats.delta = 0.01\n    trace.stats.starttime = UTCDateTime(1970,1,1,0,0,0)\n\n    # If you want to set some additional attributes in the trace header,\n    # add one and only set the attributes you want to be set. Otherwise the\n    # header will be created for you with default values.\n    if not hasattr(trace.stats, 'segy.trace_header'):\n        trace.stats.segy = {}\n\n    trace.stats.segy.trace_header = SEGYTraceHeader()\n\n#         trace.stats.segy.trace_header.trace_sequence_number_within_line = index + 1\n#         trace.stats.segy.trace_header.receiver_group_elevation = 0\n    trace.stats.segy.trace_header.source_coordinate_x = int(df_trace.binx.values[0])\n    trace.stats.segy.trace_header.source_coordinate_y = int(df_trace.biny.values[0])\n    trace.stats.segy.trace_header.ensemble_number = ensemble_number #Not sure how this is actually determined\n    trace.stats.segy.trace_header.lag_time_A = 2400\n    trace.stats.segy.trace_header.lag_time_B = 3000\n    trace.stats.segy.trace_header.number_of_samples_in_this_trace = len(data)\n\n    ensemble_number +=1\n\n    # Add trace to stream\n    stream_out.append(trace)\n\n# A SEGY file has file wide headers. This can be attached to the stream\n# object.  If these are not set, they will be autocreated with default\n# values.\nstream_out.stats = AttribDict()\nstream_out.stats.textual_file_header = 'Textual Header!'\nstream_out.stats.binary_file_header = SEGYBinaryFileHeader()\nstream_out.stats.binary_file_header.trace_sorting_code = 5\n# stream.stats.binary_file_header.number_of_data_traces_per_ensemble=1\n\nprint(\"Stream object before writing...\")\nprint(stream_out)\n\nstream_out.write(\"TEST.sgy\", format=\"SEGY\", data_encoding=1, byteorder=sys.byteorder)\n\nprint(\"Stream object after writing. Will have some segy attributes...\")\nprint(stream_out)\n\n217\nStream object before writing...\n217 Trace(s) in Stream:\n\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.060000Z | 100.0 Hz, 7 samples\n...\n(215 other traces)\n...\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.100000Z | 100.0 Hz, 11 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\nStream object after writing. Will have some segy attributes...\n217 Trace(s) in Stream:\n\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.060000Z | 100.0 Hz, 7 samples\n...\n(215 other traces)\n...\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.100000Z | 100.0 Hz, 11 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]"
  },
  {
    "objectID": "notebooks/merge.html",
    "href": "notebooks/merge.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01a-fundamentals.html",
    "href": "notebooks/01a-fundamentals.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Dealing with data\n\n#First we have to load some modules to do the work for us.\n#Modules are packages people have written so we do not have to re-invent everything!\n\n#The first is NUMerical PYthon. A very popular matrix, math, array and data manipulation library.\nimport numpy\n\n#This is a library for making figures (originally based off Matlab plotting routines)\n#We use the alias 'plt' because we don't want to type out the whole name every time we reference it!\nimport matplotlib.pyplot as plt \n\n\n#Set the variable name for the file we are loading in. \n#It is in the 'data' directory, and the file is called EarthChemCU.txt. \n#We are currently working in /notebooks.\nfilename = '../data/EarthChemCU.txt'\n\n#Now read in the data\n# loadtxt() is a function that we can now use because we loaded the library called numpy\nchemdata=numpy.loadtxt(filename, delimiter=',')\n#chemdata <- the name of a variable we are making that will hold the table of data\n#filename <- this is the name of the variable we declared above\n#delimiter <- this is a csv file\n\n\nWant more details about a command/function we use?\n\n#Try these help commands\n#help(numpy.loadtxt)\n#?numpy.loadtxt\n\nOr really, search the function! Online documentation and discussion boards are filled with great content.\n\n\nExploring your data\nIt is often a good idea to look at the data to have some idea of what you are working with\n\n#What does the data look like. Print it out\nprint(chemdata)\n\n[[ 3.92583e+01 -1.14992e+02  1.11000e+02  1.96000e+04]\n [ 3.92583e+01 -1.14992e+02  1.11000e+02  1.57000e+04]\n [ 4.12060e+01 -1.17272e+02  1.05000e+02  3.00000e+00]\n ...\n [ 2.00530e+01  1.17419e+02  0.00000e+00  3.00000e+01]\n [ 2.00530e+01  1.17419e+02  0.00000e+00  3.30000e+01]\n [ 2.00530e+01  1.17419e+02  0.00000e+00  3.50000e+01]]\n\n\nThis data is in the style: Latitude (degrees), Longitude (degrees -180:180), Age (Ma), Copper abundance (ppm)\n\n#Print the dimensions of the data\nprint(chemdata.shape)\n\n(207431, 4)\n\n\n207431 rows!\n\n\nAccessing data from an array\nchemdata is a table of data: an array with two dimensions. So to access/look at/change parts of it, we need to specify both row and column\n\n#Print the number in the first row and third column \n#(note indexing is different in \"numpy arrays\" compared \"python lists\". \n#IMPORTANT: Python counts from 0\nprint(chemdata[0,2])\n\n111.0\n\n\n\n#Print the first row\nprint(chemdata[0,:])\n\n[   39.2583  -114.992    111.     19600.    ]\n\n\n\n#Print the third column\nprint(chemdata[:,2])\n\n[111. 111. 105. ...   0.   0.   0.]\n\n\n\n#Print the first two columns for row id 2, 5 and 6. \nprint(chemdata[[2,5,6],0:2])\n\n[[  41.206 -117.272]\n [  41.186 -117.417]\n [  41.177 -117.485]]\n\n\n\n\nChallenge\nPrint the second and third columns for row 20-30.\n\n\nSolution\n\n#The indexing counts from [start:end]\n#where \"start\" is included and \"end\" is excluded!\n#Assuming we want row 30, then you need to\n#include index 29 (i.e. set the end index to 30!)\n#Same with columns, we want column 2 (index 1) and\n#column 3 (index 2) so make our slice 1:3\n\nprint(chemdata[19:30,1:3])\n\n\n\nPlotting data\nNow to make our first plot!\n\n#Plot the lats and lons, i.e. the first column vs the second column\nplt.plot(chemdata[:,1],chemdata[:,0],'k.')\nplt.title('Copper Deposit Data')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\n\nThis does not look right… It is a messy dataset! This is not uncommon. Maybe the Lats/Lons are stored as Norhtings/Eastings for some samples? Maybe they are missing a decimal place?\nAnyway, Python is a great tool to clean things up! Let’s investigate further.\n\n#Plot the Latitudes\nplt.plot(chemdata[:,0])\nplt.ylabel('Latitude')\nplt.xlabel('Number')\nplt.show()\n\n#Plot the Longitudes\nplt.plot(chemdata[:,1],'r')\nplt.ylabel('Longitude')\nplt.xlabel('Number')\nplt.show()\n\n\n\n\n\n\n\nThis kind of casual data interrogation is a really handy way to exploring your data. There are definitely some outliers with latitudes and longitudes. There are quite a few ways clean the data, but let’s simply restrict our data range to -180:180 and -90:90.\n\n#Clean up the data, remove anything outside lat lon extent\n\n#Find all the \"chemdata\" column 1 (i.e. longitude) data points that are greater than -180, save it in a new variable\n#Using a succinct method in two lines\ndatamask = ((chemdata[:,0] < 90)\n            & (chemdata[:,0] > -90)\n            & (chemdata[:,1] < 180)\n            & (chemdata[:,1] > -180))\n            \ncudata4 = chemdata[datamask]\n\n\nprint(\"We have removed\", chemdata.shape[0]-cudata4.shape[0], \"samples\")\n\nWe have removed 47 samples\n\n\n\nplt.plot(cudata4[:,1],cudata4[:,0],'k.')\nplt.title('Copper Deposits from EarthChem.org')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\n\nNow make a more informative plot:\n\n#Set reasonable variable names\nlats=cudata4[:,0]\nlongs=cudata4[:,1]\nage=cudata4[:,2]\ncopper=cudata4[:,3]\n\n#lats_rich=lats[copper>2]\n\nfig = plt.figure(figsize=(6,4),dpi=150)\n\n#Restrict the colour range between 0 and 100 (ppm?)\nplt.scatter(longs,lats,s=age/1000,c=copper,vmin=0, vmax=100,cmap=plt.cm.copper)\nplt.title('Copper Deposits from EarthChem.org')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\n\n\n#You could come up with a more intelligent way to reject your outliers, e.g.\nimport numpy as np\ndef reject_outliers(data):\n    m = 2\n    u = np.mean(data)\n    print(\"mean is:\", u)\n    s = np.std(data)\n    print(\"std is:\", s)\n    filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]\n    print(\"removed:\",np.shape(data)[0] - np.shape(filtered)[0])\n    return(filtered)\n\nfiltered_age=reject_outliers(copper)\n\nmean is: 408.55060226439844\nstd is: 6032.1541529827555\nremoved: 1163\n\n\nJust plotting the Cu content implies that better filtering could be applied (a homework exercise perhaps). Remember this is a pretty messy dataset, some Cu is reported as ppm, ppb, or %!\n\nplt.plot(copper[copper>1],'k.')\nplt.show()\n\n\n\n\n\n\nLet’s make an even nicer map\n\n#Import another module called Cartopy - great for plotting things on globes\nimport cartopy.crs as ccrs\n\n#Make new variables from our array (so it is easier to see what we are doing)\nlats=cudata4[:,0]\nlongs=cudata4[:,1]\nage=cudata4[:,2]\n\n#######\n## Make the figure\n#######\n\n#Create a figure object\nfig = plt.figure(figsize=(12,8),dpi=150)\n\n#Make a map projection to plot on.\nax = plt.axes(projection=ccrs.Robinson())\n\n#Add some Earth-specific details (from the cartopy package)\nax.set_global()\nax.coastlines('50m', linewidth=0.8)\nax.stock_img()\nax.gridlines()\n\n#Make a scatter plot of the data coloured by age. \n#Restrict the colour range between 0 and 100 (Ma)\n#And also set the scatter plot as a variable 'mapscat' so we can reference it later\nmapscat=ax.scatter(longs,lats,marker=\".\",s=0.5,c=age,vmin=0,vmax=100,transform=ccrs.PlateCarree(),zorder=4,cmap=plt.cm.hsv)\n\n#Make a Colorbar\ncbar=plt.colorbar(mapscat, ax=ax, orientation=\"horizontal\", pad=0.05, fraction=0.15, shrink=0.5,extend='max')\ncbar.set_label('Age (Ma)')\n\n# Add a map title, and tell the figure to appear on screen\nplt.title('Age of Copper Deposits in the EarthChem.org database')\nplt.show()\n\n\n\n\nYou can explore the different color maps at https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html.\n\n\nChallenge\n\nPick a new element from the EarthChem data and make a similar map.\nHere is a link to download the data for indiviual elements\nCreate a new notebook and display the diagnostic steps leading up to your final map.\n\n\n\nSolution\n\n#We only need numpy and plotting libraries\nimport numpy\nimport matplotlib.pyplot as plt \nimport cartopy.crs as ccrs\n\n#Set the correct filename/filepath to where you have downloaded the data\nfilename = '../data/EarthChemAG.txt'\n\n#Add the \"skiprows\" flag, because this data has a header row\nchemdata=numpy.loadtxt(filename, delimiter=',',skiprows=1)\n    \n#Set some variable names\nlats=chemdata[:,0]\nlongs=chemdata[:,1]\nage=chemdata[:,3]\nsilver=chemdata[:,2]\n    \n#Do a quick plot\nplt.plot(longs,lats,'b.')\n\n#This set actually looks fine, no filtering necessary!\n#Just make the final plot again, with a new color bar\nplt.scatter(longs,lats,s=age/10,c=silver,vmin=0, vmax=1000,cmap=plt.cm.twilight)\nplt.title('Silver Deposits from EarthChem.org')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\nKey points\n\nYou can store things in Python in variables\nLists can be used to store objects of different types\nLoops with for can be used to iterate over each object in a list\nFunctions are used to write (and debug) repetitive code once\nIndexing\n\n\n\n\n\n\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/02b-Clustering.html",
    "href": "notebooks/02b-Clustering.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04b-DaskDataframes.html",
    "href": "notebooks/04b-DaskDataframes.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html",
    "href": "notebooks/03a-MachineLearning.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html",
    "href": "notebooks/04a-SimpleSpeedUps.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML.html",
    "href": "notebooks/03-ML.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML_workflow.html",
    "href": "notebooks/03-ML_workflow.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "How can I use Python for Machine learning?\nHow to I wrange my data to work within an ML context?\nHow do I assess whether my models fit well?\n\n\n\n\n\nUse scikit-learn to solve a machine learning problem\n\n\nMost machine learning problems begin with a dataset, but before we can perform any kind of inference on that dataset we must create/wrangle/build it. This is often the most time-consuming and hard part of a successful machine learning workflow. There is no set procedure here, as all data is different, although there are a few simple methods we can take to make a useful dataset.\nMachine learning can be split into:\n\nSupervised learning, where we are trying to predict:\n\nA numeric value such as a stock price, mineral quantity at a specific site etc. This is called “regression”.\nA categorical label, such as having/not having a specific disease, a response (such as “low”, “medium”, “high”) etc. This is called “classification”.\n\nUnsupervised learning, where we are trying to find groups in our data withouth specifying a particular outcome to split the dataset along.\n\nWe will be using data from a submitted Manuscript (Butterworth and Barnett-Moore 2020) which was a finalist in the Unearthed, ExploreSA: Gawler Challenge. You can visit the original repo here.\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-1---determine-our-target-variable",
    "href": "notebooks/03-ML_workflow.html#step-1---determine-our-target-variable",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Step 1 - Determine our target variable",
    "text": "Step 1 - Determine our target variable\nLet’s explore our our main dataset.\n\nDeposit locations - mine and mineral occurrences\nThe most important dataset for this workflow is the currently known locations of mineral occurrences. Using the data we already know about these mineral deposits we will build a model to predict where future occurrences will be.\n\n# For working with shapefiles (packaged is called pyshp)\nimport shapefile\n# For working with dataframes\nimport pandas as pd\n\n\n# Set the filename\nmineshape=\"../data/MinesMinerals/mines_and_mineral_occurrences_all.shp\"\n\n# Set shapefile attributes and assign\nsf = shapefile.Reader(mineshape)\nfields = [x[0] for x in sf.fields][1:]\nrecords = sf.records()\nshps = [s.points for s in sf.shapes()]\n\n# Write into a dataframe for easy use\ndf = pd.DataFrame(columns=fields, data=records)\n\nView the metadata of the South Australian all mines and mineral deposits to get a better understanding for what features we could use as a target.\n\n#See what the dataframe looks like\nprint(df.columns)\n\n#For clean printing to html drop columns that contains annoying / and \\ chars.\n#And set max columns\npd.options.display.max_columns = 8\ndf.drop(columns=['REFERENCE','O_MAP_SYMB'])\n\nIndex(['MINDEP_NO', 'DEP_NAME', 'REFERENCE', 'COMM_CODE', 'COMMODS',\n       'COMMOD_MAJ', 'COMM_SPECS', 'GCHEM_ASSC', 'DISC_YEAR', 'CLASS_CODE',\n       'OPER_TYPE', 'MAP_SYMB', 'STATUS_VAL', 'SIZE_VAL', 'GEOL_PROV',\n       'DB_RES_RVE', 'DB_PROD', 'DB_DOC_IMG', 'DB_EXV_IMG', 'DB_DEP_IMG',\n       'DB_DEP_FLE', 'COX_CLASS', 'REG_O_CTRL', 'LOC_O_CTRL', 'LOC_O_COM',\n       'O_LITH_CDE', 'O_LITH01', 'O_STRAT_NM', 'H_LITH_CDE', 'H_LITH02',\n       'H_STRAT_NM', 'H_MAP_SYMB', 'EASTING', 'NORTHING', 'ZONE', 'LONGITUDE',\n       'LATITUDE', 'SVY_METHOD', 'HORZ_ACC', 'SRCE_MAP', 'SRCE_CNTRE',\n       'COMMENTS', 'O_MAP_SYMB'],\n      dtype='object')\n\n\n\n\n\n  \n    \n      \n      MINDEP_NO\n      DEP_NAME\n      COMM_CODE\n      COMMODS\n      ...\n      HORZ_ACC\n      SRCE_MAP\n      SRCE_CNTRE\n      COMMENTS\n    \n  \n  \n    \n      0\n      5219\n      MOUNT DAVIES NO.2A\n      Ni\n      Nickel\n      ...\n      2000.0\n      500k meis\n      \n      \n    \n    \n      1\n      52\n      ONE STONE\n      Ni\n      Nickel\n      ...\n      500.0\n      71-385\n      \n      \n    \n    \n      2\n      8314\n      HINCKLEY RANGE\n      Fe\n      Iron\n      ...\n      500.0\n      \n      \n      \n    \n    \n      3\n      69\n      KALKA\n      V, ILM\n      Vanadium, Ilmenite\n      ...\n      100.0\n      1 MILE\n      mgt polygon on digital map\n      \n    \n    \n      4\n      65\n      ECHIDNA\n      Ni\n      Nickel\n      ...\n      20.0\n      50K GEOL\n      DH ECHIDNA PROSPECT\n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8672\n      6937\n      YARINGA\n      QTZE\n      Quartzite\n      ...\n      200.0\n      50k moc\n      fenced yard\n      \n    \n    \n      8673\n      4729\n      WELCHS\n      SCHT\n      Schist\n      ...\n      20.0\n      50k topo\n      \n      \n    \n    \n      8674\n      4718\n      ARCADIAN\n      CLAY\n      Clay\n      ...\n      5.0\n      Plan 1951-0327\n      Pit\n      \n    \n    \n      8675\n      1436\n      MCDONALD\n      Au\n      Gold\n      ...\n      200.0\n      50k moc\n      qz float\n      \n    \n    \n      8676\n      8934\n      FAIRFIELD FARM\n      SAND\n      Sand\n      ...\n      20.0\n      \n      pit\n      \n    \n  \n\n8677 rows × 41 columns\n\n\n\n\n#We are building a model to target South Australia, so load in a map of it.\ngawlshape=\"../data/SA/SA_STATE_POLYGON_shp\"\nshapeRead = shapefile.Reader(gawlshape)\nshapes  = shapeRead.shapes()\n\n#Save the boundary xy pairs in arrays we will use throughout the workflow\nxval = [x[0] for x in shapes[1].points]\nyval = [x[1] for x in shapes[1].points]\n\n\n# Subset the data, for a single Mineral target\ncommname='Mn'\n\n#Pull out all the occurences of the commodity and go from there\ncomm=df[df['COMM_CODE'].str.contains(commname)]\ncomm=comm.reset_index(drop=True)\nprint(\"Shape of \"+ commname, comm.shape)\n\n# Can make further subsets of the data here if needed\n#commsig=comm[comm.SIZE_VAL!=\"Low Significance\"]\n#comm=comm[comm.SIZE_VAL!=\"Low Significance\"]\n#comm=comm[comm.COX_CLASS == \"Olympic Dam Cu-U-Au\"]\n#comm=comm[(comm.lon<max(xval)) & (comm.lon>min(xval)) & (comm.lat>min(yval)) & (comm.lat<max(yval))]\n\nShape of Mn (115, 43)\n\n\n\n# For plotting\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nax.plot(df.LONGITUDE,df.LATITUDE,'b.',label=\"All Mineral Deposits\")\nax.plot(comm.LONGITUDE,comm.LATITUDE,'yx',label=commname+\" Deposits\")\n\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\n#ax.plot(comm.LONGITUDE, comm.LATITUDE, marker='o', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\nplt.show()"
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-2---wrangle-the-geophysical-and-geological-datasets-predictor-variables",
    "href": "notebooks/03-ML_workflow.html#step-2---wrangle-the-geophysical-and-geological-datasets-predictor-variables",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Step 2 - Wrangle the geophysical and geological datasets (predictor variables)",
    "text": "Step 2 - Wrangle the geophysical and geological datasets (predictor variables)\nMany geophysical data are available for South Australia overlapping our target mineral locations. We may presume that certain mineral occurrences express a combination of geology and geophysics. We can train an algorithm to learn these associations and then use the same algorithm to make predictions for where unknown occurrences may be found.\nHere we load in the (slightly) pre-processed geophysical datasets and prepare them for further manipulations, data-mining, and machine learning. All of the full/raw datasets are available from https://map.sarig.sa.gov.au/. For this exercise we have simplified the datasets by reducing complexity and resolution. Grab additional processed datasets from https://github.com/natbutter/gawler-exploration/tree/master/ML-DATA\n\nResistivity xyz data\n\n#Read in the data\ndata_res=pd.read_csv(\"../data/AusLAMP_MT_Gawler_25.xyzr\",\n                     sep=',',header=0,names=['lat','lon','depth','resistivity'])\ndata_res\n\n\n\n\n  \n    \n      \n      lat\n      lon\n      depth\n      resistivity\n    \n  \n  \n    \n      0\n      -27.363931\n      128.680796\n      -25.0\n      2.0007\n    \n    \n      1\n      -27.659362\n      128.662322\n      -25.0\n      1.9979\n    \n    \n      2\n      -27.886602\n      128.647965\n      -25.0\n      1.9948\n    \n    \n      3\n      -28.061394\n      128.636833\n      -25.0\n      1.9918\n    \n    \n      4\n      -28.195844\n      128.628217\n      -25.0\n      1.9885\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      11003\n      -35.127716\n      142.399588\n      -25.0\n      2.0079\n    \n    \n      11004\n      -35.230939\n      142.408396\n      -25.0\n      2.0084\n    \n    \n      11005\n      -35.365124\n      142.419903\n      -25.0\n      2.0085\n    \n    \n      11006\n      -35.539556\n      142.434958\n      -25.0\n      2.0076\n    \n    \n      11007\n      -35.766303\n      142.454694\n      -25.0\n      2.0049\n    \n  \n\n11008 rows × 4 columns\n\n\n\nThis data is the Lat-Lon spatial location and the value of the feature at that location.\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nim=ax.scatter(data_res.lon,data_res.lat,s=4,c=data_res.resistivity,cmap=\"jet\")\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nax.plot(comm.LONGITUDE, comm.LATITUDE, marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\ncbaxes = fig.add_axes([0.40, 0.18, 0.2, 0.015])\ncbar = plt.colorbar(im, cax = cbaxes,orientation=\"horizontal\",extend='both')\ncbar.set_label('Resistivity $\\Omega$.m', labelpad=10)\ncbar.ax.xaxis.set_label_position('top')\n\nplt.show()\n\n\n\n\n\n\nFaults and dykes vector polylines\n\n# For dealing with arrays \nimport numpy as np\n\n\n#Get fault data neo\nfaultshape=\"../data/Faults/Faults.shp\"\nshapeRead = shapefile.Reader(faultshape)\nshapes  = shapeRead.shapes()\nNshp    = len(shapes)\n\nfaultsNeo=[]\nfor i in range(0,Nshp):\n    for j in shapes[i].points:\n        faultsNeo.append([j[0],j[1]])\nfaultsNeo=np.array(faultsNeo)\nfaultsNeo\n\narray([[133.46269605, -27.41825034],\n       [133.46770683, -27.42062991],\n       [133.4723624 , -27.42259841],\n       ...,\n       [138.44613353, -35.36560605],\n       [138.44160669, -35.36672662],\n       [138.43805501, -35.36793484]])\n\n\nThis data is just a Lat-Lon location. Think how we can use this in a model.\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nplt.plot(faultsNeo[:,0],faultsNeo[:,1],'.',markersize=0.1,label=\"Neoproterozoic-Faults\")\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nax.plot(comm.LONGITUDE, comm.LATITUDE, marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\nplt.show()\n\n\n\n\n\n\nNetcdf formatted raster grids - geophysics\n\n# For timing events\nimport time\n# For making grids and reading netcdf data\nimport scipy\nimport scipy.io\n\n\n#Define a function to read the netcdf files\ndef readnc(filename):\n    tic=time.time()\n    rasterfile=filename\n    data = scipy.io.netcdf_file(rasterfile,'r',mmap=False)\n    xdata=data.variables['lon'][:]\n    ydata=data.variables['lat'][:]\n    zdata=np.array(data.variables['Band1'][:])\n    data.close()\n    \n    toc=time.time()\n    print(\"Loaded\", rasterfile, \"in\", f'{toc-tic:.2f}s')\n    print(\"Spacing x\", f'{xdata[2]-xdata[1]:.2f}', \n          \"y\", f'{ydata[2]-ydata[1]:.2f}', \n          \"Shape:\", np.shape(zdata), \"Min x:\", np.min(xdata), \"Max x:\", np.max(xdata),\n          \"Min y:\", np.min(ydata), f'Max y {np.max(ydata):.2f}')\n\n    return(xdata,ydata,zdata,np.min(xdata),np.min(ydata),xdata[2]-xdata[1],ydata[2]-ydata[1])\n\n\n# Digital Elevation Model\nx1,y1,z1,originx1,originy1,pixelx1,pixely1 = readnc(\"../data/sa-dem.nc\")\n# Total Magnetic Intensity\nx2,y2,z2,originx2,originy2,pixelx2,pixely2 = readnc(\"../data/sa-mag-tmi.nc\")\n# Gravity\nx3,y3,z3,originx3,originy3,pixelx3,pixely3 = readnc(\"../data/sa-grav.nc\")\n\nLoaded ../data/sa-dem.nc in 0.00s\nSpacing x 0.01 y 0.01 Shape: (1208, 1201) Min x: 129.005 Max x: 141.005 Min y: -38.065 Max y -25.99\nLoaded ../data/sa-mag-tmi.nc in 0.00s\nSpacing x 0.01 y 0.01 Shape: (1208, 1201) Min x: 129.005 Max x: 141.005 Min y: -38.065 Max y -25.99\nLoaded ../data/sa-grav.nc in 0.00s\nSpacing x 0.01 y 0.01 Shape: (1208, 1201) Min x: 129.005 Max x: 141.005 Min y: -38.065 Max y -25.99\n\n\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nim=plt.pcolormesh(x1,y1,z1,cmap='Greys',shading='auto')\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nax.plot(comm.LONGITUDE, comm.LATITUDE, marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\ncbaxes = fig.add_axes([0.40, 0.18, 0.2, 0.015])\ncbar = plt.colorbar(im, cax = cbaxes,orientation=\"horizontal\",extend='both')\ncbar.set_label('DEM (m)', labelpad=10)\ncbar.ax.xaxis.set_label_position('top')\n\nplt.show()\n\n\n\n\nThese data are raster grids. Essentially Lat-Lon-Value like the XYZ data, but represented in a different format.\n\n\nCategorical Geology in vector polygons\n\n#Archean basement geology\ngeolshape=shapefile.Reader(\"../data/Archaean_Early_Mesoprterzoic_polygons_shp/geology_archaean.shp\")\n\nrecsArch   = geolshape.records()\nshapesArch  = geolshape.shapes()\n\n\n# Print the field names in the shapefile\nfor i,field in enumerate(geolshape.fields):\n    print(i-1,field[0]) \n\n-1 DeletionFlag\n0 MAJORSTRAT\n1 SG_DESCRIP\n2 MAPUNIT\n3 SG_PROVINC\n4 DOMAIN\n5 AGE\n6 SEQUSET\n7 PRIMARYAGE\n8 OROGENYAGE\n9 INHERITAGE\n10 STRATNO\n11 STRATNAME\n12 STRATDESC\n13 GISCODE\n14 SUBDIVNAME\n15 SUBDIVSYMB\n16 PROVINCE\n17 MAXAGE\n18 MAXMOD\n19 MAXMETH\n20 MINAGE\n21 MINMOD\n22 MINMETH\n23 GLCODE\n\n\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\n\n#index of the geology unit #4 #10 #12\ngeoindex = 4\n#Gather all the unique Major Geology unit numbers\nlabs=[]\nfor i in recsArch:\n    labs.append(i[geoindex])\n\ngeols = list(set(labs))\n\n# Create a unique color for each geological unit label\ncolor = plt.cm.tab20(np.linspace(0, 1, len(geols)))\ncdict={}\nfor i, geol in enumerate(geols):\n    cdict.update({geol:color[i]})\n    \n#Plot each of the geology polygons\nlegend1=[]\nfor i in range(len(shapesArch)):\n    boundary = shapesArch[i].points\n    xs = [x for x, y in shapesArch[i].points]\n    ys = [y for x, y in shapesArch[i].points]\n    c = cdict[recsArch[i][geoindex]]\n    l1 = ax.fill(xs,ys,c=c,label=recsArch[i][geoindex])\n    legend1.append(l1)\n      \n#Plot the extra stuff\nl2 = ax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nl3 = ax.plot(comm.LONGITUDE, comm.LATITUDE, \n        marker='s', markeredgecolor='k', linestyle='',markersize=4, color='y',\n        label=commname+\" Deposits\")\n\n#Todo: Split the legends\n#ax.legend([l2,l3],['SA',commname+\" Deposits\"],loc=3)\n\n#Legend without duplicate values\nhandles, labels = ax.get_legend_handles_labels()\nunique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\nax.legend(*zip(*unique), bbox_to_anchor = (1.02, 1.01), ncol=3)\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\n#plt.legend(loc=3) #bbox_to_anchor = (1.05, 0.6))\n\nplt.show()\n\n\n\n\nTake a moment to appreciate the various methods you have used just to load the data!\nNow we need to think about what we actually want to achieve? What is our goal here? This will determine what kind of data analysis/manipulation we need to make here. Consider the flow diagram for choosing the right machine learning method."
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-3---assign-geophys-values-to-target-locations",
    "href": "notebooks/03-ML_workflow.html#step-3---assign-geophys-values-to-target-locations",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Step 3 - Assign geophys values to target locations",
    "text": "Step 3 - Assign geophys values to target locations\nWe need to assign the values of each of these geophysical datasets (predictor variables) to the target class (i.e. mineral deposit locations). The assumption being that the occurrence of some mineral deposit (e.g. Cu) is a function of x1, x2, x3, x4, x5, x6. Where the Resistivity is x1, the distance to a Neoprotezoic fault is x2, the value of DEM, magnetic TMI, and Gravity is x3, x4, and x5, and the geological basement unit is x6.\n\n# Make a Target DataFrame of the points we want to interrogate the features for\ntd1 = comm[['LONGITUDE', 'LATITUDE']].copy()\n\n\nResistivity\n\n# For making KD Trees\nimport scipy.spatial\n\n\n# Define a function which \"coregisters\" a point from a bunch of other points.\ndef coregPoint(tree,point,region,retval='index'):\n    '''\n    Finds the nearest neighbour to a point from a bunch of other points\n    tree - a scipy CKTree to search for the point over\n    point - array([longitude,latitude])\n    region - integer, same units as data\n    '''\n    dists, indexes = tree.query(point,k=1,distance_upper_bound=region) \n\n    if retval=='index':\n        return (indexes)\n    elif retval=='dists':\n        return(dists)\n    \n\n\n# Find the values of the resetivity grid for each lat/lon deposit location.\n\n# Make a search-tree of the point-pairs for fast lookup of nearest matches\ntreeres = scipy.spatial.cKDTree(np.c_[data_res.lon,data_res.lat])\n\n# Perform the search for each point\nindexes = td1.apply(\n    lambda x: coregPoint(treeres,np.array([x.LONGITUDE, x.LATITUDE]),1,retval='index'), axis=1)\n\n\ntd1['res'] = data_res.loc[indexes].resistivity.values\ntd1\n\n\n\n\n  \n    \n      \n      LONGITUDE\n      LATITUDE\n      res\n    \n  \n  \n    \n      0\n      139.179436\n      -29.877637\n      2.2135\n    \n    \n      1\n      138.808767\n      -30.086296\n      2.3643\n    \n    \n      2\n      138.752281\n      -30.445684\n      2.1141\n    \n    \n      3\n      138.530506\n      -30.533225\n      2.2234\n    \n    \n      4\n      138.887019\n      -30.565479\n      2.1982\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      110\n      136.059715\n      -34.327929\n      3.4926\n    \n    \n      111\n      138.016821\n      -35.733084\n      2.0868\n    \n    \n      112\n      139.250036\n      -34.250155\n      1.9811\n    \n    \n      113\n      135.905480\n      -34.425866\n      2.7108\n    \n    \n      114\n      135.835578\n      -34.509779\n      3.1224\n    \n  \n\n115 rows × 3 columns\n\n\n\n\n\nFaults\n\n#Same for the fault data \n# but this time we get the \"distance to the point\", rather than the value at that point.\ntreefaults = scipy.spatial.cKDTree(faultsNeo)\n\ndists = td1.apply(\n    lambda x: coregPoint(treefaults,np.array([x.LONGITUDE, x.LATITUDE]),100,retval='dists'), axis=1)\n\n\ntd1['faults'] = dists\ntd1\n\n\n\n\n  \n    \n      \n      LONGITUDE\n      LATITUDE\n      res\n      faults\n    \n  \n  \n    \n      0\n      139.179436\n      -29.877637\n      2.2135\n      0.010691\n    \n    \n      1\n      138.808767\n      -30.086296\n      2.3643\n      0.103741\n    \n    \n      2\n      138.752281\n      -30.445684\n      2.1141\n      0.006659\n    \n    \n      3\n      138.530506\n      -30.533225\n      2.2234\n      0.013925\n    \n    \n      4\n      138.887019\n      -30.565479\n      2.1982\n      0.007356\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      110\n      136.059715\n      -34.327929\n      3.4926\n      0.526835\n    \n    \n      111\n      138.016821\n      -35.733084\n      2.0868\n      0.002451\n    \n    \n      112\n      139.250036\n      -34.250155\n      1.9811\n      0.027837\n    \n    \n      113\n      135.905480\n      -34.425866\n      2.7108\n      0.670323\n    \n    \n      114\n      135.835578\n      -34.509779\n      3.1224\n      0.776152\n    \n  \n\n115 rows × 4 columns\n\n\n\n\n\nGeophysics\n\n# Define a function which \"coregisters\" a point within a raster.\ndef get_coords_at_point(originx,originy,pixelx,pixely,lon,lat):\n    '''\n    Given a point in some coordinate reference (e.g. lat/lon)\n    Find the closest point to that in an array (e.g. a raster)\n    and return the index location of that point in the raster.\n    INPUTS\n        \"output from \"gdal_data.GetGeoTransform()\"\n    originx: first point in first axis\n    originy: first point in second axis\n    pixelx: difference between x points\n    pixely: difference between y points\n    \n    lon: x/row-coordinate of interest\n    lat: y/column-coordinate of interest\n    \n    RETURNS\n    col: x index value from the raster\n    row: y index value from the raster\n    '''\n    row = int((lon - originx)/pixelx)\n    col = int((lat - originy)/pixely)\n\n    return (col, row)\n\n\n# Pass entire array of latlon and raster info to us in get_coords_at_point\ndef rastersearch(latlon,raster,originx,originy,pixelx,pixely):\n    zlist=[]\n    for lon,lat in zip(latlon.LONGITUDE,latlon.LATITUDE):\n        try:\n            zlist.append(raster[get_coords_at_point(originx,originy,pixelx,pixely,lon,lat)])\n        except:\n            zlist.append(np.nan)\n            \n    return(zlist)\n\n\ntd1['dem'] = rastersearch(td1,z1,originx1,originy1,pixelx1,pixely1)\ntd1['mag'] = rastersearch(td1,z2,originx2,originy2,pixelx2,pixely2)\ntd1['grav'] = rastersearch(td1,z3,originx3,originy3,pixelx3,pixely3)\n\n\ntd1\n\n\n\n\n  \n    \n      \n      LONGITUDE\n      LATITUDE\n      res\n      faults\n      dem\n      mag\n      grav\n    \n  \n  \n    \n      0\n      139.179436\n      -29.877637\n      2.2135\n      0.010691\n      187.297424\n      -118.074890\n      1.852599\n    \n    \n      1\n      138.808767\n      -30.086296\n      2.3643\n      0.103741\n      179.499237\n      -209.410507\n      -12.722121\n    \n    \n      2\n      138.752281\n      -30.445684\n      2.1141\n      0.006659\n      398.336823\n      -159.566422\n      -6.249788\n    \n    \n      3\n      138.530506\n      -30.533225\n      2.2234\n      0.013925\n      335.983429\n      -131.176437\n      -11.665316\n    \n    \n      4\n      138.887019\n      -30.565479\n      2.1982\n      0.007356\n      554.278198\n      -192.363297\n      -1.025702\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      110\n      136.059715\n      -34.327929\n      3.4926\n      0.526835\n      45.866119\n      -244.067841\n      11.410070\n    \n    \n      111\n      138.016821\n      -35.733084\n      2.0868\n      0.002451\n      145.452789\n      -203.566940\n      18.458364\n    \n    \n      112\n      139.250036\n      -34.250155\n      1.9811\n      0.027837\n      276.489319\n      -172.889587\n      -1.714886\n    \n    \n      113\n      135.905480\n      -34.425866\n      2.7108\n      0.670323\n      162.431747\n      569.713684\n      15.066316\n    \n    \n      114\n      135.835578\n      -34.509779\n      3.1224\n      0.776152\n      89.274399\n      64.385925\n      24.267015\n    \n  \n\n115 rows × 7 columns\n\n\n\n\n# Check we got it right.\n# Plot a grid, and our interrogated points\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nim=plt.pcolormesh(x3,y3,z3,cmap='jet',shading='auto',vmin=min(td1.grav),vmax=max(td1.grav))\n#ax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\n#ax.plot(comm.LONGITUDE, comm.LATITUDE, marker='o', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nax.scatter(td1.LONGITUDE, td1.LATITUDE, s=20, c=td1.grav,\n           label=commname+\" Gravity\",cmap='jet',vmin=min(td1.grav),vmax=max(td1.grav),edgecolors='white')\n\nplt.xlim(138,140)\nplt.ylim(-32,-30)\nplt.legend(loc=3)\n\ncbaxes = fig.add_axes([0.40, 0.18, 0.2, 0.015])\ncbar = plt.colorbar(im, cax = cbaxes,orientation=\"horizontal\",extend='both')\ncbar.set_label('Gravity (gal)', labelpad=10)\ncbar.ax.xaxis.set_label_position('top')\n\nplt.show()\n\n\n\n\n\n\nGeology\n\n# For dealing with shapefile components\nfrom shapely.geometry import Point\nfrom shapely.geometry import shape\n\n#Define a function to find what polygon a point lives inside (speed imporivements can be made here)\ndef shapeExplore(lon,lat,shapes,recs,record):\n    #'record' is the column index you want returned\n    for i in range(len(shapes)):\n        boundary = shapes[i]\n        if Point((lon,lat)).within(shape(boundary)):\n            return(recs[i][record])\n    #if you have been through the loop with no result\n    return('-9999')\n\n\n%%time\ngeoindex = 4\ntd1['geol']=td1.apply(lambda x: shapeExplore(x.LONGITUDE, x.LATITUDE, shapesArch,recsArch,geoindex), axis=1)\n\nCPU times: user 4.57 s, sys: 75.1 ms, total: 4.65 s\nWall time: 4.52 s\n\n\n\ntd1\n\n\n\n\n  \n    \n      \n      LONGITUDE\n      LATITUDE\n      res\n      faults\n      dem\n      mag\n      grav\n      geol\n    \n  \n  \n    \n      0\n      139.179436\n      -29.877637\n      2.2135\n      0.010691\n      187.297424\n      -118.074890\n      1.852599\n      Crustal element Muloorina\n    \n    \n      1\n      138.808767\n      -30.086296\n      2.3643\n      0.103741\n      179.499237\n      -209.410507\n      -12.722121\n      Crustal element Adelaide\n    \n    \n      2\n      138.752281\n      -30.445684\n      2.1141\n      0.006659\n      398.336823\n      -159.566422\n      -6.249788\n      Crustal element Adelaide\n    \n    \n      3\n      138.530506\n      -30.533225\n      2.2234\n      0.013925\n      335.983429\n      -131.176437\n      -11.665316\n      Crustal element Adelaide\n    \n    \n      4\n      138.887019\n      -30.565479\n      2.1982\n      0.007356\n      554.278198\n      -192.363297\n      -1.025702\n      Crustal element Adelaide\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      110\n      136.059715\n      -34.327929\n      3.4926\n      0.526835\n      45.866119\n      -244.067841\n      11.410070\n      Cleve, Spencer, Olympic Domains\n    \n    \n      111\n      138.016821\n      -35.733084\n      2.0868\n      0.002451\n      145.452789\n      -203.566940\n      18.458364\n      Crustal element Kanmantoo SW\n    \n    \n      112\n      139.250036\n      -34.250155\n      1.9811\n      0.027837\n      276.489319\n      -172.889587\n      -1.714886\n      Crustal element Kanmantoo Main\n    \n    \n      113\n      135.905480\n      -34.425866\n      2.7108\n      0.670323\n      162.431747\n      569.713684\n      15.066316\n      Cleve Domain\n    \n    \n      114\n      135.835578\n      -34.509779\n      3.1224\n      0.776152\n      89.274399\n      64.385925\n      24.267015\n      Cleve Domain\n    \n  \n\n115 rows × 8 columns\n\n\n\nCongrats, you now have an ML dataset ready to go!\nAlmost… but what is the target? Let’s make a binary classifier."
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-4---generate-a-non-deposit-dataset",
    "href": "notebooks/03-ML_workflow.html#step-4---generate-a-non-deposit-dataset",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Step 4 - Generate a “non-deposit” dataset",
    "text": "Step 4 - Generate a “non-deposit” dataset\nWe have a set of locations where a certain mineral deposit occurs along with the values of various geophysical parameters at those locations. To identify what values of the geophysics are associated with a mineral deposit then we need a representation of the “background noise” of those parameters, i.e. what the values are when there is no mineral deposit.\nThis step is important. There are numerous ways to generate our non-deposit set, each with different benefits and trade-offs. The randomisation of points throughout some domain appears to be robust. But you must think, is this domain a reasonable estimation of “background” geophysics/geology? Why are you picking these locations as non-deposits? Will they be over/under-representing actual deposits? Will they be over/under-representing actual non-deposits?\n\n#Now make a set of \"non-deposits\" using a random location within our exploration area\nlats_rand=np.random.uniform(low=min(df.LATITUDE), high=max(df.LATITUDE), size=len(comm.LATITUDE))\nlons_rand=np.random.uniform(low=min(df.LONGITUDE), high=max(df.LONGITUDE), size=len(comm.LONGITUDE))\n\nprint(\"Produced\", len(lats_rand),len(lons_rand), \"latitude-longitude pairs for non-deposits.\")\n\nProduced 115 115 latitude-longitude pairs for non-deposits.\n\n\n\n# Where are these randomised \"non deposits\"\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\n\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\n\nax.plot(lons_rand, lats_rand, \n        marker='.', linestyle='',markersize=1, color='b',label=\"Random Samples\")\n\nax.plot(td1.LONGITUDE, td1.LATITUDE, \n        marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\nplt.show()\n\n\n\n\nWe must do the same coregistration/interrogation of the different data layers for our randomised “non-deposit” data.\n\n%%time\n\ntd2 = pd.DataFrame({'LONGITUDE': lons_rand, 'LATITUDE': lats_rand})\n                   \n# Res\nindexes = td2.apply(\n    lambda x: coregPoint(treeres,np.array([x.LONGITUDE, x.LATITUDE]),10,retval='index'), axis=1)\n    \ntd2['res'] = data_res.loc[indexes].resistivity.values\n\n# Faults\ntd2['faults'] = td2.apply(\n    lambda x: coregPoint(treefaults,np.array([x.LONGITUDE, x.LATITUDE]),100,retval='dists'), axis=1)\n\n# Geophys\ntd2['dem'] = rastersearch(td2,z1,originx1,originy1,pixelx1,pixely1)\ntd2['mag'] = rastersearch(td2,z2,originx2,originy2,pixelx2,pixely2)\ntd2['grav'] = rastersearch(td2,z3,originx3,originy3,pixelx3,pixely3)\n\n#Geology\ntd2['geol']=td2.apply(lambda x: shapeExplore(x.LONGITUDE, x.LATITUDE, shapesArch,recsArch,geoindex), axis=1)\n\nCPU times: user 10.5 s, sys: 122 ms, total: 10.6 s\nWall time: 10.4 s\n\n\n\n#Add flag indicating classification label\ntd1['deposit']=1\ntd2['deposit']=0\n\n\nfv = pd.concat([td1,td2],axis=0,ignore_index=True)\nfv\n\n\n\n\n  \n    \n      \n      LONGITUDE\n      LATITUDE\n      res\n      faults\n      ...\n      mag\n      grav\n      geol\n      deposit\n    \n  \n  \n    \n      0\n      139.179436\n      -29.877637\n      2.2135\n      0.010691\n      ...\n      -118.074890\n      1.852599\n      Crustal element Muloorina\n      1\n    \n    \n      1\n      138.808767\n      -30.086296\n      2.3643\n      0.103741\n      ...\n      -209.410507\n      -12.722121\n      Crustal element Adelaide\n      1\n    \n    \n      2\n      138.752281\n      -30.445684\n      2.1141\n      0.006659\n      ...\n      -159.566422\n      -6.249788\n      Crustal element Adelaide\n      1\n    \n    \n      3\n      138.530506\n      -30.533225\n      2.2234\n      0.013925\n      ...\n      -131.176437\n      -11.665316\n      Crustal element Adelaide\n      1\n    \n    \n      4\n      138.887019\n      -30.565479\n      2.1982\n      0.007356\n      ...\n      -192.363297\n      -1.025702\n      Crustal element Adelaide\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      225\n      140.264586\n      -36.024709\n      1.9738\n      0.109419\n      ...\n      82.679138\n      3.195437\n      Crustal element Kanmantoo Main\n      0\n    \n    \n      226\n      132.210526\n      -26.808665\n      2.0358\n      0.445198\n      ...\n      -311.140411\n      -32.652912\n      Fregon Subdomain\n      0\n    \n    \n      227\n      137.838001\n      -33.729123\n      -0.6225\n      0.333648\n      ...\n      1048.547852\n      -20.456957\n      Nawa, Mount Woods, Fowler, Wilgena, Harris Gr\n      0\n    \n    \n      228\n      140.016574\n      -32.481088\n      2.0752\n      0.234001\n      ...\n      -259.214233\n      -16.506145\n      Crustal element Adelaide\n      0\n    \n    \n      229\n      131.226528\n      -30.922765\n      1.9347\n      0.020291\n      ...\n      15.309577\n      -38.666344\n      Fisher Domain\n      0\n    \n  \n\n230 rows × 9 columns\n\n\n\n\n# Save all our hard work to a csv file for more hacking to come!\nfv.to_csv('../data/fv.csv',index=False)"
  },
  {
    "objectID": "notebooks/02a-mapping.html",
    "href": "notebooks/02a-mapping.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/text2segy.html",
    "href": "notebooks/text2segy.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "A. Read in text data\n\n# Set the filename\nf = open(\"userdata/P-impedance_2400-3000ms.gslib\",'r')\n\n#Read in all the lines in the file and save them to a variable\nmylist = f.readlines()\n\n#Close the file\nf.close()\nprint(\"Done reading file. Number of lines:\", len(mylist))\n\nDone reading file. Number of lines: 3957609\n\n\n\n#Have a look at the top of the data\nfor line in mylist[0:11]:\n    print(line)\n\nPETREL: Properties\n\n7\n\ni_index unit1 scale1\n\nj_index unit1 scale1\n\nk_index unit1 scale1\n\nx_coord unit1 scale1\n\ny_coord unit1 scale1\n\nz_coord unit1 scale1\n\nP-Impedance unit1 scale1\n\n127 1 300 438131.65314303 6475378.74871708 -2999.00000000 9556.294922 \n\n128 1 300 438181.65314303 6475378.74871708 -2999.00000000 9627.205078 \n\n\n\n\n# A few ways to map this to a useful python format. But the simplest may be the following\ndata = []\n\n#Skip the header rows\nfor line in mylist[9:]:\n    linesplit = line.split()\n    \n    i = int(linesplit[0])\n    j = int(linesplit[1])\n    k = int(linesplit[2])\n    x = float(linesplit[3])\n    y = float(linesplit[4])\n    z = float(linesplit[5])\n    p = float(linesplit[6])\n    \n    data.append([i,j,k,x,y,z,p])\n\n\n#Put the list in a dataframe\ndf=pd.DataFrame(data)\n\n#Then free up some memory (because this is a fairly big chunk of data)\ndata=None\n\n#Set the names of the columns of the dataframe\ndf.columns=['i','j','k','x','y','z','p']\n\ndf\n\n\n\n\n  \n    \n      \n      i\n      j\n      k\n      x\n      y\n      z\n      p\n    \n  \n  \n    \n      0\n      127\n      1\n      300\n      438131.653143\n      6.475379e+06\n      -2999.0\n      9556.294922\n    \n    \n      1\n      128\n      1\n      300\n      438181.653143\n      6.475379e+06\n      -2999.0\n      9627.205078\n    \n    \n      2\n      129\n      1\n      300\n      438231.653143\n      6.475379e+06\n      -2999.0\n      9555.066406\n    \n    \n      3\n      130\n      1\n      300\n      438281.653143\n      6.475379e+06\n      -2999.0\n      9468.100586\n    \n    \n      4\n      123\n      2\n      300\n      437931.653143\n      6.475429e+06\n      -2999.0\n      9601.517578\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3957595\n      26\n      127\n      1\n      433081.653143\n      6.481679e+06\n      -2401.0\n      5848.801758\n    \n    \n      3957596\n      27\n      127\n      1\n      433131.653143\n      6.481679e+06\n      -2401.0\n      5924.203125\n    \n    \n      3957597\n      28\n      127\n      1\n      433181.653143\n      6.481679e+06\n      -2401.0\n      6037.129883\n    \n    \n      3957598\n      29\n      127\n      1\n      433231.653143\n      6.481679e+06\n      -2401.0\n      5978.708984\n    \n    \n      3957599\n      25\n      128\n      1\n      433031.653143\n      6.481729e+06\n      -2401.0\n      5812.972168\n    \n  \n\n3957600 rows × 7 columns\n\n\n\n\n#Plot a single trace to see everything looks okay\none_trace = df[(df.i==127) & (df.j==1)].p\n\nplt.figure(figsize=(16,2))\nplt.plot(one_trace)\nplt.show()\n\n\n\n\n\nplt.plot(df.z)\n\n\n\n\n\n\nB. Read in equivalent segy data.\nGet the segy header information from here, and just see what the goal is for the conversion.\n\nstream = None \nstream = _read_segy(\"userdata/P-impedance_2400-3000ms.sgy\", headonly=True)\nprint(np.shape(stream.traces))\nstream\n\n(216540,)\n\n\n216540 traces in the SEG Y structure.\n\n\n\narrs=[]\nfor index, one_trace in enumerate(stream.traces):\n    p = one_trace.data\n    lenp = len(p)\n    i = np.full((lenp), index,dtype=int)\n    j = np.full((lenp), one_trace.header.ensemble_number,dtype=int)\n    k = np.arange(lenp,0,-1,dtype=int)\n    x = np.full((lenp), one_trace.header.source_coordinate_x)\n    y = np.full((lenp), one_trace.header.source_coordinate_y)\n    z = np.full((lenp), one_trace.header.original_field_record_number)\n    \n    temparr = np.c_[i,j,k,x,y,z,p]\n    arrs.append(temparr)\n    \narrs = np.vstack(arrs)\n\n\narrs[-1]\n\narray([2.16539000e+05, 2.50000000e+03, 1.00000000e+00, 4.32975000e+05,\n       6.48179200e+06, 1.03610000e+04, 9.38641016e+03])\n\n\n\nheader =\"PETREL: Properties\\n7\\n\\ni_index unit1 scale1\\nj_index unit1 scale1\\nk_index unit1 scale1\\nx_coord unit1 scale1\\ny_coord unit1 scale1\\nz_coord unit1 scale1\\nP-Impedance unit1 scale1\\n\"\nnp.savetxt(\"data.txt\", arrs,header=header, comments='', fmt=\"%i %i %i %i %i %i %f\")\n\n\none_trace = stream.traces[100]\n\n#Print out details single trace\nprint(one_trace)\n\n#Plot a single trace to see everything looks okay\nplt.figure(figsize=(16,2))\nplt.plot(one_trace.data)\nplt.show()\n\nTrace sequence number within line: 0\n151 samples, dtype=float32, 250.00 Hz\n\n\n\n\n\n\n#Here is the header information - for one trace\nstream.traces[0].header\n\ntrace_sequence_number_within_line: 0\ntrace_sequence_number_within_segy_file: 0\noriginal_field_record_number: 9961\ntrace_number_within_the_original_field_record: 0\nenergy_source_point_number: 0\nensemble_number: 1961\ntrace_number_within_the_ensemble: 0\ntrace_identification_code: 0\nnumber_of_vertically_summed_traces_yielding_this_trace: 0\nnumber_of_horizontally_stacked_traces_yielding_this_trace: 0\ndata_use: 0\ndistance_from_center_of_the_source_point_to_the_center_of_the_receiver_group: 0\nreceiver_group_elevation: 0\nsurface_elevation_at_source: 0\nsource_depth_below_surface: 0\ndatum_elevation_at_receiver_group: 0\ndatum_elevation_at_source: 0\nwater_depth_at_source: 0\nwater_depth_at_group: 0\nscalar_to_be_applied_to_all_elevations_and_depths: 0\nscalar_to_be_applied_to_all_coordinates: 0\nsource_coordinate_x: 438302\nsource_coordinate_y: 6475310\ngroup_coordinate_x: 0\ngroup_coordinate_y: 0\ncoordinate_units: 0\nweathering_velocity: 0\nsubweathering_velocity: 0\nuphole_time_at_source_in_ms: 0\nuphole_time_at_group_in_ms: 0\nsource_static_correction_in_ms: 0\ngroup_static_correction_in_ms: 0\ntotal_static_applied_in_ms: 0\nlag_time_A: 2400\nlag_time_B: 3000\ndelay_recording_time: 0\nmute_time_start_time_in_ms: 0\nmute_time_end_time_in_ms: 0\nnumber_of_samples_in_this_trace: 151\nsample_interval_in_ms_for_this_trace: 4000\ngain_type_of_field_instruments: 0\ninstrument_gain_constant: 0\ninstrument_early_or_initial_gain: 0\ncorrelated: 0\nsweep_frequency_at_start: 0\nsweep_frequency_at_end: 0\nsweep_length_in_ms: 0\nsweep_type: 0\nsweep_trace_taper_length_at_start_in_ms: 0\nsweep_trace_taper_length_at_end_in_ms: 0\ntaper_type: 0\nalias_filter_frequency: 0\nalias_filter_slope: 0\nnotch_filter_frequency: 0\nnotch_filter_slope: 0\nlow_cut_frequency: 0\nhigh_cut_frequency: 0\nlow_cut_slope: 0\nhigh_cut_slope: 0\nyear_data_recorded: 0\nday_of_year: 0\nhour_of_day: 0\nminute_of_hour: 0\nsecond_of_minute: 0\ntime_basis_code: 0\ntrace_weighting_factor: 0\ngeophone_group_number_of_roll_switch_position_one: 0\ngeophone_group_number_of_trace_number_one: 0\ngeophone_group_number_of_last_trace: 0\ngap_size: 0\nover_travel_associated_with_taper: 0\nx_coordinate_of_ensemble_position_of_this_trace: 0\ny_coordinate_of_ensemble_position_of_this_trace: 0\nfor_3d_poststack_data_this_field_is_for_in_line_number: 0\nfor_3d_poststack_data_this_field_is_for_cross_line_number: 0\nshotpoint_number: 0\nscalar_to_be_applied_to_the_shotpoint_number: 0\ntrace_value_measurement_unit: 0\ntransduction_constant_mantissa: 0\ntransduction_constant_exponent: 0\ntransduction_units: 0\ndevice_trace_identifier: 0\nscalar_to_be_applied_to_times: 0\nsource_type_orientation: 0\nsource_energy_direction_mantissa: 0\nsource_energy_direction_exponent: 0\nsource_measurement_mantissa: 0\nsource_measurement_exponent: 0\nsource_measurement_unit: 0\n\n\n\n\nC. Write out the text data as segy\n\n# Group all the text traces by their the i-j coordinates\ngroups=df.groupby(['i','j'])\nprint(len(groups))\n\n#Here I notice there are only 13192 traces in the text data, compared with the 216540 in the segy data...\n\n13192\n\n\n\n%%time\n\n#Make a stream object (flush it out to begin because we have used this variable names for demos)\nstream_out = None \nstream_out = Stream()\n\n#not sure how to group the trace ensembles but can use a counter to keep track of them\nensemble_number = 0\n       \nfor ids,df_trace in groups:\n    #ids are the i, j coordinate locations\n    #trc is the subset of the full dataframe for just that i-j location\n\n    #For each i-j location, a trace is impdence at all the depth values, i.e.\n    data = df_trace.p.values\n\n    # Enforce correct byte number and set to the Trace object\n    data = np.require(data, dtype=np.float32)\n    trace = Trace(data=data)\n\n    # Set all the segy header information\n    # Attributes in trace.stats will overwrite everything in trace.stats.segy.trace_header\n    trace.stats.delta = 0.01\n    trace.stats.starttime = UTCDateTime(1970,1,1,0,0,0)\n\n    # If you want to set some additional attributes in the trace header,\n    # add one and only set the attributes you want to be set. Otherwise the\n    # header will be created for you with default values.\n    if not hasattr(trace.stats, 'segy.trace_header'):\n        trace.stats.segy = {}\n\n    trace.stats.segy.trace_header = SEGYTraceHeader()\n\n#         trace.stats.segy.trace_header.trace_sequence_number_within_line = index + 1\n#         trace.stats.segy.trace_header.receiver_group_elevation = 0\n    trace.stats.segy.trace_header.source_coordinate_x = int(df_trace.x.values[0])\n    trace.stats.segy.trace_header.source_coordinate_y = int(df_trace.y.values[0])\n    trace.stats.segy.trace_header.ensemble_number = ensemble_number #Not sure how this is actually determined\n    trace.stats.segy.trace_header.lag_time_A = 2400\n    trace.stats.segy.trace_header.lag_time_B = 3000\n    trace.stats.segy.trace_header.number_of_samples_in_this_trace = len(data)\n\n    ensemble_number +=1\n\n    # Add trace to stream\n    stream_out.append(trace)\n\n# A SEGY file has file wide headers. This can be attached to the stream\n# object.  If these are not set, they will be autocreated with default\n# values.\nstream_out.stats = AttribDict()\nstream_out.stats.textual_file_header = 'Textual Header!'\nstream_out.stats.binary_file_header = SEGYBinaryFileHeader()\nstream_out.stats.binary_file_header.trace_sorting_code = 5\n# stream.stats.binary_file_header.number_of_data_traces_per_ensemble=1\n\nprint(\"Stream object before writing...\")\nprint(stream_out)\n\nstream_out.write(\"TEST.sgy\", format=\"SEGY\", data_encoding=1, byteorder=sys.byteorder)\n\nprint(\"Stream object after writing. Will have some segy attributes...\")\nprint(stream_out)\n\nStream object before writing...\n13192 Trace(s) in Stream:\n\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n...\n(13190 other traces)\n...\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\nStream object after writing. Will have some segy attributes...\n13192 Trace(s) in Stream:\n\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n...\n(13190 other traces)\n...\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\nCPU times: total: 7.55 s\nWall time: 7.52 s\n\n\n\n#Now check it\nprint(\"Reading using obspy.io.segy...\")\nst1 = _read_segy(\"TEST.sgy\")\nprint(st1)\n\nprint(np.shape(st1.traces))\nst1\n\nReading using obspy.io.segy...\n13192 traces in the SEG Y structure.\n(13192,)\n\n\n13192 traces in the SEG Y structure.\n\n\n\none_trace = st1.traces[0]\n\n#Print out details single trace\nprint(one_trace)\n\n#Plot a single trace\nplt.figure(figsize=(16,2))\nplt.plot(one_trace.data)\nplt.show()\n\nTrace sequence number within line: 0\n300 samples, dtype=float32, 100.00 Hz\n\n\n\n\n\n\n\nBonus - why is there a data discrepency\nI would have expected the traces from the segy data and from the gslib data to match up. But there are only 13192 traces in the text data and 216540 in the segy data. Something weird is going. I can plot these side by side and see if some data are missing.\n\nxx=[]\nyy=[]\nfor i in range(len(stream.traces)):\n    xx.append(stream.traces[i].header.source_coordinate_x)\n    yy.append(stream.traces[i].header.source_coordinate_y)  \n\n\nplt.scatter(xx,yy)\nplt.scatter(df.x,df.y)\n\n<matplotlib.collections.PathCollection at 0x2d03be3b550>\n\n\n\n\n\nAt first glance this seems ok. The data coverage is over the same area, the edges are missing buuuuut zooming in we can see the text data is not as dense as the segy data. Problem solved.\n\nplt.scatter(xx,yy,s=0.1)\nplt.scatter(df.x,df.y,s=0.1)\n\nplt.xlim([433000,434000])\n# plt.ylim([433000,434000])\n\n(433000.0, 434000.0)\n\n\n\n\n\n\n\n\n\nCopyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03b-DeepLearningTS.html",
    "href": "notebooks/03b-DeepLearningTS.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/PDF2Python.html",
    "href": "notebooks/PDF2Python.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "Copyright Sydney Informatics Hub, University of Sydney"
  }
]