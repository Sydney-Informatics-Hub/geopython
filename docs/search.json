[
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Python for Geoscience",
    "section": "",
    "text": "Questions\n\nWhat is Python?\nWhy do you want to learn Python?\n\n\n\nObjectives\n\nUnderstand the Python ecosystem and other tools.\n\n\nThis course we will introduce you to foundations of Python programming. We will utilise common geosicence data types (geospatial, temporal, vector, raster, etc) to demonstrate a variety of practical workflows and showcase fundamental capabilities of Python. We will carry out exploratory, analytical, computational and machine learning analyses on these datasets. At the end of the course you will be able to adapt these workflows to your own datasets.\nThe course is presented by the Sydney Informatics Hub on behalf of the Petroleum Exploration Society of Australia.\n\nWhat is Python?\nThe Python programming language was written in the 1980’s. It is an interpreted and high-level language, this makes it easy to use for rapid development with lots of flexibility. Version 2.0 was released in 2000. Version 3.0 was released in 2008 (the current version is 3.11).\nI use it everyday to:\n\nAutomate tasks (do things millions of times easily)\nCalculate big numbers (can solve most computational problems)\nFormat and analyse data\nProcess images\n\nThere are many comparable languages (e.g. R, Julia, C++, bash, Matlab, Java). But there are a few reasons to favour Python:\n\nPython is free and open-source.\nThere is a large community of people using it all over the world on different projects, which means there is a lot of help and documentation.\nThere are millions of codes, packages, libraries, and extensions. Some that leverage other programming languages to make your Python tasks fast, efficient and capable of doing whatever you need it to.\n\n\n\nHow do we use Python? Terminals and Notebooks.\nTraditionally one writes a “Python script file”, like a recipe of instructions, and Python executes the script.\nSimply, you can create Python files in a text editor:\nprint(\"Hello World\")\nSave this as hello.py and execute it with python hello.py.\nAs you go deeper into Python, you will see more advanced syntax:\n#!/usr/bin/env python\n\n'''\nA Python program which greets the Earth!\nNathaniel Butterworth\nSIH Python course\n\nusage: python helloworld_advanced.py\n'''\n\ndef main():\n    print(\"Hello World! This is basically the same result, but a different way to get there.\")\n\nif __name__ == '__main__':\n    main()\nOnce again, you can save this in a text editor as hello_advanced.py and execute with python hello_advanced.py.\nYou can also start a Python IDE session, and execute commands one by one.\nA handy tool is the Jupyter Notebook (modelled from Mathematica’s Notebooks), that we will predominately be using throughout this course. They are good for the kind of development (as opposed to production/deployment) focused Python tasks you may need.\nThere are also online environments that can host Python code and notebooks for you.\nThroughout the course you will see when and why to use different environments.\nNow let’s get into in the practical session!\n\nKey points\n\nPython is a programming language.\nThere is a rich ecosystem of tools around creating and deploying Python code.\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03b-DeepLearningTS.html",
    "href": "notebooks/03b-DeepLearningTS.html",
    "title": "Deep Learning with Time Series data",
    "section": "",
    "text": "Questions\n\n“What is deep learning?”\n“What is a GPU and why do I care?”\n\n\n\nObjectives\n\nRun a Tensorflow job\nPredict the weather\n\n\nPython offers many ways to make use of the compute capability in your GPU. A very common application is deep learning using the tensorflow and keras (or pytorch) packages. In this example we are going to look at forecasting a timeseries using recurrent neural netowrks based on the history of the time series itself.\nWe are looking at temperature data in Sydney for the last 150 years with daily measurements (Based on an example from https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/). We want to predict what the future is going to look like. Note: the default values in the notebook restrict the length of dataset used in the analysis purely for time constraints. But feel free to adjust the numbers as you like. Using a GPU-enabled version of tensorflow will greatly reduce the time training. This deep-learning framework to predict time series data is known Long Short-Term Memory (LSTM) deep learning network.\nThe data is from the Australian Bureau of Meteorology (BOM) representing the daily maximum temperatures for the last 150 years from the Sydney Observatory\nA problem might be, given the last few decades of temperature cycles, what will tomorrow’s weather be? Let’s try and predict the future!\n\n#import all the libraries we need\nimport numpy\nimport time\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MaxAbsScaler \n\n\n#Load in the dataset\nfilename='../data/sydney_temperature.csv'\ndataframe = pd.read_csv(filename, usecols=[5], engine='python')\ndataset = dataframe.dropna()\ndataset = dataset.values\ndataset = dataset.astype('float32')\n\n# normalize the dataset to be betwenn 0 and 1\nscaler = MinMaxScaler(feature_range=(0, 1))\ndatasetScaled = scaler.fit_transform(dataset)\n\n\n#Print some stats about the data\nprint(dataframe.describe())\n\n       Maximum temperature (Degree C)\ncount                    58316.000000\nmean                        21.731120\nstd                          4.669517\nmin                          7.700000\n25%                         18.200000\n50%                         21.600000\n75%                         24.900000\nmax                         45.800000\n\n\n\n#Look at some of the data set\n#This is the temperature throughout the year.\n#The summer and winter cycles are obvious\n#But there is a fair bit of variablity day-to-day\nplt.plot(dataset[50000:])\nplt.xlabel(\"Day\")\nplt.ylabel(\"Temperature (degrees Celsius)\");\n\n\n\n\n\n# split into train and test sets\n#Use the first 58000 days as training\ntrain=datasetScaled[0:58000,:]\n#Use from 55000 to 58316 as testing set, \n#that means we will test on 3000 days we know the answer for, \n#leaving 316 that the algorithm has never seen!\ntest=datasetScaled[55000:,:]\n\nprint(\"Traing set is: \", train.shape)\nprint(\"Test set is: \", test.shape)\n\nTraing set is:  (58000, 1)\nTest set is:  (3316, 1)\n\n\n\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n# previous time steps to use as input variables to predict the next time period\nlook_back = 30 \n\n# reshape into X=t and Y=t+look_back\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n\n\n# create the LSTM network\n#The network has a visible layer with 1 input, \n#a hidden layer with 4 LSTM blocks or neurons, \n#and an output layer that makes a single value prediction. \n#The default sigmoid activation function is used for the LSTM blocks. \n#The network is trained for 4 epochs and a batch size of 1 is used.\n\nprint(\"Running model...\")\nmodel = tf.keras.models.Sequential()\n\nprint(\"Adding LSTM.\")\nmodel.add(tf.keras.layers.LSTM(4, input_shape=(look_back, 1)))\n\nprint(\"Adding dense.\")\nmodel.add(tf.keras.layers.Dense(1))\n\nprint(\"Compiling.\")\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nRunning model...\nAdding LSTM.\nAdding dense.\nCompiling.\n\n\n\n#Fit the model, this takes the longest time\nprint(\"fitting...\")\nstartT=time.time()\nmodel.fit(trainX, trainY, epochs=4, batch_size=30, verbose=1)\nendT=time.time()\n\nprint(\"Time taken: \", endT-startT)\n\nfitting...\nEpoch 1/4\n1933/1933 [==============================] - 8s 4ms/step - loss: 0.0168\nEpoch 2/4\n1933/1933 [==============================] - 7s 4ms/step - loss: 0.0066\nEpoch 3/4\n1933/1933 [==============================] - 7s 4ms/step - loss: 0.0063\nEpoch 4/4\n1933/1933 [==============================] - 7s 4ms/step - loss: 0.0060\nTime taken:  28.57616114616394\n\n\n2022-08-23 15:47:52.934724: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n\n\n\n#Save or load the model\n#model.save('kerasmodel.hdf5')\n#model = tf.keras.models.load_model('kerasmodel.hdf5')\n\n\n#make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n# invert and rescale predictions\ntrainPredicti = scaler.inverse_transform(trainPredict)\ntrainYi = scaler.inverse_transform([trainY])\ntestPredicti = scaler.inverse_transform(testPredict)\ntestYi = scaler.inverse_transform([testY])\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainYi[0], trainPredicti[:,0]))\nprint('Train Score: %.4f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testYi[0], testPredicti[:,0]))\nprint('Test Score: %.4f RMSE' % (testScore))\n\n1812/1812 [==============================] - 2s 866us/step\n103/103 [==============================] - 0s 861us/step\nTrain Score: 2.9189 RMSE\nTest Score: 2.9696 RMSE\n\n\n\n#PLOT the result\n\n#Create a dataset that is the same size as the testing/training set \ndummyfull=numpy.ones((datasetScaled.shape[0]-test.shape[0],1))*numpy.mean(testPredict)\nprint(dummyfull.shape,testPredicti.shape,datasetScaled.shape)\ntestvec = numpy.concatenate((dummyfull,testPredict))\n\n#Scale the data\ntransformer = MaxAbsScaler().fit(train[:])\ntestScale= transformer.transform(testvec)\n\nprint(trainPredict.shape,testPredict.shape,testvec.shape)\n\ntrain=datasetScaled[0:58000,:]\n#Use from 50000 to 58316 as testing set, \n#that means we will test on 8000 days we know the answer for, \n#leaving 316 that the algorithm has never seen!\ntest=datasetScaled[55000:,:]\n\nplt.plot(datasetScaled[:])\nplt.plot(train[:],'r')\nplt.plot(testScale[:],'k')\n\nplt.legend([\"All Data\",\"Training Data\",\"Predicted data\"])\nplt.xlabel(\"Day\")\nplt.ylabel(\"Scaled Temperature\")\nplt.xlim([54000,58500])\nplt.show()\n\n(55000, 1) (3285, 1) (58316, 1)\n(57969, 1) (3285, 1) (58285, 1)\n\n\n\n\n\nDepending on the environment you are working in, you will have to use a different combo of python/cuda/tensorflow/keras versions and GPU hardware. Check compatability for NVIDIA Drivers/CUDA, CUDA/Python/Tensorflow.\nIn Deep Learning, training the model can take a seriously long time, so we often only want to do this once and then tweak our model. In which case we can do that by saving out our data as as a *.hdf5 file.\n\nChallenge\nPick another site at http://www.bom.gov.au/climate/data/ and re-run the analysis.\n\n\nSolution\n\n…\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/02a-mapping.html",
    "href": "notebooks/02a-mapping.html",
    "title": "Mapping with Cartopy",
    "section": "",
    "text": "What plotting options are available in Pyton?\nWhat mapping features are availble in Python?\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/02a-mapping.html#introduction-to-matplotlib",
    "href": "notebooks/02a-mapping.html#introduction-to-matplotlib",
    "title": "Mapping with Cartopy",
    "section": "Introduction to matplotlib",
    "text": "Introduction to matplotlib\nA more general interface is available and a more structured approach to using matplotlib is helpful.\n\nPDF documentation\nOnline gallery / cookbook\n\nMatplotlib fully embraces the Python object-oriented model, but for some tasks the design of the object hierarchy is a little bit counter-intuitive. It’s best to find a common pattern for building plots and stick to it.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nfig = plt.figure(figsize=(6, 4), facecolor=\"none\")\n\nax = plt.subplot(111) # 1x1 array of plots, ax refers to the 1st of them\n\n# Content is added to the blank axes instance\n# ...\n\nfig.savefig(\"test-figure.png\", dpi=150)\nplt.show()  # happens anyway !\n\n\n\n\n\nfig = plt.figure(figsize=(6,6), facecolor=\"none\")\n\nax = plt.subplot(321)   # 3x2 array of plots, ax refers to the 1st of them\nax2 = plt.subplot(322) \n\nax3 = plt.subplot(323) \nax4 = plt.subplot(324) \n\nax5 = plt.subplot(325) \nax6 = plt.subplot(326) # 3x2 array of plots, ax6 refers to the last of them\n\n\n# Content is added to the blank axes instance\n# ...\n\nfig.savefig(\"test-figure2.png\", dpi=150)\nplt.show()  # happens anyway !\n\n\n\n\n\n# Demo example\n\n# First, define some synthetic data\n\nx1 = np.linspace(0.0, 5.0)\nx2 = np.linspace(0.0, 2.0)\n\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\ny2 = np.cos(2 * np.pi * x2)\n\n\n# Option 1\nplt.subplot(211)\nplt.plot(x1, y1, 'o-')\nplt.title('A tale of 2 subplots')\nplt.ylabel('Damped oscillation')\n\nplt.subplot(212)\nplt.plot(x2, y2, '.-')\nplt.xlabel('time (s)')\nplt.ylabel('Undamped')\n\nplt.show()\n\n\n\n\n\n# My preference - option 2\n# Set up the plot\n\nfig = plt.figure(figsize=(6,6), facecolor=\"none\")\nax  = plt.subplot(211)   # 2s1 array of plots, ax refers to the 1st of them\nax2 = plt.subplot(212) \n\nax.plot(x1, y1, 'o-')\nax.set_title('A tale of 2 subplots')\nax.set_ylabel('Damped oscillation')\n\nax2.plot(x2, y2, '.-')\nax2.set_xlabel('time (s)')\nax2.set_ylabel('Not Damped')\nplt.show()\n\n\n\n\n\n\"\"\"\n==================\nggplot style sheet\n==================\n\nThis example demonstrates the \"ggplot\" style, which adjusts the style to\nemulate ggplot_ (a popular plotting package for R_).\n\nThese settings were shamelessly stolen from [1]_ (with permission).\n\n.. [1] http://www.huyng.com/posts/sane-color-scheme-for-matplotlib/\n\n.. _ggplot: http://ggplot2.org/\n.. _R: https://www.r-project.org/\n\n\"\"\"\n\nplt.style.use('ggplot')\n\n\n# This is another way to set up the axes objects\n# and may be preferable, but whichever - choose one and stick with it !\n\nfig, axes = plt.subplots(ncols=2, nrows=2)\nax1, ax2, ax3, ax4 = axes.ravel()\nfig.set_size_inches((8,8))\n\n# scatter plot (Note: `plt.scatter` doesn't use default colors)\nx, y = np.random.normal(size=(2, 200))\nax1.plot(x, y, 'o')\n\n# sinusoidal lines with colors from default color cycle\nL = 2*np.pi\nx = np.linspace(0, L)\nncolors = len(plt.rcParams['axes.prop_cycle'])\nshift = np.linspace(0, L, ncolors, endpoint=False)\nfor s in shift:\n    ax2.plot(x, np.sin(x + s), '-')\nax2.margins(0)\n\n# bar graphs\nx = np.arange(5)\ny1, y2 = np.random.randint(1, 25, size=(2, 5))\nwidth = 0.25\nax3.bar(x, y1, width)\nax3.bar(x + width, y2, width,\n        color=list(plt.rcParams['axes.prop_cycle'])[2]['color'])\nax3.set_xticks(x + width)\nax3.set_xticklabels(['a', 'b', 'c', 'd', 'e'])\n\n# circles with colors from default color cycle\nfor i, color in enumerate(plt.rcParams['axes.prop_cycle']):\n    xy = np.random.normal(size=2)\n    ax4.add_patch(plt.Circle(xy, radius=0.3, color=color['color']))\nax4.axis('equal')\nax4.margins(0)\n\nplt.show()\n\n\n\n\n\n\nfrom matplotlib.colors import LightSource, Normalize\n\n\ndef display_colorbar():\n    \"\"\"Display a correct numeric colorbar for a shaded plot.\"\"\"\n    y, x = np.mgrid[-4:2:200j, -4:2:200j]\n    z = 10 * np.cos(x**2 + y**2)\n\n    cmap = plt.cm.copper\n    ls = LightSource(315, 45)\n    rgb = ls.shade(z, cmap)\n\n    fig, ax = plt.subplots()\n    ax.imshow(rgb, interpolation='bilinear')\n\n    # Use a proxy artist for the colorbar...\n    im = ax.imshow(z, cmap=cmap)\n    im.remove()\n    fig.colorbar(im)\n\n    ax.set_title('Using a colorbar with a shaded plot', size='x-large')\n\n\ndef avoid_outliers():\n    \"\"\"Use a custom norm to control the displayed z-range of a shaded plot.\"\"\"\n    y, x = np.mgrid[-4:2:200j, -4:2:200j]\n    z = 10 * np.cos(x**2 + y**2)\n\n    # Add some outliers...\n    z[100, 105] = 2000\n    z[120, 110] = -9000\n\n    ls = LightSource(315, 45)\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4.5))\n\n    rgb = ls.shade(z, plt.cm.copper)\n    ax1.imshow(rgb, interpolation='bilinear')\n    ax1.set_title('Full range of data')\n\n    rgb = ls.shade(z, plt.cm.copper, vmin=-10, vmax=10)\n    ax2.imshow(rgb, interpolation='bilinear')\n    ax2.set_title('Manually set range')\n\n    fig.suptitle('Avoiding Outliers in Shaded Plots', size='x-large')\n\n\ndef shade_other_data():\n    \"\"\"Demonstrates displaying different variables through shade and color.\"\"\"\n    y, x = np.mgrid[-4:2:200j, -4:2:200j]\n    z1 = np.sin(x**2)  # Data to hillshade\n    z2 = np.cos(x**2 + y**2)  # Data to color\n\n    norm = Normalize(z2.min(), z2.max())\n    cmap = plt.cm.RdBu\n\n    ls = LightSource(315, 45)\n    rgb = ls.shade_rgb(cmap(norm(z2)), z1)\n\n    fig, ax = plt.subplots()\n    ax.imshow(rgb, interpolation='bilinear')\n    ax.set_title('Shade by one variable, color by another', size='x-large')\n\ndisplay_colorbar()\navoid_outliers()\nshade_other_data()\nplt.show()\n\n/var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/ipykernel_36732/2991489525.py:19: MatplotlibDeprecationWarning: Starting from Matplotlib 3.6, colorbar() will steal space from the mappable's axes, rather than from the current axes, to place the colorbar.  To silence this warning, explicitly pass the 'ax' argument to colorbar().\n  fig.colorbar(im)\n\n\n\n\n\n\n\n\n\n\n\n\n\"\"\"\nDemonstrates the visual effect of varying blend mode and vertical exaggeration\non \"hillshaded\" plots.\n\nNote that the \"overlay\" and \"soft\" blend modes work well for complex surfaces\nsuch as this example, while the default \"hsv\" blend mode works best for smooth\nsurfaces such as many mathematical functions.\n\nIn most cases, hillshading is used purely for visual purposes, and *dx*/*dy*\ncan be safely ignored. In that case, you can tweak *vert_exag* (vertical\nexaggeration) by trial and error to give the desired visual effect. However,\nthis example demonstrates how to use the *dx* and *dy* kwargs to ensure that\nthe *vert_exag* parameter is the true vertical exaggeration.\n\"\"\"\n\nfrom matplotlib.cbook import get_sample_data\nfrom matplotlib.colors import LightSource\n\ndem = np.load(get_sample_data('jacksboro_fault_dem.npz', asfileobj= False))\nz = dem['elevation']\n\n#-- Optional dx and dy for accurate vertical exaggeration --------------------\n# If you need topographically accurate vertical exaggeration, or you don't want\n# to guess at what *vert_exag* should be, you'll need to specify the cellsize\n# of the grid (i.e. the *dx* and *dy* parameters).  Otherwise, any *vert_exag*\n# value you specify will be relative to the grid spacing of your input data\n# (in other words, *dx* and *dy* default to 1.0, and *vert_exag* is calculated\n# relative to those parameters).  Similarly, *dx* and *dy* are assumed to be in\n# the same units as your input z-values.  Therefore, we'll need to convert the\n# given dx and dy from decimal degrees to meters.\ndx, dy = dem['dx'], dem['dy']\ndy = 111200 * dy\ndx = 111200 * dx * np.cos(np.radians(dem['ymin']))\n#-----------------------------------------------------------------------------\n\n# Shade from the northwest, with the sun 45 degrees from horizontal\nls = LightSource(azdeg=315, altdeg=45)\ncmap = plt.cm.gist_earth\n\nfig, axes = plt.subplots(nrows=4, ncols=3, figsize=(8, 9))\nplt.setp(axes.flat, xticks=[], yticks=[])\n\n# Vary vertical exaggeration and blend mode and plot all combinations\nfor col, ve in zip(axes.T, [0.1, 1, 10]):\n    # Show the hillshade intensity image in the first row\n    col[0].imshow(ls.hillshade(z, vert_exag=ve, dx=dx, dy=dy), cmap='gray')\n\n    # Place hillshaded plots with different blend modes in the rest of the rows\n    for ax, mode in zip(col[1:], ['hsv', 'overlay', 'soft']):\n        rgb = ls.shade(z, cmap=cmap, blend_mode=mode,\n                       vert_exag=ve, dx=dx, dy=dy)\n        ax.imshow(rgb)\n\n# Label rows and columns\nfor ax, ve in zip(axes[0], [0.1, 1, 10]):\n    ax.set_title('{0}'.format(ve), size=18)\nfor ax, mode in zip(axes[:, 0], ['Hillshade', 'hsv', 'overlay', 'soft']):\n    ax.set_ylabel(mode, size=18)\n\n# Group labels...\naxes[0, 1].annotate('Vertical Exaggeration', (0.5, 1), xytext=(0, 30),\n                    textcoords='offset points', xycoords='axes fraction',\n                    ha='center', va='bottom', size=20)\naxes[2, 0].annotate('Blend Mode', (0, 0.5), xytext=(-30, 0),\n                    textcoords='offset points', xycoords='axes fraction',\n                    ha='right', va='center', size=20, rotation=90)\nfig.subplots_adjust(bottom=0.05, right=0.95)\n\nplt.show()\n\n# note - if you get a sample data not found error here run\n#  conda install -c conda-forge mpl_sample_data \n# after activating your geopy environment"
  },
  {
    "objectID": "notebooks/02a-mapping.html#cartopy",
    "href": "notebooks/02a-mapping.html#cartopy",
    "title": "Mapping with Cartopy",
    "section": "Cartopy",
    "text": "Cartopy\nIs a mapping and imaging package originating from the Met. Office in the UK. The home page for the package is http://scitools.org.uk/cartopy/. Like many python packages, the documentation is patchy and the best way to learn is to try to do things and ask other people who have figured out this and that.\nWe are going to work through a number of the examples and try to extend them to do the kinds of things you might find interesting and useful in the future. The examples are in the form of a gallery\nYou might also want to look at the list of map projections from time to time. Not all maps can be plotted in every projection (sometimes because of bugs and sometimes because they are not supposed to work for the data you have) but you can try them and see what happens.\nCartopy is built on top of a lot of the matplotlib graphing tools. It works by introducing a series of projections associated with the axes of a graph. On top of that there is a big toolkit for reading in images, finding data from standard web feeds, and manipulating geographical objects. Many, many libraries are involved and sometimes things break. Luckily the installation that is built for this course is about as reliable as we can ever get. I’m just warning you, though, that it can be quite tough if you want to put this on your laptop from scratch.\nWe have a number of imports that we will need almost every time.\nIf we are going to plot anything then we need to include matplotlib.\n\nimport cartopy\nimport cartopy.crs as ccrs\n\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nax = plt.axes(projection=ccrs.PlateCarree())\nax.stock_img()\nax.coastlines()\nplt.show()\n\n\n\n\nThe simplest plot: global map using the default image built into the package and adding coastlines\n\n\nfig = plt.figure(figsize=(12, 12))\nax  = plt.axes(projection=ccrs.Mercator())\n\n    # make the map global rather than have it zoom in to\n    # the extents of any plotted data\n    \nax.set_global()\nax.coastlines()  \nax.stock_img()\n\nplt.show()\n\n\n\n\nTry changing the projection - either look at the list in the link I gave you above or use the tab-completion feature of iPython to see what ccrs has available ( not everything will be a projection, but you can see what works and what breaks ).\nHere is how you can plot a region instead of the globe:\n\nfig = plt.figure(figsize=(12, 12))\nax  = plt.axes(projection=ccrs.Robinson())    \nax.set_extent([0, 40, 28, 48])\n\nax.coastlines(resolution='50m')  \nax.stock_img()\nplt.show()\n\n\n\n\n\nhelp(ax.stock_img)\n\nHelp on method stock_img in module cartopy.mpl.geoaxes:\n\nstock_img(name='ne_shaded') method of cartopy.mpl.geoaxes.GeoAxesSubplot instance\n    Add a standard image to the map.\n    \n    Currently, the only (and default) option is a downsampled version of\n    the Natural Earth shaded relief raster.\n\n\n\n\nChallenge\n\nMake a Mollweide plot of Australia.\n\n\n\nSolution\n\nfig = plt.figure(figsize=(12, 12), facecolor=\"none\")\nax  = plt.axes(projection=ccrs.Mollweide(central_longitude=130))\nax.set_extent([112, 153, -43, -10]) \n\nax.coastlines(resolution='50m')  \nax.stock_img()\nplt.show()\n\n\n\nNetCDF data and more Cartopy\n\n#We use the scipy netcdf reader\nimport scipy.io\n\n\n#Set the file name and read in the data\nfilename=\"../data/topodata.nc\"\ndata = scipy.io.netcdf.netcdf_file(filename,'r')\n\n#Netcdf could be stored with multiple different formats\n#Print out these data\ndata.variables\n\n/var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/ipykernel_36732/1104130496.py:3: DeprecationWarning: Please use `netcdf_file` from the `scipy.io` namespace, the `scipy.io.netcdf` namespace is deprecated.\n  data = scipy.io.netcdf.netcdf_file(filename,'r')\n\n\n{'X': &lt;scipy.io._netcdf.netcdf_variable at 0x11adc3e80&gt;,\n 'Y': &lt;scipy.io._netcdf.netcdf_variable at 0x11af4fe80&gt;,\n 'elev': &lt;scipy.io._netcdf.netcdf_variable at 0x11fccb940&gt;}\n\n\n\n#Set each layer as a variable\nworldbath=data.variables['elev'][:]\nlons=data.variables['X'][:]\nlats=data.variables['Y'][:]\n\n\nprint(worldbath.shape)\nprint(lons.shape)\nprint(lats.shape)\n\n(2160, 4320)\n(4320,)\n(2160,)\n\n\n\n#A quick plot of the main dataset\nplt.pcolormesh(worldbath)\nplt.show()\n\n\n\n\nmatplotlib lib has no native understaning of geocentric coordinate systems. Cartopy does.\nmatplotlib commands with “x” and “y” values of latitudes and longitudes make sense too!\n\n#A more earthly plot of the data\nfig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection=ccrs.Orthographic(-10, 45))\nax.pcolormesh(lons,lats,worldbath, transform=ccrs.PlateCarree())\nplt.show()\n\n\n\n\nThis is fairly high resolution for what we are looking at, so we could downsample the grid to save some time. Also, not all grids are global, so we can subset the data to represent perhaps a small area we may have a grid/image over.\n\n#Take a subset of the data by slicing the array with indexing\nsubbath=worldbath[1200:1650:2,1300:1900:2]\n#Take the corresponding lat/lon values as the main data array\nsublons=lons[1300:1900:2]\nsublats=lats[1200:1650:2]\n\n\n#Make the figure object\nfig = plt.figure(figsize=(8, 8),dpi=300)\n\n#Create the main plot axes\nax = plt.axes(projection=ccrs.Mollweide(central_longitude=130))\n\n#Plot the data on the axes\nmapdat=ax.pcolormesh(sublons,sublats,subbath, \n              transform=ccrs.PlateCarree(),cmap=plt.cm.gist_earth,vmax=1000,vmin=-6000)\n\n#Add background oceans, land\n#ax.add_feature(cartopy.feature.OCEAN, zorder=0)\n#ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='blue')  \n\n#Add Gridlines\ngl=ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n             linewidth=0.1, color='gray', alpha=0.5, linestyle='-',\n             x_inline=False, y_inline=False)\n\ngl.top_labels = False\ngl.bottom_labels = True\ngl.right_labels = False\ngl.left_labels = True\n\ngl.xlabel_style = {'size': 8, 'color': 'gray'}\ngl.ylabel_style = {'size': 8, 'color': 'gray'}\n\n\n#Add a colorbar axes\ncbaxes = fig.add_axes([0.60, 0.22, 0.2, 0.015],frameon=True,facecolor='white')\n\n#Add the colorbar to the axes with the data\ncbar = plt.colorbar(mapdat, cax = cbaxes,orientation=\"horizontal\",extend='both')\n\n#Fix additional options of the colorbar\ncbar.set_label('Height (m)', labelpad=-40,fontsize=8,color='white')\ncbar.ax.set_xticklabels([-5000,-2500,0,1000],color='white',fontsize=8)\n\n#Show the plot!\nplt.show()\n\n/var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/ipykernel_36732/346796257.py:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  cbar.ax.set_xticklabels([-5000,-2500,0,1000],color='white',fontsize=8)\n\n\n\n\n\n\n\nChallenge\n\nRe-make the last map, but with the full topo dataset.\nCrop the image.\nAdd coastlines back in.\nPick a different color scale.\nMove the colorbar to the bottom left.\n\n\n\nSolution\n\n#Still make the subsets, but at full res, otherwise plot takes long time\nsubbath=worldbath[1200:1650,1300:1900]\nsublons=lons[1300:1900]\nsublats=lats[1200:1650]\n\n#Make the figure object\nfig = plt.figure(figsize=(8, 8),dpi=300)\n\n#Create the main plot axes\nax = plt.axes(projection=ccrs.Mollweide(central_longitude=130))\n\n#Plot the data on the axes\nmapdat=ax.pcolormesh(sublons,sublats,subbath, \n              transform=ccrs.PlateCarree(),cmap=plt.cm.terrain,vmax=1000,vmin=-6000)\n\n#Add coastlines, restrict the image to an extent\nax.coastlines(resolution='50m',color='black')\nax.set_extent([112, 153, -43, -10])    \n\n#Add Gridlines\ngl=ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n             linewidth=0.1, color='gray', alpha=0.5, linestyle='-',\n             x_inline=False, y_inline=False)\n\ngl.top_labels = True\ngl.bottom_labels = False\ngl.right_labels = False\ngl.left_labels = True\ngl.xlabel_style = {'size': 8, 'color': 'gray'}\ngl.ylabel_style = {'size': 8, 'color': 'gray'}\n\n#Add a colorbar axes\ncbaxes = fig.add_axes([0.20, 0.22, 0.2, 0.015],frameon=True,facecolor='white')\n\n#Add the colorbar to the axes with the data\ncbar = plt.colorbar(mapdat, cax = cbaxes,orientation=\"horizontal\",extend='both')\n\n#Fix additional options of the colorbar\ncbar.set_label('Height (m)', labelpad=-40,fontsize=8,color='white')\ncbar.ax.set_xticklabels([-5000,-2500,0,1000],color='white',fontsize=8)\n\n#Show the plot!\nplt.show()"
  },
  {
    "objectID": "notebooks/02b-Clustering.html",
    "href": "notebooks/02b-Clustering.html",
    "title": "Numerical analysis",
    "section": "",
    "text": "What computational/numercal techniques can be performed in Python?\nWhat tools are availble for analysis?\nHow can I learn more information about my data?\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/02b-Clustering.html#numerical-models",
    "href": "notebooks/02b-Clustering.html#numerical-models",
    "title": "Numerical analysis",
    "section": "Numerical models",
    "text": "Numerical models\nWe start with the numerical solution of a very simple differential equation. In fact we choose something simple enough that we already know the answer.\n\n\\(\\frac{d\\theta}{dt} = - k \\theta\\)\n\nThis is the equation which governs radioactive decay, in which case \\(\\theta\\) is the amount of the radioactive isotope remaining and \\(d\\theta / dt\\) is the activity that we can measure. \\(k\\) is closely related to the half life.\nThe solution to this equation is\n\n\\(\\theta(t) = \\theta_0 e^{-kt}\\)\n\nwhere \\(\\theta_0\\) is the amount of the radioactive material remaining. The same equation also describes the cooling of, say, a cup of coffee. In this case we interpret \\(\\theta\\) as the excess temperature (above room temperature).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Set the amount of isotope remaining (i.e. 1=100%)\ntheta_0 = 1.0\n\n#Create a regularly spaced vector of time values\n#20 units between 0 and 1.\nsteps=20\ntimelength=1.0\ntime_values = np.linspace(0,timelength,steps)\n\n#Try several different values for the half-life, k\nfor const_k in [1.0, 3.1, 10.0, 31, 100.0]:\n    #This is the solution to the decay equation, how much isotope remains.\n    exact_theta_values = theta_0 * np.exp(-const_k * time_values)\n    #Plot the results for different k values\n    plt.plot(time_values, exact_theta_values)\n    \n#Add some plot flair    \nplt.legend(('1', '3.1', '10', '31', '1000'),title=r'$k$')\nplt.xlabel('time')\nplt.ylabel(r'$\\theta$')\nplt.show()\n\n\n\n\nWe want to be able to march forward in time from our starting point (just like the picture above) where \\(\\theta = \\theta_0\\) to obtain the value of \\(\\theta\\) at later times. To do this, we need to approximate the original differential equation, and, in particular, the value of the time derivative at each time. There are a number of ways to do this.\n\nFirst order numerical approximation\nAssume that the variation in \\(\\theta(t)\\) is linear, i.e.\n\n\\(\\theta(t') = \\theta_n + \\beta t'\\)\n\nwhere we use a local time coordinate \\(t' = t - n\\Delta t\\), so that when we differentiate\n\n\\(\\frac{d \\theta}{dt} = \\beta\\)\n\nTo determine the approximation for the derivative therefore becomes the solution to the following equation:\n\n\\(\\theta_{n+1} = \\theta_n + \\beta \\Delta t\\)\n\\(\\Rightarrow \\beta = \\frac{d \\theta}{dt} = \\frac{\\theta_{n+1} - \\theta_n}{\\Delta t}\\)\n\nThis is a first order difference expression for the derivative which we substitute into the original differential equation for radioactive decay at the current timestep\n\n\\(\\frac{\\theta_{n+1} - \\theta_n}{\\Delta t} = - k \\theta_n\\)\n\nThis rearranges to give us a time-marching algorithm:\n\n\\(\\theta_{n+1} = \\theta_n (1-k \\Delta t)\\)\n\nIn a moment we will compute some values for this expression to see how accurate it is.\n\n#Set the known constant values\ntheta_0 = 1.0\nconst_k = 10.0\n#How many timesteps to solve\nsteps = 20\ntimelength = 1.0\ndelta_t = timelength / steps\n\n#Create a regularly spaced vector of time values\ntime_values = np.linspace(0,timelength,steps)\n\n#Create an empty array to store the solutions\ntheta_values = np.zeros(steps)\n\n#Set the starting values\ntheta_values[0] = theta_0\n\n#Step through the time values\nfor i in range(1, steps):\n    #Find the value for theta at this time step\n    theta_values[i] = theta_values[i-1] * (1 - const_k * delta_t)\n\n#Compare with the exact solution\nexact_theta_values = theta_0 * np.exp(-const_k * time_values)\n\n#Plot and compare your results\nplt.plot(time_values, theta_values, linewidth=3.0, color=\"red\")\nplt.plot(time_values, exact_theta_values, 'b-')\n\n\n\n\n\n\nSecond Order Runge-Kutta\nThe Runge-Kutta method can be a more accurate approach to higher order integration solutions. The idea is to estimate the gradient \\(d / d t\\) at the half way point between two timestep values. This is done in two stages. Initially a first order estimate, \\( \\) is made for the value of the function \\( \\) at \\(t=t+t /2\\) in the future. This value is then subsituted into the differential equation to obtain the estimate for the gradient at this time. The revised gradient is then used to update the original \\((t)\\) by an entire timestep.\nThe first order step is \\[\n        \\begin{split}\n        \\hat{\\theta}(t+\\Delta t /2) & = \\theta(t) + \\left.  \\frac{d \\theta}{d t} \\right|_t \\frac{\\Delta t}{2} \\\\\n         &= \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right]\n        \\end{split}\n\\]\nSubstitute to estimate the gradient at the mid-point \\[\n    \\left. \\frac{d \\theta}{d t} \\right|_{t+\\Delta t /2} \\approx -k \\theta(t)  \\left[ 1-\\frac{k\\Delta t}{2} \\right]\n\\]\nUse this value as the average gradient over the interval \\(t\\rightarrow t+\\Delta t\\) to update \\(\\)\n\n\\(\\theta(t+\\Delta t) \\approx \\theta(t) + \\delta t \\left( -k \\theta(t) \\left[ 1-\\frac{k\\Delta t}{2} \\right] \\right)\\)\n\nWhich gives the Second Order Runge-Kutta estimation for updated \\(\\theta\\) values:\n\n\\(\\theta(t+\\Delta t) \\approx \\theta(t) \\left( 1 - k \\Delta t + k^2 \\frac{\\Delta t^2}{2} \\right)\\)\n\n\n\nChallenge\n\nCan you implement the Second Order Runge-Kutta Numerical Solution in Python? Start as before:\n\n#Set the known constant values\ntheta_0 = 1.0\nconst_k = 10.0\n#How many timesteps to solve\nsteps = 20\ntimelength = 1.0\ndelta_t = timelength / steps\n\n#Create a regularly spaced vector of time values\ntime_values = np.linspace(0,timelength,steps)\n\n#Create an empty array to store the solutions\ntheta_values = np.zeros(steps)\n\n#Set the starting values\ntheta_values[0] = theta_0\n\n#Step through the time values\nfor i in range(1, steps):\n    #Find the value for theta at this time step\n    theta_values[????] = ????\n\n#Compare with the exact solution\nexact_theta_values = theta_0 * np.exp(-const_k * time_values)\n\n#Plot and compare your results\nplt.plot(time_values, theta_values, linewidth=3.0, color=\"red\")\nplt.plot(time_values, exact_theta_values, 'b-')\n\n\nSolution\n\nThis is my solution\ntheta_0 = 1.0\nconst_k = 10.0\nsteps = 20\ntimelength = 1.0\ndelta_t = timelength / steps\n\ntime_values = np.linspace(0,timelength,steps)\n\ntheta_values = np.zeros(steps)\n\ntheta_values[0] = theta_0\n\nfor i in range(1, steps):\n    theta_values[i] = theta_values[i-1] * (1 - const_k * delta_t + const_k**2 * delta_t**2 / 2.0)\n\nexact_theta_values = theta_0 * np.exp(-const_k * time_values)\n\nplt.plot(time_values, theta_values, linewidth=3.0, color=\"red\")\nplt.plot(time_values, exact_theta_values, 'b-')"
  },
  {
    "objectID": "notebooks/02b-Clustering.html#scipy.interpolate",
    "href": "notebooks/02b-Clustering.html#scipy.interpolate",
    "title": "Numerical analysis",
    "section": "scipy.interpolate",
    "text": "scipy.interpolate\nThis module provides general interpolation capability for data in 1, 2, and higher dimensions. This list of features is from the documentation:\n\nA class representing an interpolant (interp1d) in 1-D, offering several interpolation methods.\nConvenience function griddata offering a simple interface to interpolation in N dimensions (N = 1, 2, 3, 4, …). Object-oriented interface for the underlying routines is also available.\nFunctions for 1- and 2-dimensional (smoothed) cubic-spline interpolation, based on the FORTRAN library FITPACK. There are both procedural and object-oriented interfaces for the FITPACK library.\nInterpolation using Radial Basis Functions.\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n1D data\n\nfrom scipy.interpolate import interp1d\n\n\nx = np.linspace(0, 10, num=11, endpoint=True)\ny = np.cos(-x**2/9.0)\nf = interp1d(x, y, kind='linear') # default if kind=None\nf2 = interp1d(x, y, kind='cubic')\nf3 = interp1d(x, y, kind='nearest')\n\n\nxnew = np.linspace(0, 10, num=41, endpoint=True)\nplt.plot(x, y, 'o', xnew, f(xnew), '-', xnew, f2(xnew), '--', xnew, f3(xnew), '.-')\nplt.legend(['data', 'linear', 'cubic', 'nearest'], loc='best')\nplt.show()\n\n\n\n\n\n\nnD data\nThere are fewer approaches to n-dimensional data, the evaluation for arbitrary dimensions is always for points on an n dimensional grid.\n\nfrom scipy.interpolate import griddata\n\ndef func(x, y):\n    return x*(1-x)*np.cos(4*np.pi*x) * np.sin(4*np.pi*y**2)**2\n\n\n# A regular grid array of x,y coordinates\n\ngrid_x, grid_y = np.mgrid[0:1:100j, 0:1:200j] # see np.info(np.mgrid) for an explanation of the 200j !!\n\n\nnp.info(np.mgrid)\n\n`nd_grid` instance which returns a dense multi-dimensional \"meshgrid\".\n\nAn instance of `numpy.lib.index_tricks.nd_grid` which returns an dense\n(or fleshed out) mesh-grid when indexed, so that each returned argument\nhas the same shape.  The dimensions and number of the output arrays are\nequal to the number of indexing dimensions.  If the step length is not a\ncomplex number, then the stop is not inclusive.\n\nHowever, if the step length is a **complex number** (e.g. 5j), then\nthe integer part of its magnitude is interpreted as specifying the\nnumber of points to create between the start and stop values, where\nthe stop value **is inclusive**.\n\nReturns\n-------\nmesh-grid `ndarrays` all of the same dimensions\n\nSee Also\n--------\nlib.index_tricks.nd_grid : class of `ogrid` and `mgrid` objects\nogrid : like mgrid but returns open (not fleshed out) mesh grids\nr_ : array concatenator\n\nExamples\n--------\n&gt;&gt;&gt; np.mgrid[0:5, 0:5]\narray([[[0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2],\n        [3, 3, 3, 3, 3],\n        [4, 4, 4, 4, 4]],\n       [[0, 1, 2, 3, 4],\n        [0, 1, 2, 3, 4],\n        [0, 1, 2, 3, 4],\n        [0, 1, 2, 3, 4],\n        [0, 1, 2, 3, 4]]])\n&gt;&gt;&gt; np.mgrid[-1:1:5j]\narray([-1. , -0.5,  0. ,  0.5,  1. ])\n\n\n\n# A random sampling within the same area\npoints = np.random.rand(1000, 2)\nvalues = func(points[:,0], points[:,1])\n\n# Resample from the values at these points onto the regular mesh\ngrid_z0 = griddata(points, values, (grid_x, grid_y), method='nearest')\ngrid_z1 = griddata(points, values, (grid_x, grid_y), method='linear')\ngrid_z2 = griddata(points, values, (grid_x, grid_y), method='cubic')\n\n\nplt.subplot(221)\nplt.imshow(func(grid_x, grid_y).T, extent=(0,1,0,1), origin='lower', cmap='jet')\nplt.plot(points[:,0], points[:,1], 'k.', ms=1)\nplt.title('Original')\nplt.subplot(222)\nplt.imshow(grid_z0.T, extent=(0,1,0,1), origin='lower', cmap='jet')\nplt.title('Nearest')\nplt.subplot(223)\nplt.imshow(grid_z1.T, extent=(0,1,0,1), origin='lower', cmap='jet')\nplt.title('Linear')\nplt.subplot(224)\nplt.imshow(grid_z2.T, extent=(0,1,0,1), origin='lower', cmap='jet')\nplt.title('Cubic')\nplt.gcf().set_size_inches(6, 6)\nplt.show()\n\n\n\n\n\n\nSplines\nWhich have the added benefit of giving smooth derivative information\n\nfrom scipy.interpolate import splrep, splev\n\n\nx = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/8)\ny = np.sin(x)\ntck = splrep(x, y, s=0)\nxnew = np.arange(0, 2*np.pi, np.pi/50)\nynew = splev(xnew, tck, der=0)\nyder = splev(xnew, tck, der=1)\n\n\nplt.figure()\nplt.plot(x, y, 'x', xnew, ynew, xnew, np.sin(xnew), x, y, 'b')\nplt.legend(['Linear', 'Cubic Spline', 'True'])\nplt.axis([-0.05, 6.33, -1.05, 1.05])\nplt.title('Cubic-spline interpolation')\nplt.show()\n\n\n\n\n\nplt.figure()\nplt.plot(xnew, yder, xnew, np.cos(xnew),'--')\nplt.legend(['Cubic Spline', 'True'])\nplt.axis([-0.05, 6.33, -1.05, 1.05])\nplt.title('Derivative estimation from spline')\nplt.show()\n\n\n\n\n2D splines are also available\n\nfrom scipy.interpolate import bisplrep, bisplev\n\n# Gridded function (at low resolution ... doesn't need to be gridded data here)\n\nx, y = np.mgrid[-1:1:20j, -1:1:20j]\nz = (x+y) * np.exp(-6.0*(x*x+y*y))\n\nplt.figure()\nplt.pcolor(x, y, z, cmap='jet',shading='auto')\nplt.colorbar()\nplt.title(\"Sparsely sampled function.\")\nplt.show()\n\n\n\n\n\nxnew, ynew = np.mgrid[-1:1:70j, -1:1:70j]\n\n## Create the spline-representation object tck\n\ntck = bisplrep(x, y, z, s=0)\nznew = bisplev(xnew[:,0], ynew[0,:], tck)\n\n\nplt.figure()\nplt.pcolor(xnew, ynew, znew, cmap='jet',shading='auto')\nplt.colorbar()\nplt.title(\"Interpolated function.\")\nplt.show()\n\n\n\n\n\n\nSee also\n\nRadial basis function interpolation for scattered data in n dimensions (slow for large numbers of points): from scipy.interpolate import Rbf\nscipy.ndimage for fast interpolation operations on image-like arrays\nB-splines on regular arrays are found in the scipy.signal module"
  },
  {
    "objectID": "notebooks/02b-Clustering.html#clustering-data-with-scikit-learn",
    "href": "notebooks/02b-Clustering.html#clustering-data-with-scikit-learn",
    "title": "Numerical analysis",
    "section": "Clustering data with scikit-learn",
    "text": "Clustering data with scikit-learn\nHere we want to explore a neat and efficient way of exploring a (seisimic tomography) dataset in Python. We will be using a Machine Learning algorithm known as K-Means clustering.\nData is from: Li, C., van der Hilst, R. D., Engdahl, E. R., and Burdick, S. (2008), A new global model for P wave speed variations in Earth’s mantle, Geochem. Geophys. Geosyst., 9, Q05018, doi:10.1029/2007GC001806\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits.mplot3d import Axes3D\n\nLoad in the tomography data set. Assign the column vectors to unique variables (for clarity).\n\ntomo=np.loadtxt('../data/ggge1202-sup-0002-ds01.txt', skiprows=1)\n\nlat=tomo[:,0]\nlon=tomo[:,1]\ndepth=tomo[:,2]\ndvp=tomo[:,3]\n\nNow run the clustering algorithm\n\nkmeans = KMeans(n_clusters=5, random_state=0).fit(dvp.reshape(-1, 1))\n\n#When completed, check the clusters the algorithm has identified.\nprint(kmeans.cluster_centers_)\n\n[[-0.2770305 ]\n [ 3.92536372]\n [ 0.25228711]\n [-0.03758588]\n [ 1.82168895]]\n\n\nNote, many functions have been “parallelised” and tuned to best take advantage of your computer, see e.g. for more details https://scikit-learn.org/stable/modules/computing.html#parallelism\nYou can have a look what the labels look like. It is essentially a vector the same length as the data indicating which label it has classified.\n\nkmeans.labels_\n\narray([4, 4, 4, ..., 3, 3, 3], dtype=int32)\n\n\nChoose one of the clusters to visualise, so subset the data into new vectors accordingly\n\ncentre=3\n\nlatClust=lat[kmeans.labels_==centre]\nlonClust=lon[kmeans.labels_==centre]\ndepthClust=depth[kmeans.labels_==centre]\ndvpClust=dvp[kmeans.labels_==centre]\n\nFinally, plot the results!\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(lonClust, latClust, -depthClust, c=dvpClust);\n\n\n\n\n\nKey points\n\nYou can use Python to solve math equations\nscipy for interpolation\nsklearn for clustering\nNew ways to plot data\nRead the docs to learn more"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html",
    "href": "notebooks/03a-MachineLearning.html",
    "title": "Machine Learning for Geoscience",
    "section": "",
    "text": "What data science tools and techniques can be used in Python?\nHow do I do it?\nLet’s use some standard Machine Learning tools available in Python packages to analyse some data.\nWe have a dataset (from Butterworth et al. 2016) with a collection of tectonomagmatic parameters associated with the time and location of porphyry copper deposits. We want to determine which of these (if any) parameters are geologically important (or at least statistically significant) in relation to the formation of porphyry coppers.\nBelow is an animation of the tectonomagmatic evolution of the South American plate margin since 150Ma, representing many of the parameters in the data.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#import-most-of-the-modules-we-need",
    "href": "notebooks/03a-MachineLearning.html#import-most-of-the-modules-we-need",
    "title": "Machine Learning for Geoscience",
    "section": "Import most of the modules we need",
    "text": "Import most of the modules we need\nBy convention module loads go at the top of your workflows.\n\nimport pandas #For dealing with data structures\nimport numpy as np #Data array manipulation\nimport scipy #Scientific Python, has lots of useful tools\nimport scipy.io #A specific sub-module for input/output of sci data\n\n#scikit-learn tools to perform machine learning classification\n#from sklearn import cross_validation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\n\n#For making pretty figures\nimport matplotlib.pyplot as plt \nfrom matplotlib import cm\n\n#For easy geographic projections on a map\nimport cartopy.crs as ccrs\n\n#For dealing with shapefiles\nimport shapefile"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#load-in-the-data",
    "href": "notebooks/03a-MachineLearning.html#load-in-the-data",
    "title": "Machine Learning for Geoscience",
    "section": "Load in the data",
    "text": "Load in the data\n\n#Use pandas to load in the machine learning dataset\nml_data=pandas.read_csv(\"../data/ml_data_points.csv\",index_col=0)\n\n\n#Print out the dataset so we can see what it looks like\nml_data\n\n\n\n\n\n\n\n\n0 Present day longitude (degrees)\n1 Present day latitude (degrees)\n2 Reconstructed longitude (degrees)\n3 Reconstructed latitude (degrees)\n4 Age (Ma)\n5 Time before mineralisation (Myr)\n6 Seafloor age (Myr)\n7 Segment length (km)\n8 Slab length (km)\n9 Distance to trench edge (km)\n...\n11 Subducting plate parallel velocity (km/Myr)\n12 Overriding plate normal velocity (km/Myr)\n13 Overriding plate parallel velocity (km/Myr)\n14 Convergence normal rate (km/Myr)\n15 Convergence parallel rate (km/Myr)\n16 Subduction polarity (degrees)\n17 Subduction obliquity (degrees)\n18 Distance along margin (km)\n19 Subduction obliquity signed (radians)\n20 Ore Deposits Binary Flag (1 or 0)\n\n\n\n\n0\n-66.28\n-27.37\n-65.264812\n-28.103781\n6.0\n0.0\n48.189707\n56.08069\n2436.30907\n2436.30907\n...\n40.63020\n-17.43987\n12.20271\n102.31471\n28.82518\n5.67505\n15.73415\n2269.19769\n0.274613\n1.0\n\n\n1\n-69.75\n-30.50\n-67.696759\n-31.970639\n12.0\n0.0\n52.321162\n56.09672\n2490.68735\n2490.68735\n...\n39.60199\n-22.80622\n13.40127\n115.35820\n27.39401\n5.78937\n13.35854\n1823.34107\n0.233151\n1.0\n\n\n2\n-66.65\n-27.27\n-65.128689\n-28.374772\n9.0\n0.0\n53.506085\n55.77705\n2823.54951\n2823.54951\n...\n45.32425\n-18.08485\n11.27500\n100.24282\n34.62444\n8.97218\n19.05520\n2269.19769\n0.332576\n1.0\n\n\n3\n-66.61\n-27.33\n-65.257928\n-28.311094\n8.0\n0.0\n51.317135\n55.90088\n2656.71724\n2656.71724\n...\n43.13319\n-17.78538\n11.72618\n101.21965\n31.92962\n7.42992\n17.50782\n2269.19769\n0.305569\n1.0\n\n\n4\n-66.55\n-27.40\n-65.366917\n-28.257580\n7.0\n0.0\n49.340097\n56.09011\n2547.29585\n2547.29585\n...\n40.57322\n-17.43622\n12.23778\n102.25748\n28.80235\n5.65657\n15.73067\n2269.19769\n0.274552\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n296\n-78.67\n-6.73\n-70.657487\n-11.057387\n39.0\n0.0\n62.727249\n56.14919\n5373.67650\n1076.30110\n...\n13.21524\n-25.08597\n12.24246\n60.45651\n-7.46828\n-22.30925\n7.04216\n4708.08568\n-0.122909\n0.0\n\n\n297\n-75.09\n-13.69\n-37.112536\n-19.124363\n121.0\n0.0\n30.740063\n54.09642\n269.79929\n269.79929\n...\n-39.68330\n11.56758\n7.99788\n-19.41449\n-59.05957\n-46.36908\n71.80290\n3761.82099\n1.253197\n0.0\n\n\n298\n-71.31\n-14.91\n-38.398992\n-21.934657\n151.0\n0.0\n17.739843\n53.93117\n323.86191\n323.86191\n...\n-3.42257\n-17.25992\n-22.78837\n8.88338\n-7.68381\n-40.99490\n40.85864\n3378.69739\n-0.713118\n0.0\n\n\n299\n-70.61\n-17.25\n-37.243172\n-24.160112\n145.0\n0.0\n11.744395\n53.94534\n163.59542\n163.59542\n...\n-2.26253\n14.87833\n0.05195\n2.36178\n-23.78566\n-38.97366\n84.32944\n3160.06366\n-1.471826\n0.0\n\n\n300\n-76.13\n-11.60\n-43.993914\n-16.965040\n101.0\n0.0\n35.880790\n54.85460\n1190.90698\n1190.90698\n...\n40.29418\n-31.96652\n41.93348\n71.76161\n-29.57451\n-38.50603\n22.39762\n4093.90633\n-0.390912\n0.0\n\n\n\n\n301 rows × 21 columns\n\n\n\nThere are 21 columns (python (usually) counts from 0) representing different parameters. Some of these parameters may be useful for us. Some are not. The final column contains a binary flag representing whether there is a known porphyry copper deposit at that location or not. The “non-deposits” are required to train our Machine Learning classifier what a porphyry deposit looks like, and also, what a porphyry deposit doesn’t look like!"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#perform-machine-learning-binary-classification.",
    "href": "notebooks/03a-MachineLearning.html#perform-machine-learning-binary-classification.",
    "title": "Machine Learning for Geoscience",
    "section": "Perform Machine Learning binary classification.",
    "text": "Perform Machine Learning binary classification.\n\n#Change data format to numpy array for easy manipulation\nml_data_np=ml_data.values\n\n#Set the indices of the parameters (features) to include in the ML\n#params=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n# Alternatively try include any set of features you'd like to include!\n#params=[6,9,14,17] \nparams=[16,17,18,19] \n\n\n#Save the number of parameters we have chosen\ndatalength=len(params)\n\n#Normalise the data for Machine Learning\nml_data_norm=preprocessing.scale(ml_data_np[:,params])\n\n#Create a 'feature vector' and a 'target classification vector'\nfeatures=ml_data_norm\ntargets=ml_data_np[:,20]\n\n#Print out some info about our final dataset\nprint(\"Shape of ML data array: \", ml_data_norm.shape)\nprint(\"Positive (deposits) examples: \",np.shape(ml_data_np[ml_data_np[:,20]==1,:]))\nprint(\"Negative (non-deposits) examples: \",np.shape(ml_data_np[ml_data_np[:,20]==0,:]))\n\nShape of ML data array:  (301, 4)\nPositive (deposits) examples:  (147, 21)\nNegative (non-deposits) examples:  (154, 21)\n\n\n\nprint('Make the classifiers')\n\nprint('Random Forest...')\n#create and train the random forest\n#multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\n#n_estimators use between 64-128 doi: 10.1007/978-3-642-31537-4_13\nrf = RandomForestClassifier(n_estimators=128, n_jobs=1,class_weight=None)\nrf.fit(features,targets)\nprint(\"Done RF\")\n\nscores = cross_val_score(rf, features,targets, cv=10)\nprint(\"RF Scores: \",scores)\nprint(\"SCORE Mean: %.2f\" % np.mean(scores), \"STD: %.2f\" % np.std(scores), \"\\n\")\n\nprint(\"Targets (expected result):\")\nprint(targets)\n\nprint(\"Prediction (actual result):\")\nprint(rf.predict(features))\n\nMake the classifiers\nRandom Forest...\nDone RF\nRF Scores:  [0.74193548 0.8        0.63333333 0.73333333 0.76666667 0.6\n 0.4        0.66666667 0.76666667 0.66666667]\nSCORE Mean: 0.68 STD: 0.11 \n\nTargets (expected result):\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nPrediction (actual result):\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\n\n#Make a list of labels for our chosen features\nparamColumns=np.array(ml_data.columns)\nparamLabels=paramColumns[params].tolist()\n\n#Create a new figure\nfig, ax = plt.subplots()\n\n#Plot the bar graph\nrects=ax.barh(np.arange(0, datalength, step=1),rf.feature_importances_)\n\n#Label the axes\nax.set_yticks(np.arange(0, datalength, step=1))\nax.set_yticklabels(paramLabels)\nax.set_xlabel('Feature Importance')\n\n#Print the feature importance to compare with plot\nnp.set_printoptions(precision=3,suppress=True)\nprint(\"Importance \\t Feature\")\nfor i,label in enumerate(paramLabels):\n    print(\"%1.3f \\t\\t %s\" % (rf.feature_importances_[i],label))\n\nplt.show()\n\nImportance   Feature\n0.305        16 Subduction polarity (degrees)\n0.295        17 Subduction obliquity (degrees)\n0.142        18 Distance along margin (km)\n0.257        19 Subduction obliquity signed (radians)\n\n\n\n\n\nNow if we can measure the tectonomagmatic properties at some point. Based on our trained classifier we can predict a probability that porphyry copper deposits have formed\n\n#Apply the trained ML to our gridded data to determine the probabilities at each of the points\nprint('RF...')\npRF=np.array(rf.predict_proba(features))\nprint(\"Done RF\")\n\nRF...\nDone RF\n\n\n\n#Now you have a working ML model. You can use NEW DATA (you go and collect in the field or whatever)\n#to make predictions\nnewdata = np.array([[0.5, -0.6, -0.7,  0.2]])\nrf.predict_proba(newdata)\n\narray([[0.75, 0.25]])"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#mapping-the-ml-result",
    "href": "notebooks/03a-MachineLearning.html#mapping-the-ml-result",
    "title": "Machine Learning for Geoscience",
    "section": "Mapping the ML result",
    "text": "Mapping the ML result\n\nfilename=\"../data/topodata.nc\"\ndata = scipy.io.netcdf.netcdf_file(filename,'r')\n\ndata.variables\n\n/var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/ipykernel_40834/2264377247.py:2: DeprecationWarning: Please use `netcdf_file` from the `scipy.io` namespace, the `scipy.io.netcdf` namespace is deprecated.\n  data = scipy.io.netcdf.netcdf_file(filename,'r')\n\n\n{'X': &lt;scipy.io._netcdf.netcdf_variable at 0x1391686d0&gt;,\n 'Y': &lt;scipy.io._netcdf.netcdf_variable at 0x139304f70&gt;,\n 'elev': &lt;scipy.io._netcdf.netcdf_variable at 0x139281c40&gt;}\n\n\n\ntopoX=data.variables['X'][:]\ntopoY=data.variables['Y'][:]\ntopoZ=np.array(data.variables['elev'][:])\n\n#Some file types and readers (like netcdf) can actually change the data directly on disk\n#Good practice, is to close the file when done (for safety and memory saving)\ndata.close()\n\n/Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages/scipy/io/_netcdf.py:304: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\n  warnings.warn((\n\n\n\n#Make a figure object\nplt.figure()\n\n#Get the axes of the current figure, for manipulation\nax = plt.gca()\n\n#Put down the main topography dataset\nim=ax.imshow(topoZ,vmin=-5000,vmax=1000,extent=[0,360,-90,90],origin='upper',aspect=1,cmap=cm.gist_earth)\n\n#Make a colorbar\ncbar=plt.colorbar(im,fraction=0.025,pad=0.05,ticks=[-5000,0, 1000],extend='both')\ncbar.set_label('Height \\n ASL \\n (m)', rotation=0)\n\n#Clean up the default axis ticks\nplt.yticks([-90,-45,0,45,90])\nplt.xticks([0,90,180,270,360])\n\n#Put labels on the figure\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\n#Put a title on it\nplt.title(\"Bathymetry and Topography of the World \\n (ETOPO5 2020)\")\n\nplt.show()"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#plotting-shapefiles-with-for-loops",
    "href": "notebooks/03a-MachineLearning.html#plotting-shapefiles-with-for-loops",
    "title": "Machine Learning for Geoscience",
    "section": "Plotting shapefiles with for loops",
    "text": "Plotting shapefiles with for loops\n\n#Load in plate polygons for plotting\ntopologyFile='../data/platepolygons/topology_platepolygons_0.00Ma.shp'\n\n#read in the file\nshapeRead = shapefile.Reader(topologyFile)\n\n#And save out some of the shape file attributes\nrecs    = shapeRead.records()\nshapes  = shapeRead.shapes()\nfields  = shapeRead.fields\nNshp    = len(shapes)\n\nfor i, nshp in enumerate(range(Nshp)):\n    #if nshp!=35 and nshp!=36 and nshp!=23:\n    #These are the plates that cross the dateline and cause \n        #banding errors\n        polygonShape=shapes[nshp].points\n        poly=np.array(polygonShape)\n        plt.plot(poly[:,0], poly[:,1], c='k',zorder=1)\n        \nplt.show()"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#cartopy-for-a-prettier-map",
    "href": "notebooks/03a-MachineLearning.html#cartopy-for-a-prettier-map",
    "title": "Machine Learning for Geoscience",
    "section": "Cartopy for a prettier map",
    "text": "Cartopy for a prettier map\n\n###Set up the figure\nfig = plt.figure(figsize=(16,12),dpi=150)\n\nax = plt.axes(projection=ccrs.PlateCarree())\nax.set_extent([-85, -30, -55, 10])\nax.coastlines('50m', linewidth=0.8)\n\n###Add the map grid lines and format them\ngl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n                  linewidth=2, color='gray', alpha=0.5, linestyle='-')\n\nfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\nimport matplotlib.ticker as mticker\nfrom matplotlib import colorbar, colors\n\ngl.top_labels = False\ngl.left_labels = True\ngl.right_labels = False\ngl.xlines = False\ngl.ylines = False\ngl.xlocator = mticker.FixedLocator([-75,-60, -45,-30])\ngl.ylocator = mticker.FixedLocator([-60, -45, -30, -15, 0,15])\ngl.xformatter = LONGITUDE_FORMATTER\ngl.yformatter = LATITUDE_FORMATTER\n#gl.xlabel_style = {'size': 15, 'color': 'gray'}\n#gl.xlabel_style = {'color': 'black', 'weight': 'normal'}\n\nprint(\"Made base map\")\n\n###Plot a topography underlay image\n#Make a lat lon grid to fit the topo grid\nlons, lats = np.meshgrid(topoX,topoY)\nim1=ax.pcolormesh(lons,lats,topoZ, shading=\"auto\",cmap=plt.cm.gist_earth,transform=ccrs.PlateCarree())              \ncbar=plt.colorbar(im1, ax=ax, orientation=\"horizontal\", pad=0.01, fraction=0.05, shrink=0.2,extend='both')\ncbar.set_label('Topography (m)')\n\nprint(\"Added topo\")\n\n###Plot shapefile polygon outlines\n#Load in plate polygons for plotting\ntopologyFile='../data/platepolygons/topology_platepolygons_0.00Ma.shp'\n\n#read in the file\nshapeRead = shapefile.Reader(topologyFile)\n\n#And save out some of the shape file attributes\nrecs    = shapeRead.records()\nshapes  = shapeRead.shapes()\nfields  = shapeRead.fields\nNshp    = len(shapes)\n\nfor i, nshp in enumerate(range(Nshp)):\n    if nshp!=35 and nshp!=36 and nshp!=23:\n    #These are the plates that cross the dateline and cause \n        #banding errors\n        polygonShape=shapes[nshp].points\n        poly=np.array(polygonShape)\n        xh=poly[:,0]\n        yh=poly[:,1]\n        ax.plot(xh, yh, c='w',zorder=1)\n\nprint(\"Added shapes\")\n        \n###Plot the ore deposit probability\nxh = ml_data_np[ml_data_np[:,-1]==1,0]\nyh= ml_data_np[ml_data_np[:,-1]==1,1]\nl2 = ax.scatter(xh, yh, 500, marker='.',c=pRF[:147,1],cmap=plt.cm.copper,zorder=3,transform=ccrs.PlateCarree(),vmin=0,vmax=1)\n#l2 = pmap.scatter(xh, yh, 20, marker='.',edgecolor='dimgrey',linewidth=0.5,c=pRF[:147,1],cmap=plt.cm.copper,zorder=3)\ncbar=fig.colorbar(l2, ax=ax, orientation=\"horizontal\", pad=0.05, fraction=0.05, shrink=0.2,ticks=[0,0.5,1.0])\nl2.set_clim(-0.1, 1.1)\ncbar.set_label('Prediction Probability (%)')\n\n###Plot the ore deposit Age\nxh=ml_data_np[ml_data_np[:,-1]==1,0]\nyh = ml_data_np[ml_data_np[:,-1]==1,1]\nl2 = ax.scatter(xh, yh, 50, marker='.',c=ml_data_np[ml_data_np[:,-1]==1,4],cmap=plt.cm.hsv,zorder=3)\ncbar=fig.colorbar(l2, ax=ax, orientation=\"horizontal\", pad=0.1, fraction=0.05, shrink=0.2,extend='max',ticks=[0,50,100,150])\nl2.set_clim(0, 170)\ncbar.set_label('Age of Deposit (Ma)')\n\nprint(\"Added deposit probability\")\n\nplt.show()\n\nMade base map\nAdded topo\nAdded shapes\nAdded deposit probability"
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#exercise",
    "href": "notebooks/03a-MachineLearning.html#exercise",
    "title": "Machine Learning for Geoscience",
    "section": "Exercise",
    "text": "Exercise\nDo the same analysis but using a different Machine Learning algorithm for your classification. You can use this as a guide for picking a good classification algorithm https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html. Present your results on a map, and compare it with the Random Forest method."
  },
  {
    "objectID": "notebooks/03a-MachineLearning.html#datasets",
    "href": "notebooks/03a-MachineLearning.html#datasets",
    "title": "Machine Learning for Geoscience",
    "section": "Datasets",
    "text": "Datasets\n\nTopography/Bathymetry\nWORLDBATH: ETOPO5 5x5 minute Navy bathymetry. http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NGDC/.ETOPO5/\n\n\nML dataset\nButterworth et al 2016 https://doi.org/10.1002/2016TC004289\n\n\nShapefile plate polygons\nGPlates2.0. https://www.gplates.org/\n\n\nKey points\n\nApplying ML workflows\nWrangling data."
  },
  {
    "objectID": "notebooks/PDF2Python.html",
    "href": "notebooks/PDF2Python.html",
    "title": "Reading PDF files into Python",
    "section": "",
    "text": "Questions\n\nHow do I read PDF files in Python?\n\n\n\nObjectives\n\nUse tabula-py to work with PDF files in Python\n\n\nNote: in the time it took me to figure out this code, I could have manually transcribed about 50 of these tables I reckon! Just because you can does not mean you should.\nThere seem to be a few approaches to reading PDFs with Python. If the PDF is already searchable and you just want to transcribe it, then this notebook using the tabula-py library seems like a good method.\nIf your PDF is just a plain image, a more versatile approach is to use an OCR on your document or to convert it to and image. Adjust to your needs, but these workflows and libraries may be helpful: - https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052, or - https://pypi.org/project/ocrmypdf/\nNote that this requres Java! To install on a Mac via Homebrew, follow the instructions here.\n\n!pip install tabula-py\n\nCollecting tabula-py\n  Downloading tabula_py-2.5.1-py3-none-any.whl (12.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 87.4 MB/s eta 0:00:00a 0:00:01\nCollecting distro\n  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: pandas&gt;=0.25.3 in /Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages (from tabula-py) (1.4.3)\nRequirement already satisfied: numpy in /Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages (from tabula-py) (1.23.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages (from pandas&gt;=0.25.3-&gt;tabula-py) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages (from pandas&gt;=0.25.3-&gt;tabula-py) (2022.1)\nRequirement already satisfied: six&gt;=1.5 in /Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=0.25.3-&gt;tabula-py) (1.16.0)\nInstalling collected packages: distro, tabula-py\nSuccessfully installed distro-1.7.0 tabula-py-2.5.1\n\n\n\n#https://pypi.org/project/tabula-py/\n\nimport tabula\n\n# Read pdf into list of DataFrame\ndfs = tabula.read_pdf(\"../userdata//G32716A3.pdf\", pages='all', pandas_options={\"header\":None})\n\n# Read remote pdf into list of DataFrame\n#dfs2 = tabula.read_pdf(\"https://github.com/tabulapdf/tabula-java/raw/master/src/test/resources/technology/tabula/arabic.pdf\")\n\n# convert PDF into CSV file\n#tabula.convert_into(\"test.pdf\", \"output.csv\", output_format=\"csv\", pages='all')\n\n# convert all PDFs in a directory\n#tabula.convert_into_by_batch(\"input_directory\", output_format='csv', pages='all')\n\nGot stderr: Aug 30, 2022 3:04:07 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider loadDiskCache\nWARNING: New fonts found, font cache will be re-built\nAug 30, 2022 3:04:07 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider &lt;init&gt;\nWARNING: Building on-disk font cache, this may take a while\nAug 30, 2022 3:04:08 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider &lt;init&gt;\nWARNING: Finished building on-disk font cache, found 948 fonts\n\n\n\n\n#What format has the load returned?\ntype(dfs)\n\nlist\n\n\n\n#Import pandas to do some table manipulation\nimport pandas as pd\n\n\ncolnames = [\"SampleID\", \"Project\", \"Season\", \"OrigGeo\", \"Lithology\", \"CoreName\", \"CoreDepth\", \"Geochronology\"]\ndf = dfs[0]\ndf.columns=colnames\ndf\n\n\n\n\n\n\n\n\nSampleID\nProject\nSeason\nOrigGeo\nLithology\nCoreName\nCoreDepth\nGeochronology\n\n\n\n\n0\n214830.0\n53.0\nNaN\nMarkwitz\nQuartz-garnet gneiss (PJO)\nNaN\n1246.75-\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1246.6\nNaN\n\n\n3\n214831.0\n53.0\nNaN\nMarkwitz\nCordierite-sillimanite-garnet\nNaN\n1244.5-\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\n\n\n5\nNaN\nNaN\nNaN\nNaN\ngneiss (PJO)\nNaN\n1244.3\nNaN\n\n\n6\n214832.0\n53.0\nNaN\nMarkwitz\nCordierite-sillimanite-garnet\nNaN\n1241.4-\nNaN\n\n\n7\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\n\n\n8\nNaN\nNaN\nNaN\nNaN\ngneiss (PJO)\nNaN\n1241.3\nNaN\n\n\n9\n214833.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1209.72-\nNaN\n\n\n10\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n11\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1209.0\nNaN\n\n\n12\nNaN\nNaN\n2014.0\nNaN\nNaN\nWendy-1\nNaN\nNaN\n\n\n13\n214834.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1134.4-\nNaN\n\n\n14\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n15\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1133.9\nNaN\n\n\n16\n214835.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1055.2-\nNaN\n\n\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\n\n\n18\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1054.9\nNaN\n\n\n19\n214836.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n932.15-\nNaN\n\n\n20\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n21\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n931.75\nNaN\n\n\n22\n214837.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\nNaN\nNaN\n\n\n23\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n915.2-915\nYes\n\n\n24\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\nNaN\nNaN\n\n\n25\n214839.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1082.0-\nNaN\n\n\n26\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n27\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1082.3\nNaN\n\n\n28\n214840.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\nNaN\nNaN\n\n\n29\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1072-1071.7\nYes\n\n\n30\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\nNaN\nNaN\n\n\n31\n214841.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1065.8-\nNaN\n\n\n32\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n33\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1066.0\nNaN\n\n\n34\nNaN\nNaN\n2015.0\nNaN\nNaN\nCoburn 1\nNaN\nNaN\n\n\n35\n214842.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1048.5-\nNaN\n\n\n36\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n37\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1048.8\nNaN\n\n\n38\n214843.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\n1014.7-\nNaN\n\n\n39\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nYes\n\n\n40\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\n1015.0\nNaN\n\n\n41\n214844.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda\nNaN\nNaN\nNaN\n\n\n42\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n985.8-986.1\nYes\n\n\n43\nNaN\nNaN\nNaN\nNaN\nSandstone\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Fill \"forward\" all the approriate groups\ndf[[\"SampleID\",\"Project\",\"OrigGeo\"]] = df[[\"SampleID\",\"Project\",\"OrigGeo\"]].fillna(method=\"ffill\")\n\n#Group by the unique sample id...\n\n#...then fill all the nan values in that group\ndf['Season'] = df.groupby('SampleID').Season.transform('first')\ndf['CoreName'] = df.groupby('SampleID').CoreName.transform('first')\ndf['Geochronology'] = df.groupby('SampleID').Geochronology.transform('first')\n\n#..then combine strings if the group has multiple lines of text, note what we want to pad each bit of text with\ndf['Lithology'] = df.groupby(['SampleID'])['Lithology'].transform(lambda x: ' '.join(x.dropna()))\ndf['CoreDepth'] = df.groupby(['SampleID'])['CoreDepth'].transform(lambda x: ''.join(x.dropna()))\n\n\n#Drop all the repeated lines to get the final table\ndf = df.drop_duplicates(keep='first')\ndf\n\n\n\n\n\n\n\n\nSampleID\nProject\nSeason\nOrigGeo\nLithology\nCoreName\nCoreDepth\nGeochronology\n\n\n\n\n0\n214830.0\n53.0\nNaN\nMarkwitz\nQuartz-garnet gneiss (PJO)\nNone\n1246.75-1246.6\nYes\n\n\n3\n214831.0\n53.0\nNaN\nMarkwitz\nCordierite-sillimanite-garnet gneiss (PJO)\nNone\n1244.5-1244.3\nNo\n\n\n6\n214832.0\n53.0\nNaN\nMarkwitz\nCordierite-sillimanite-garnet gneiss (PJO)\nNone\n1241.4-1241.3\nNo\n\n\n9\n214833.0\n53.0\n2014.0\nMarkwitz\nSandstone – Tumblagooda Sandstone\nWendy-1\n1209.72-1209.0\nYes\n\n\n13\n214834.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n1134.4-1133.9\nYes\n\n\n16\n214835.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n1055.2-1054.9\nNo\n\n\n19\n214836.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n932.15-931.75\nYes\n\n\n22\n214837.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n915.2-915\nYes\n\n\n25\n214839.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n1082.0-1082.3\nYes\n\n\n28\n214840.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n1072-1071.7\nYes\n\n\n31\n214841.0\n53.0\n2015.0\nMarkwitz\nSandstone – Tumblagooda Sandstone\nCoburn 1\n1065.8-1066.0\nYes\n\n\n35\n214842.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n1048.5-1048.8\nYes\n\n\n38\n214843.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n1014.7-1015.0\nYes\n\n\n41\n214844.0\n53.0\nNaN\nMarkwitz\nSandstone – Tumblagooda Sandstone\nNone\n985.8-986.1\nYes\n\n\n\n\n\n\n\n\n#df.to_csv()\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML.html",
    "href": "notebooks/03-ML.html",
    "title": "Exploratory Data Analysis (EDA) for ML",
    "section": "",
    "text": "Introduce some of the key packages for EDA and ML.\nIntroduce and explore an dataset for ML\nClean up a dataset\nInstall additional Python libraries\nFirst, let’s load the required libraries. We will use the sklearn library for our ML tasks, and the pandas, numpy, matplotlib seaborn and upsetplot libraries for general data processing and visualisation.\n# Disable some warnings produced by pandas etc.\n# (Don't do this in your actual analyses!)\nimport warnings\nwarnings.simplefilter('ignore', category=UserWarning)\nwarnings.simplefilter('ignore', category=FutureWarning)\nwarnings.simplefilter('ignore', category=RuntimeWarning)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\n#You probably do not have this library! Install it with pip:\n#!pip install UpSetPlot\nimport upsetplot\n%matplotlib inline\nsns.set(font_scale = 1.5)\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML.html#load-the-data",
    "href": "notebooks/03-ML.html#load-the-data",
    "title": "Exploratory Data Analysis (EDA) for ML",
    "section": "Load the data",
    "text": "Load the data\nDownload the data and put it in your “data” folder. You will have to download it from the GitHub repo (right click on the Download button and select “Save link as..”). Our data is based on a submitted Manuscript (Butterworth and Barnett-Moore 2020) which was a finalist in the Unearthed, ExploreSA: Gawler Challenge.\nThe dataset contains a mix of categorical and numerical values, representing various geophysical and geological measurements across the Gawler Craton in South Australia.\n\n#training_data-Cu.txt\n\n#Read in the data\n#Set a value for NaNs\n#Drop many of the columns (so it is easier to work with)\ndf = pd.read_csv('../data/training_data-Cu.csv',na_values='-9999.0')\ncols = list(range(5,65))\ncols.insert(0,0)\ndf.drop(df.columns[cols],axis=1,inplace=True)\n\ndf=df.astype({'archean27':'object','geol28':'object','random':'int64','deposit':'int64'})"
  },
  {
    "objectID": "notebooks/03-ML.html#exploratory-data-analysis",
    "href": "notebooks/03-ML.html#exploratory-data-analysis",
    "title": "Exploratory Data Analysis (EDA) for ML",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nExploratory data analysis involves looking at:\n\nthe distribution of variables in your dataset\nwhether any data is missing\nskewed\ncorrelated variables\n\n\n#What are the dimensions of the data?\ndf.shape\n\n(3138, 38)\n\n\n\n#Look at the data:\ndf.head()\n\n\n\n\n\n\n\n\nlon\nlat\nres-25\nres-77\nres-309183\nneoFaults\narchFaults\ngairFaults\naster1-AlOH-cont\naster2-AlOH\n...\nmag21-tmi\nrad22-dose\nrad23-k\nrad24-th\nrad25-u\ngrav26\narchean27\ngeol28\nrandom\ndeposit\n\n\n\n\n0\n129.106649\n-26.135900\n1.9959\n1.9935\n2.5780\n0.858696\n0.874997\n2.718781\n1.907609\nNaN\n...\n-88.364891\n34.762928\n1.269402\n6.065621\n38.492386\n27.176790\n14552.0\n17296.0\n999\n1\n\n\n1\n132.781571\n-26.151144\n2.0450\n2.0651\n2.3873\n0.607134\n0.936479\n1.468679\n2.032987\n1.076198\n...\n-190.025864\n89.423668\n3.169631\n15.980172\n56.650471\n-83.541550\n14552.0\n17068.0\n-999\n1\n\n\n2\n132.816676\n-26.159202\n2.0450\n2.0651\n2.3873\n0.577540\n0.914588\n1.446256\n1.982274\n1.050442\n...\n-251.018036\n75.961006\n2.525403\n15.625917\n58.361298\n-81.498817\n14552.0\n17296.0\n-999\n1\n\n\n3\n128.945869\n-26.179362\n1.9978\n1.9964\n2.6844\n0.810394\n0.826784\n2.813603\n1.947705\nNaN\n...\n873.983521\n46.321651\nNaN\nNaN\n50.577263\n33.863503\nNaN\nNaN\n-999\n1\n\n\n4\n132.549807\n-26.185500\n2.0694\n2.0999\n2.3574\n0.652131\n1.026991\n1.499793\n1.977050\n1.064977\n...\n71.432777\n47.194534\n2.367707\n6.874684\n29.794928\n-90.970375\n14552.0\n17296.0\n-999\n1\n\n\n\n\n5 rows × 38 columns\n\n\n\n\n#What types are each of the columns?\ndf.dtypes\n\nlon                     float64\nlat                     float64\nres-25                  float64\nres-77                  float64\nres-309183              float64\nneoFaults               float64\narchFaults              float64\ngairFaults              float64\naster1-AlOH-cont        float64\naster2-AlOH             float64\naster3-FeOH-cont        float64\naster4-Ferric-cont      float64\naster5-Ferrous-cont     float64\naster6-Ferrous-index    float64\naster7-MgOH-comp        float64\naster8-MgOH-cont        float64\naster9-green            float64\naster10-kaolin          float64\naster11-opaque          float64\naster12-quartz          float64\naster13-regolith-b3     float64\naster14-regolith-b4     float64\naster15-silica          float64\nbase16                  float64\ndem17                   float64\ndtb18                   float64\nmag19-2vd               float64\nmag20-rtp               float64\nmag21-tmi               float64\nrad22-dose              float64\nrad23-k                 float64\nrad24-th                float64\nrad25-u                 float64\ngrav26                  float64\narchean27                object\ngeol28                   object\nrandom                    int64\ndeposit                   int64\ndtype: object\n\n\n\n#Get information about index type and column types, non-null values and memory usage.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3138 entries, 0 to 3137\nData columns (total 38 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   lon                   3138 non-null   float64\n 1   lat                   3138 non-null   float64\n 2   res-25                3138 non-null   float64\n 3   res-77                3138 non-null   float64\n 4   res-309183            3138 non-null   float64\n 5   neoFaults             3138 non-null   float64\n 6   archFaults            3138 non-null   float64\n 7   gairFaults            3138 non-null   float64\n 8   aster1-AlOH-cont      2811 non-null   float64\n 9   aster2-AlOH           2010 non-null   float64\n 10  aster3-FeOH-cont      1811 non-null   float64\n 11  aster4-Ferric-cont    2811 non-null   float64\n 12  aster5-Ferrous-cont   1644 non-null   float64\n 13  aster6-Ferrous-index  2811 non-null   float64\n 14  aster7-MgOH-comp      1644 non-null   float64\n 15  aster8-MgOH-cont      1811 non-null   float64\n 16  aster9-green          3129 non-null   float64\n 17  aster10-kaolin        1811 non-null   float64\n 18  aster11-opaque        753 non-null    float64\n 19  aster12-quartz        3130 non-null   float64\n 20  aster13-regolith-b3   3127 non-null   float64\n 21  aster14-regolith-b4   3073 non-null   float64\n 22  aster15-silica        3130 non-null   float64\n 23  base16                3135 non-null   float64\n 24  dem17                 3133 non-null   float64\n 25  dtb18                 1490 non-null   float64\n 26  mag19-2vd             3132 non-null   float64\n 27  mag20-rtp             3132 non-null   float64\n 28  mag21-tmi             3132 non-null   float64\n 29  rad22-dose            2909 non-null   float64\n 30  rad23-k               2900 non-null   float64\n 31  rad24-th              2904 non-null   float64\n 32  rad25-u               2909 non-null   float64\n 33  grav26                3131 non-null   float64\n 34  archean27             3135 non-null   object \n 35  geol28                3135 non-null   object \n 36  random                3138 non-null   int64  \n 37  deposit               3138 non-null   int64  \ndtypes: float64(34), int64(2), object(2)\nmemory usage: 931.7+ KB\n\n\n\n#Explore how many null values are in the dataset\ndf.isnull().sum(axis = 0)\n\nlon                        0\nlat                        0\nres-25                     0\nres-77                     0\nres-309183                 0\nneoFaults                  0\narchFaults                 0\ngairFaults                 0\naster1-AlOH-cont         327\naster2-AlOH             1128\naster3-FeOH-cont        1327\naster4-Ferric-cont       327\naster5-Ferrous-cont     1494\naster6-Ferrous-index     327\naster7-MgOH-comp        1494\naster8-MgOH-cont        1327\naster9-green               9\naster10-kaolin          1327\naster11-opaque          2385\naster12-quartz             8\naster13-regolith-b3       11\naster14-regolith-b4       65\naster15-silica             8\nbase16                     3\ndem17                      5\ndtb18                   1648\nmag19-2vd                  6\nmag20-rtp                  6\nmag21-tmi                  6\nrad22-dose               229\nrad23-k                  238\nrad24-th                 234\nrad25-u                  229\ngrav26                     7\narchean27                  3\ngeol28                     3\nrandom                     0\ndeposit                    0\ndtype: int64\n\n\n\n#Find out what's the top missing:\nmissingNo = df.isnull().sum(axis = 0).sort_values(ascending = False)\nmissingNo = missingNo[missingNo.values  &gt; 0]\nmissingNo\n\naster11-opaque          2385\ndtb18                   1648\naster5-Ferrous-cont     1494\naster7-MgOH-comp        1494\naster3-FeOH-cont        1327\naster8-MgOH-cont        1327\naster10-kaolin          1327\naster2-AlOH             1128\naster1-AlOH-cont         327\naster4-Ferric-cont       327\naster6-Ferrous-index     327\nrad23-k                  238\nrad24-th                 234\nrad25-u                  229\nrad22-dose               229\naster14-regolith-b4       65\naster13-regolith-b3       11\naster9-green               9\naster15-silica             8\naster12-quartz             8\ngrav26                     7\nmag19-2vd                  6\nmag20-rtp                  6\nmag21-tmi                  6\ndem17                      5\narchean27                  3\ngeol28                     3\nbase16                     3\ndtype: int64\n\n\n\n#Plot the missingness with Seaborn\nf, ax = plt.subplots(figsize = (10, 10))\nsns.barplot(missingNo.values, missingNo.index, ax = ax);\n\n\n\n\n\n# Use upsetplot to see where missing values occur\n# together\n# Only use the top 5 columns\nmissing_cols = missingNo.index[:5].tolist()\nmissing_counts = (df.loc[:, missing_cols]\n                  .isnull()\n                  .groupby(missing_cols)\n                  .size())\n\nupsetplot.plot(missing_counts);\n\n\n\n\nWhy is this useful to know? Can our future data analysis deal with mising data?"
  },
  {
    "objectID": "notebooks/03-ML.html#explore-the-data-to-see-whether-there-are-any-unusual-relationships-between-variables",
    "href": "notebooks/03-ML.html#explore-the-data-to-see-whether-there-are-any-unusual-relationships-between-variables",
    "title": "Exploratory Data Analysis (EDA) for ML",
    "section": "Explore the data to see whether there are any unusual relationships between variables",
    "text": "Explore the data to see whether there are any unusual relationships between variables\n\nPull out numeric and categoric variables:\n\nWhat data types do I have in my data? Can I infer that some of them are categorical, and others are not?\n\n\ndf.dtypes.value_counts()\n\nfloat64    34\nobject      2\nint64       2\ndtype: int64\n\n\n\nPull out the categorical and numerical variables\n\n\nnumericVars = df.select_dtypes(exclude = ['int64','object']).columns\ncatVars = df.select_dtypes(include = ['object']).columns\n\n\nPlot the first 11 numerical variables, and their relationship with whether deposit information.\n\n\ndf.shape\n\n(3138, 38)\n\n\n\n#Select which columns to plot (all of them are too many), and be sure to include the \"deposit\" variable\ncols = [np.append(np.arange(0, 11), 37)]\n#Make a pairwise plot to find all the relationships in the data\nsns.pairplot(df[df.columns[cols].tolist()[0]],hue =\"deposit\",palette=\"Set1\",diag_kind=\"kde\",diag_kws={'bw': 0.1})\n\n\n\n\n\n\nChallenge\nWhat variables are the most correlated? Hint: pandas has a function to find e.g. “pearson” corrrelations.\n\n\nSolution\n\ndf.corr()\n    \n#Or pick a variable that you want to sort by. And round out the sig figs.\n#df.corr().round(2).sort_values('dem17', ascending = False)\n\n\nBut, no need to dig through a table! We can plot the relationships.\n\ncorr = df.corr() \n\n# Draw the heatmap with the mask and correct aspect ratio\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corr,\n            cmap=plt.cm.BrBG, \n            vmin=-0.5, vmax=0.5, \n            square=True,\n            xticklabels=True, yticklabels=True,\n            ax=ax);\n\n\n\n\n\n#Plot a regression model through the data\nsns.lmplot(\n    data = df,\n    x = 'res-25', y = 'res-77',hue='deposit'\n);\n\n\n\n\n\n\n\nKey points\n\nEDA is the first step of any analysis, and often very time consuming.\nSkipping EDA can result in substantial issues with subsequent analysis.\n\n\n\n\n\nQuestions:\n\nWhat is the first step of any ML project (and often the most time consuming)?"
  },
  {
    "objectID": "notebooks/text2segy.html",
    "href": "notebooks/text2segy.html",
    "title": "A. Read in text data",
    "section": "",
    "text": "# Standard Python libraries\nimport sys\nimport pandas as pd\nimport numpy as np\n\n#Plotting\nimport matplotlib.pyplot as plt\n\n# For reading segy data\nfrom obspy.io.segy.segy import _read_segy\n\n#For writing segy data\nfrom obspy import read, Trace, Stream, UTCDateTime\nfrom obspy.core import AttribDict\nfrom obspy.io.segy.segy import SEGYTraceHeader, SEGYBinaryFileHeader\n\n\n# Set the filename\nf = open(\"userdata/P-impedance_2400-3000ms.gslib\",'r')\n\n#Read in all the lines in the file and save them to a variable\nmylist = f.readlines()\n\n#Close the file\nf.close()\nprint(\"Done reading file. Number of lines:\", len(mylist))\n\nDone reading file. Number of lines: 3957609\n\n\n\n#Have a look at the top of the data\nfor line in mylist[0:11]:\n    print(line)\n\nPETREL: Properties\n\n7\n\ni_index unit1 scale1\n\nj_index unit1 scale1\n\nk_index unit1 scale1\n\nx_coord unit1 scale1\n\ny_coord unit1 scale1\n\nz_coord unit1 scale1\n\nP-Impedance unit1 scale1\n\n127 1 300 438131.65314303 6475378.74871708 -2999.00000000 9556.294922 \n\n128 1 300 438181.65314303 6475378.74871708 -2999.00000000 9627.205078 \n\n\n\n\n# A few ways to map this to a useful python format. But the simplest may be the following\ndata = []\n\n#Skip the header rows\nfor line in mylist[9:]:\n    linesplit = line.split()\n    \n    i = int(linesplit[0])\n    j = int(linesplit[1])\n    k = int(linesplit[2])\n    x = float(linesplit[3])\n    y = float(linesplit[4])\n    z = float(linesplit[5])\n    p = float(linesplit[6])\n    \n    data.append([i,j,k,x,y,z,p])\n\n\n#Put the list in a dataframe\ndf=pd.DataFrame(data)\n\n#Then free up some memory (because this is a fairly big chunk of data)\ndata=None\n\n#Set the names of the columns of the dataframe\ndf.columns=['i','j','k','x','y','z','p']\n\ndf\n\n\n\n\n\n\n\n\ni\nj\nk\nx\ny\nz\np\n\n\n\n\n0\n127\n1\n300\n438131.653143\n6.475379e+06\n-2999.0\n9556.294922\n\n\n1\n128\n1\n300\n438181.653143\n6.475379e+06\n-2999.0\n9627.205078\n\n\n2\n129\n1\n300\n438231.653143\n6.475379e+06\n-2999.0\n9555.066406\n\n\n3\n130\n1\n300\n438281.653143\n6.475379e+06\n-2999.0\n9468.100586\n\n\n4\n123\n2\n300\n437931.653143\n6.475429e+06\n-2999.0\n9601.517578\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3957595\n26\n127\n1\n433081.653143\n6.481679e+06\n-2401.0\n5848.801758\n\n\n3957596\n27\n127\n1\n433131.653143\n6.481679e+06\n-2401.0\n5924.203125\n\n\n3957597\n28\n127\n1\n433181.653143\n6.481679e+06\n-2401.0\n6037.129883\n\n\n3957598\n29\n127\n1\n433231.653143\n6.481679e+06\n-2401.0\n5978.708984\n\n\n3957599\n25\n128\n1\n433031.653143\n6.481729e+06\n-2401.0\n5812.972168\n\n\n\n\n3957600 rows × 7 columns\n\n\n\n\n#Plot a single trace to see everything looks okay\none_trace = df[(df.i==127) & (df.j==1)].p\n\nplt.figure(figsize=(16,2))\nplt.plot(one_trace)\nplt.show()\n\n\n\n\n\nplt.plot(df.z)\n\n\n\n\n\nB. Read in equivalent segy data.\nGet the segy header information from here, and just see what the goal is for the conversion.\n\nstream = None \nstream = _read_segy(\"userdata/P-impedance_2400-3000ms.sgy\", headonly=True)\nprint(np.shape(stream.traces))\nstream\n\n(216540,)\n\n\n216540 traces in the SEG Y structure.\n\n\n\narrs=[]\nfor index, one_trace in enumerate(stream.traces):\n    p = one_trace.data\n    lenp = len(p)\n    i = np.full((lenp), index,dtype=int)\n    j = np.full((lenp), one_trace.header.ensemble_number,dtype=int)\n    k = np.arange(lenp,0,-1,dtype=int)\n    x = np.full((lenp), one_trace.header.source_coordinate_x)\n    y = np.full((lenp), one_trace.header.source_coordinate_y)\n    z = np.full((lenp), one_trace.header.original_field_record_number)\n    \n    temparr = np.c_[i,j,k,x,y,z,p]\n    arrs.append(temparr)\n    \narrs = np.vstack(arrs)\n\n\narrs[-1]\n\narray([2.16539000e+05, 2.50000000e+03, 1.00000000e+00, 4.32975000e+05,\n       6.48179200e+06, 1.03610000e+04, 9.38641016e+03])\n\n\n\nheader =\"PETREL: Properties\\n7\\n\\ni_index unit1 scale1\\nj_index unit1 scale1\\nk_index unit1 scale1\\nx_coord unit1 scale1\\ny_coord unit1 scale1\\nz_coord unit1 scale1\\nP-Impedance unit1 scale1\\n\"\nnp.savetxt(\"data.txt\", arrs,header=header, comments='', fmt=\"%i %i %i %i %i %i %f\")\n\n\none_trace = stream.traces[100]\n\n#Print out details single trace\nprint(one_trace)\n\n#Plot a single trace to see everything looks okay\nplt.figure(figsize=(16,2))\nplt.plot(one_trace.data)\nplt.show()\n\nTrace sequence number within line: 0\n151 samples, dtype=float32, 250.00 Hz\n\n\n\n\n\n\n#Here is the header information - for one trace\nstream.traces[0].header\n\ntrace_sequence_number_within_line: 0\ntrace_sequence_number_within_segy_file: 0\noriginal_field_record_number: 9961\ntrace_number_within_the_original_field_record: 0\nenergy_source_point_number: 0\nensemble_number: 1961\ntrace_number_within_the_ensemble: 0\ntrace_identification_code: 0\nnumber_of_vertically_summed_traces_yielding_this_trace: 0\nnumber_of_horizontally_stacked_traces_yielding_this_trace: 0\ndata_use: 0\ndistance_from_center_of_the_source_point_to_the_center_of_the_receiver_group: 0\nreceiver_group_elevation: 0\nsurface_elevation_at_source: 0\nsource_depth_below_surface: 0\ndatum_elevation_at_receiver_group: 0\ndatum_elevation_at_source: 0\nwater_depth_at_source: 0\nwater_depth_at_group: 0\nscalar_to_be_applied_to_all_elevations_and_depths: 0\nscalar_to_be_applied_to_all_coordinates: 0\nsource_coordinate_x: 438302\nsource_coordinate_y: 6475310\ngroup_coordinate_x: 0\ngroup_coordinate_y: 0\ncoordinate_units: 0\nweathering_velocity: 0\nsubweathering_velocity: 0\nuphole_time_at_source_in_ms: 0\nuphole_time_at_group_in_ms: 0\nsource_static_correction_in_ms: 0\ngroup_static_correction_in_ms: 0\ntotal_static_applied_in_ms: 0\nlag_time_A: 2400\nlag_time_B: 3000\ndelay_recording_time: 0\nmute_time_start_time_in_ms: 0\nmute_time_end_time_in_ms: 0\nnumber_of_samples_in_this_trace: 151\nsample_interval_in_ms_for_this_trace: 4000\ngain_type_of_field_instruments: 0\ninstrument_gain_constant: 0\ninstrument_early_or_initial_gain: 0\ncorrelated: 0\nsweep_frequency_at_start: 0\nsweep_frequency_at_end: 0\nsweep_length_in_ms: 0\nsweep_type: 0\nsweep_trace_taper_length_at_start_in_ms: 0\nsweep_trace_taper_length_at_end_in_ms: 0\ntaper_type: 0\nalias_filter_frequency: 0\nalias_filter_slope: 0\nnotch_filter_frequency: 0\nnotch_filter_slope: 0\nlow_cut_frequency: 0\nhigh_cut_frequency: 0\nlow_cut_slope: 0\nhigh_cut_slope: 0\nyear_data_recorded: 0\nday_of_year: 0\nhour_of_day: 0\nminute_of_hour: 0\nsecond_of_minute: 0\ntime_basis_code: 0\ntrace_weighting_factor: 0\ngeophone_group_number_of_roll_switch_position_one: 0\ngeophone_group_number_of_trace_number_one: 0\ngeophone_group_number_of_last_trace: 0\ngap_size: 0\nover_travel_associated_with_taper: 0\nx_coordinate_of_ensemble_position_of_this_trace: 0\ny_coordinate_of_ensemble_position_of_this_trace: 0\nfor_3d_poststack_data_this_field_is_for_in_line_number: 0\nfor_3d_poststack_data_this_field_is_for_cross_line_number: 0\nshotpoint_number: 0\nscalar_to_be_applied_to_the_shotpoint_number: 0\ntrace_value_measurement_unit: 0\ntransduction_constant_mantissa: 0\ntransduction_constant_exponent: 0\ntransduction_units: 0\ndevice_trace_identifier: 0\nscalar_to_be_applied_to_times: 0\nsource_type_orientation: 0\nsource_energy_direction_mantissa: 0\nsource_energy_direction_exponent: 0\nsource_measurement_mantissa: 0\nsource_measurement_exponent: 0\nsource_measurement_unit: 0\n\n\n\n\nC. Write out the text data as segy\n\n# Group all the text traces by their the i-j coordinates\ngroups=df.groupby(['i','j'])\nprint(len(groups))\n\n#Here I notice there are only 13192 traces in the text data, compared with the 216540 in the segy data...\n\n13192\n\n\n\n%%time\n\n#Make a stream object (flush it out to begin because we have used this variable names for demos)\nstream_out = None \nstream_out = Stream()\n\n#not sure how to group the trace ensembles but can use a counter to keep track of them\nensemble_number = 0\n       \nfor ids,df_trace in groups:\n    #ids are the i, j coordinate locations\n    #trc is the subset of the full dataframe for just that i-j location\n\n    #For each i-j location, a trace is impdence at all the depth values, i.e.\n    data = df_trace.p.values\n\n    # Enforce correct byte number and set to the Trace object\n    data = np.require(data, dtype=np.float32)\n    trace = Trace(data=data)\n\n    # Set all the segy header information\n    # Attributes in trace.stats will overwrite everything in trace.stats.segy.trace_header\n    trace.stats.delta = 0.01\n    trace.stats.starttime = UTCDateTime(1970,1,1,0,0,0)\n\n    # If you want to set some additional attributes in the trace header,\n    # add one and only set the attributes you want to be set. Otherwise the\n    # header will be created for you with default values.\n    if not hasattr(trace.stats, 'segy.trace_header'):\n        trace.stats.segy = {}\n\n    trace.stats.segy.trace_header = SEGYTraceHeader()\n\n#         trace.stats.segy.trace_header.trace_sequence_number_within_line = index + 1\n#         trace.stats.segy.trace_header.receiver_group_elevation = 0\n    trace.stats.segy.trace_header.source_coordinate_x = int(df_trace.x.values[0])\n    trace.stats.segy.trace_header.source_coordinate_y = int(df_trace.y.values[0])\n    trace.stats.segy.trace_header.ensemble_number = ensemble_number #Not sure how this is actually determined\n    trace.stats.segy.trace_header.lag_time_A = 2400\n    trace.stats.segy.trace_header.lag_time_B = 3000\n    trace.stats.segy.trace_header.number_of_samples_in_this_trace = len(data)\n\n    ensemble_number +=1\n\n    # Add trace to stream\n    stream_out.append(trace)\n\n# A SEGY file has file wide headers. This can be attached to the stream\n# object.  If these are not set, they will be autocreated with default\n# values.\nstream_out.stats = AttribDict()\nstream_out.stats.textual_file_header = 'Textual Header!'\nstream_out.stats.binary_file_header = SEGYBinaryFileHeader()\nstream_out.stats.binary_file_header.trace_sorting_code = 5\n# stream.stats.binary_file_header.number_of_data_traces_per_ensemble=1\n\nprint(\"Stream object before writing...\")\nprint(stream_out)\n\nstream_out.write(\"TEST.sgy\", format=\"SEGY\", data_encoding=1, byteorder=sys.byteorder)\n\nprint(\"Stream object after writing. Will have some segy attributes...\")\nprint(stream_out)\n\nStream object before writing...\n13192 Trace(s) in Stream:\n\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n...\n(13190 other traces)\n...\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\nStream object after writing. Will have some segy attributes...\n13192 Trace(s) in Stream:\n\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n...\n(13190 other traces)\n...\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:02.990000Z | 100.0 Hz, 300 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\nCPU times: total: 7.55 s\nWall time: 7.52 s\n\n\n\n#Now check it\nprint(\"Reading using obspy.io.segy...\")\nst1 = _read_segy(\"TEST.sgy\")\nprint(st1)\n\nprint(np.shape(st1.traces))\nst1\n\nReading using obspy.io.segy...\n13192 traces in the SEG Y structure.\n(13192,)\n\n\n13192 traces in the SEG Y structure.\n\n\n\none_trace = st1.traces[0]\n\n#Print out details single trace\nprint(one_trace)\n\n#Plot a single trace\nplt.figure(figsize=(16,2))\nplt.plot(one_trace.data)\nplt.show()\n\nTrace sequence number within line: 0\n300 samples, dtype=float32, 100.00 Hz\n\n\n\n\n\n\n\nBonus - why is there a data discrepency\nI would have expected the traces from the segy data and from the gslib data to match up. But there are only 13192 traces in the text data and 216540 in the segy data. Something weird is going. I can plot these side by side and see if some data are missing.\n\nxx=[]\nyy=[]\nfor i in range(len(stream.traces)):\n    xx.append(stream.traces[i].header.source_coordinate_x)\n    yy.append(stream.traces[i].header.source_coordinate_y)  \n\n\nplt.scatter(xx,yy)\nplt.scatter(df.x,df.y)\n\n&lt;matplotlib.collections.PathCollection at 0x2d03be3b550&gt;\n\n\n\n\n\nAt first glance this seems ok. The data coverage is over the same area, the edges are missing buuuuut zooming in we can see the text data is not as dense as the segy data. Problem solved.\n\nplt.scatter(xx,yy,s=0.1)\nplt.scatter(df.x,df.y,s=0.1)\n\nplt.xlim([433000,434000])\n# plt.ylim([433000,434000])\n\n(433000.0, 434000.0)\n\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python for Geoscience",
    "section": "",
    "text": "This course is aimed at researchers, students, and industry professionals who want to learn about the capabilities of Python and get experience using it applied to real-world problems. This course will introduce you to foundations of Python programming. We will utilise common geoscience data types (geospatial, temporal, vector, raster, etc) to demonstrate a variety of practical workflows and showcase fundamental capabilities of Python. We will carry out exploratory, analytical, computational and machine learning analyses on these datasets. At the end of the course you will be able to adapt these workflows to your own datasets.\nThe course is presented by the Sydney Informatics Hub on behalf of the Petroleum Exploration Society of Australia.\nThe Sydney Informatics Hub (SIH) is a Core Research Facility of the University of Sydney. Core Research Facilities centralise essential research equipment and services that would otherwise be too expensive or impractical for individual Faculties to purchase and maintain. We provide a wide range of research services to aid investigators, such as:\nWe also aim to cultivate a data community, organising monthly Hacky Hours, outside training events (eg NVIDIA, Pawsey Center), and data/coding-related events. Look out for everything happening on our calendar or contact us (at sih.info@sydney.edu.au) to get some digital collaboration going.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Introduction to Python for Geoscience",
    "section": "Trainers",
    "text": "Trainers\n\nNathaniel (Nate) Butterworth, nathaniel.butterworth@sydney.edu.au\nJulian Giordani\nThomas Mauch"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Introduction to Python for Geoscience",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nNo previous programming experience is required, but Session 1 is a pre-requisite for the other sessions. Training will be delivered online, so you will need access to a modern computer with a stable internet connection and around 5GB of storage space for data downloaded prior to the course. Participants are encouraged to setup a Python environment on their local computer (as per the Setup Instructions provided), but participation using other platforms/environments can be supported where necessary."
  },
  {
    "objectID": "index.html#venue-online-via-zoom",
    "href": "index.html#venue-online-via-zoom",
    "title": "Introduction to Python for Geoscience",
    "section": "Venue, online via Zoom",
    "text": "Venue, online via Zoom\nParticipants will be provided with a Zoom link. Trainers will be broadcasting from Sydney.\n\nZoom etiquette and how we interact\nSessions will be recorded for attendees only, and it is set up to only record the host shared screen and host audio. We will try and get these uploaded to this site as soon as possible. Please interrupt whenever you want! Ideally, have your camera on and interact as much as possible. There will be someone monitoring the chat-window with any questions you would like to post there. Four hours is a long Zoom session so we have plenty of scheduled breaks combined with a mix of content to be delivered as demos, plus sections as independent exercises, but most of the course will be pretty-hands on with everyone writing their own code. We will use Zoom break-out rooms as needed with the Trainers and participants."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Introduction to Python for Geoscience",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available at https://pages.github.sydney.edu.au/informatics/sih_codeofconduct/"
  },
  {
    "objectID": "index.html#general-session-timings",
    "href": "index.html#general-session-timings",
    "title": "Introduction to Python for Geoscience",
    "section": "General session timings",
    "text": "General session timings\n\nStart at 12:00pm promptly.\n5 min to discuss feedback and summarise previous week.\n55 min of content.\n5 min break.\n55 min content\n5 min break\n55 min content"
  },
  {
    "objectID": "index.html#date-time-in-aest-brisbane-time",
    "href": "index.html#date-time-in-aest-brisbane-time",
    "title": "Introduction to Python for Geoscience",
    "section": "Date & Time (in AEST-Brisbane time):",
    "text": "Date & Time (in AEST-Brisbane time):\nWe will be working through the following general content over our 4 sessions together.\nSession 01 Python fundamentals Tues Aug 15, 12:00 pm - 3:00 pm (AEST)\nSession 02 Specialist Python libraries and data analysis for geoscience Tues Aug 22, 12:00 pm - 3:00 pm (AEST)\nSession 03 Pattern recognition and prediction in geoscience Tues Aug 29, 12:00 pm - 3:00 pm (AEST)\nSession 04 Large data and long running workflow strategies Tues Sep 05, 12:00 pm - 3:00 pm (AEST)"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Introduction to Python for Geoscience",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP."
  },
  {
    "objectID": "JupyterhubOnAWS.html",
    "href": "JupyterhubOnAWS.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "The first steps are adapted from TLJH instuctions here\n\nSelect a Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami- instance (the one maintained by Canonical as per the TLJH screenshots in the documentation)\nChoose a t3.small node type for testing - it’s the smallest that supports TLJH. Note that this does not have a free tier, but running it for a few hours costs approximately $0.10 in my experience.\nFor “production” PESA try m5a.8xlarge in US-Ohio (this gives 32 vCPU and 128 Gb RAM).\n\nIn theory, you are meant to be able to use a custom command to install TLJH in one go, and paste it into the user script box. Unfortunately, none of the flags have worked for me, and the installation script itself sometimes works and sometimes doesn’t - so I recommend doing this over the ssh after the instance has been created.\n#!/bin/bash\ncurl -L https://tljh.jupyter.org/bootstrap.py \\\n  | sudo python3 - \\\n    --admin myusername --show-progress-page --plugin tljh-repo2docker # can also in theory use a requirements.txt file here\n\nWhen selecting storage, request General Purpose SSD (gp2) for most workloads; according to TLJH Provisioned IOPS SSD (io1) is the highest-performant when performance is critical. I have only used gp2 so far.\n[Only the first time you set up] After downloading the AWS .pem file, make sure to chmod 600 it as otherwise you get a permissions error.\n\nchmod 600 ~/Desktop/pesaaws1.pem\n\n\n\n\n\n\nssh into the server via the Public IPv4 DNS provided in the instance details. ubuntu is the admin username for the root ubuntu user, and you’ll need root privileges for the below\n\nssh -i ~/Desktop/pesaaws1.pem ubuntu@ec2-18-220-114-100.us-east-2.compute.amazonaws.com\n\n\n\nimage-20210423141931861\n\n\n\nInstall TLJH using the following command\n\ncurl -L https://tljh.jupyter.org/bootstrap.py \\\n  | sudo python3 - \\\n    --admin myusername #--user-requirements-txt-url https://raw.githubusercontent.com/data-8/materials-sp18/master/requirements.txt\n\n\n\n\n\n\nCreate a folder as recommended in these instructions\nsudo mkdir -p /srv/\n# you may need to chmod it but I can't figure out the optimal permissions\n# sudo chmod 777 data\nOpen a terminal on your local machine and upload the data to the AWS machine with the following command:\nsudo scp -r -i ~/Desktop/pesaaws1.pem * ubuntu@ec2-3-141-195-91.us-east-2.compute.amazonaws.com:/srv/data/\nTo grab from cloudstor (recommended):\ncd /srv/\nsudo wget https://cloudstor.aarnet.edu.au/plus/s/62la9C8dn5vjAaa/download\nsudo apt-get install -y unzip\nsudo unzip download\nThen create a soft link to the data directory with the following command.\nsudo ln -s /srv/geopython/data /etc/skel/data\nsudo ln -s /srv/geopython/notebooks /etc/skel/notebooks\nIt should now be visible for any new user created.\nThe geopython workshop also requires graphviz\nsudo apt install graphviz\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "JupyterhubOnAWS.html#instructor-instructions---setting-up-jupyterhub-on-aws",
    "href": "JupyterhubOnAWS.html#instructor-instructions---setting-up-jupyterhub-on-aws",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "The first steps are adapted from TLJH instuctions here\n\nSelect a Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami- instance (the one maintained by Canonical as per the TLJH screenshots in the documentation)\nChoose a t3.small node type for testing - it’s the smallest that supports TLJH. Note that this does not have a free tier, but running it for a few hours costs approximately $0.10 in my experience.\nFor “production” PESA try m5a.8xlarge in US-Ohio (this gives 32 vCPU and 128 Gb RAM).\n\nIn theory, you are meant to be able to use a custom command to install TLJH in one go, and paste it into the user script box. Unfortunately, none of the flags have worked for me, and the installation script itself sometimes works and sometimes doesn’t - so I recommend doing this over the ssh after the instance has been created.\n#!/bin/bash\ncurl -L https://tljh.jupyter.org/bootstrap.py \\\n  | sudo python3 - \\\n    --admin myusername --show-progress-page --plugin tljh-repo2docker # can also in theory use a requirements.txt file here\n\nWhen selecting storage, request General Purpose SSD (gp2) for most workloads; according to TLJH Provisioned IOPS SSD (io1) is the highest-performant when performance is critical. I have only used gp2 so far.\n[Only the first time you set up] After downloading the AWS .pem file, make sure to chmod 600 it as otherwise you get a permissions error.\n\nchmod 600 ~/Desktop/pesaaws1.pem\n\n\n\n\n\n\nssh into the server via the Public IPv4 DNS provided in the instance details. ubuntu is the admin username for the root ubuntu user, and you’ll need root privileges for the below\n\nssh -i ~/Desktop/pesaaws1.pem ubuntu@ec2-18-220-114-100.us-east-2.compute.amazonaws.com\n\n\n\nimage-20210423141931861\n\n\n\nInstall TLJH using the following command\n\ncurl -L https://tljh.jupyter.org/bootstrap.py \\\n  | sudo python3 - \\\n    --admin myusername #--user-requirements-txt-url https://raw.githubusercontent.com/data-8/materials-sp18/master/requirements.txt\n\n\n\n\n\n\nCreate a folder as recommended in these instructions\nsudo mkdir -p /srv/\n# you may need to chmod it but I can't figure out the optimal permissions\n# sudo chmod 777 data\nOpen a terminal on your local machine and upload the data to the AWS machine with the following command:\nsudo scp -r -i ~/Desktop/pesaaws1.pem * ubuntu@ec2-3-141-195-91.us-east-2.compute.amazonaws.com:/srv/data/\nTo grab from cloudstor (recommended):\ncd /srv/\nsudo wget https://cloudstor.aarnet.edu.au/plus/s/62la9C8dn5vjAaa/download\nsudo apt-get install -y unzip\nsudo unzip download\nThen create a soft link to the data directory with the following command.\nsudo ln -s /srv/geopython/data /etc/skel/data\nsudo ln -s /srv/geopython/notebooks /etc/skel/notebooks\nIt should now be visible for any new user created.\nThe geopython workshop also requires graphviz\nsudo apt install graphviz"
  },
  {
    "objectID": "JupyterhubOnAWS.html#installing-packages",
    "href": "JupyterhubOnAWS.html#installing-packages",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Installing packages",
    "text": "Installing packages\nInstall the packages you need into the base environment.\nWe use the Anaconda package manager for our training, and share installation instructions using a conda environment.yml file. If you instead have a pip requirements.txt file, you could have used the --user-requirements-txt-url flag at the end of the TLJH install above to install all of the packages in one go.\nModify the base conda environment for all users:\nexport PATH=/opt/tljh/user/bin:${PATH}\nsudo -E conda env update -n base -f /srv/geopython/environment.yml"
  },
  {
    "objectID": "JupyterhubOnAWS.html#adding-users",
    "href": "JupyterhubOnAWS.html#adding-users",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "Adding users",
    "text": "Adding users\nLaunch the jupyterhub by following the Public IPv4 address indicated for the EC2 instance, changing the https: at the beginning of the url to http:\nIgnoring the unsecurity warnings, log in using the administrator login and a random password (make sure to write this down) to see the jupyter hub.\nGo to the Control Panel -&gt; Admin interface and click the Add Users button.\nManually paste usernames you’d like into the box.\n\nNotes on getting our specific notebooks to work\n#Import dask dataframe modules\nimport dask.dataframe as dd\n#NOTE: to run this example (with diagrams) you will need to \"pip install graphviz\" and donwload graphviz\n#https://graphviz.org/download/\nimport os\n# Nate has the below; replace\n#os.environ[\"PATH\"] += os.pathsep + 'C:/APPS/Graphviz/bin'\nos.chdir('/home/jupyter-user8/')\n# may need to replace data file to dd.read_csv(\"data/ml_data_points.csv\")\nNote: mpi example at end of notebook 4 will not work\n\n\nUsing cloudwatch agent to monitor usage\nNeeded to create an IAM role as describe here once"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "We generally use and recommend the Miniconda Python distribution: https://docs.conda.io/en/latest/miniconda.html. But feel free to use whatever one works for you (and the course materials). We will be using Miniconda3-py39_4.11.0.\nYou can get this specific version here for:\n\nWindows 64 bit Download\nMac OSX Download\nLinux Download\n\nFollow the prompts (the default recommendations in the installer are generally fine.) Once installed, launch an “Anaconda Prompt” from the Start Menu / Applications Folder to begin your Python adventure.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html#launching-the-jupyterpython-notebook",
    "href": "setup.html#launching-the-jupyterpython-notebook",
    "title": "Setup",
    "section": "Launching the Jupyter/Python Notebook",
    "text": "Launching the Jupyter/Python Notebook\nNow you have built your environment with all the packages we need, you can launch it. We will be working mostly with Python Notebooks to run Python (as opposed to running an interpreter on the command line/prompt). Each time you restart your work you will have to follow these steps:\n\nLaunch an Anaconda Prompt (or equivalent).\nChange directories to your workspace.\nActivate the geopy environment.\nLaunch the Jupyter/Python Notebook server.\n\ncd C:\\Users\\nbutter\\Desktop\\geopython\nconda activate geopy\njupyter notebook\n\nThis will launch the Notebook server (and may automatically launch a web-browser and take you to the page). If the Notebook does not automatically start, copy the generated link into a browser."
  },
  {
    "objectID": "setup.html#jupyter-hub-in-the-cloud",
    "href": "setup.html#jupyter-hub-in-the-cloud",
    "title": "Setup",
    "section": "Jupyter Hub in the Cloud",
    "text": "Jupyter Hub in the Cloud\nIf the above options do not work for you, you can use an on-demand cloud instance. You will be given a web link, login with provided credentials. Done."
  },
  {
    "objectID": "setup.html#docker",
    "href": "setup.html#docker",
    "title": "Setup",
    "section": "Docker",
    "text": "Docker\nIf you are familiar with Docker you may use our Docker image with something like:\nsudo docker run -it -p 8888:8888 nbutter/geopy:pesa2022 /bin/bash -c \"jupyter notebook --allow-root --ip=0.0.0.0 --no-browser\"\nThis will launch the Python notebook server in the /notebooks folder. Access the notebook by entering the generated link in a web-browser, e.g. http://127.0.0.1:8888/?token=9b16287ab91dc69d6b265e6c9c31a49586a35291bb20d0ab"
  },
  {
    "objectID": "notebooks/hdfeos.html",
    "href": "notebooks/hdfeos.html",
    "title": "Sydney Informatics Hub Python for Geoscience",
    "section": "",
    "text": "#Examples from https://www.hdfeos.org/zoo/index_openOBPG_Examples.php#MODIST\n\n#Download swath data\ndata_grid = \"https://gamma.hdfgroup.org/ftp/pub/outgoing/NASAHDF/T2010001000000.L2_LAC_SST.nc\"\n    \n#Download grid data\n#https://gamma.hdfgroup.org/ftp/pub/outgoing/NASAHDF/T20000322000060.L3m_MO_NSST_sst_4km.nc\n    \nimport urllib.request\nurllib.request.urlretrieve(data_grid, \"../data/demo.nc\")\n\n('../data/demo.nc', &lt;http.client.HTTPMessage at 0x7f95c8145340&gt;)\n\n\n\n# Install these two extra packages into the \"geopy\" environment\n!pip install basemap\n!pip install netCDF4\n\n\n\"\"\"\n\nThis example code illustrates how to access and visualize a OBPG MODIS Terra\n Grid netCDF-4 file in Python.\n\nIf you have any questions, suggestions, or comments on this example, please use\nthe HDF-EOS Forum (http://hdfeos.org/forums).  If you would like to see an\nexample of any other NASA HDF/HDF-EOS data product that is not listed in the\nHDF-EOS Comprehensive Examples page (http://hdfeos.org/zoo), feel free to\ncontact us at eoshelp@hdfgroup.org or post it at the HDF-EOS Forum\n(http://hdfeos.org/forums).\n\nUsage: save this script and run\n\n    $python T20000322000060.L3m_MO_NSST_sst_4km.nc.py\n\nTested under: Python 3.7.3 :: Anaconda custom (x86_64)\nLast Update: 2019-12-12\n\"\"\"\n\nimport os\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom netCDF4 import Dataset\nfrom mpl_toolkits.basemap import Basemap\n\n# Open netCDF-4 file.\nFILE_NAME = '../data/T20000322000060.L3m_MO_NSST_sst_4km.nc'\nnc = Dataset(FILE_NAME)\n\n# Read dataset.\nDATAFIELD_NAME='sst'\ndset = nc.variables[DATAFIELD_NAME]\ndata = dset[:]\nlatitude = nc.variables['lat'][:]\nlongitude = nc.variables['lon'][:]\n\n# Dataset is too big for plotting.\n# Subset every n-th point to visualize data.\nn = 2\ndata = data[::n, ::n]\nlatitude = latitude[::n]\nlongitude = longitude[::n]\n\nm = Basemap(projection='cyl', resolution='l',\n            llcrnrlat=-90, urcrnrlat = 90,\n            llcrnrlon=-180, urcrnrlon = 180)\nm.drawcoastlines(linewidth=0.5)\nm.drawparallels(np.arange(-90, 91, 45))\nm.drawmeridians(np.arange(-180, 180, 45), labels=[True,False,False,True])\nm.pcolormesh(longitude, latitude, data, latlon=True)\ncb = m.colorbar()\ndset = nc.variables[DATAFIELD_NAME]\nunits = dset.units\ncb.set_label('Unit: '+units)\nlong_name = dset.long_name\nplt.title('{0}\\n {1}'.format(FILE_NAME, long_name))\nfig = plt.gcf()\n\n# Save plot.\npngfile = \"{0}.py.png\".format(FILE_NAME)\nfig.savefig(pngfile)\n\n\n\n\n\n\"\"\"\n\nThis example code illustrates how to access and visualize a OBPG TERRA MODIS\n Swath netCDF-4 file in Python.\n\nIf you have any questions, suggestions, or comments on this example, please use\nthe HDF-EOS Forum (http://hdfeos.org/forums).  If you would like to see an\nexample of any other NASA HDF/HDF-EOS data product that is not listed in the\nHDF-EOS Comprehensive Examples page (http://hdfeos.org/zoo), feel free to\ncontact us at eoshelp@hdfgroup.org or post it at the HDF-EOS Forum\n(http://hdfeos.org/forums).\n\nUsage:  save this script and run\n\n    $python T2010001000000.L2_LAC_SST.nc.py\n\nTested under: Python 3.7.3 :: Anaconda custom (x86_64)\nLast Update: 2019-12-11\n\"\"\"\n\nimport os\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom netCDF4 import Dataset\nfrom mpl_toolkits.basemap import Basemap\n\n# Open netCDF-4 file.\nFILE_NAME = '../data/T2010001000000.L2_LAC_SST.nc'\nnc = Dataset(FILE_NAME)\n\n# Read dataset.\nDATAFIELD_NAME='sst'\ng = nc.groups['geophysical_data']\ndata = g.variables[DATAFIELD_NAME][:]\nn = nc.groups['navigation_data']\nlatitude = n.variables['latitude'][:]\nlongitude = n.variables['longitude'][:]\nm = Basemap(projection='cyl', resolution='l',\n            llcrnrlat=-90, urcrnrlat = 90,\n            llcrnrlon=-180, urcrnrlon = 180)\nm.drawcoastlines(linewidth=0.5)\nm.drawparallels(np.arange(-90, 91, 45))\nm.drawmeridians(np.arange(-180, 180, 45), labels=[True,False,False,True])\nm.scatter(longitude, latitude, c=data, s=0.05, cmap=plt.cm.jet,\n           edgecolors=None, linewidth=0)\ncb = m.colorbar()\ndset = g.variables[DATAFIELD_NAME]\nunits = dset.units\ncb.set_label('Unit: '+units)\nlong_name = dset.long_name\nplt.title('{0}\\n {1}'.format(FILE_NAME, long_name))\nfig = plt.gcf()\n\n# Save plot.\npngfile = \"{0}.py.png\".format(FILE_NAME)\nfig.savefig(pngfile)\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01a-fundamentals.html",
    "href": "notebooks/01a-fundamentals.html",
    "title": "Python fundamentals",
    "section": "",
    "text": "Questions\n\nWhat can Python do?\nHow do I do it?\n\n\n\nObjectives\n\nLearn the basic Python commands\n\n\nGenerally, cells like this are what to type into your Python shell/notebook/colab:\n\n2+4*10\n\n42\n\n\nCells like the above are the expected output.\nLook at that, your first Python code! Who knew Python is just a big ol’ calculator! But it can do so much more…\n\n#This is a comment. This is for a human to read (so you remember what your code does!)\n#Python ignores anything behind the '#'.\n\n#The next line is an example of a 'variable'. Assign values using a single '=' sign.\ntimeStart=145\n\nNote: A variable (e.g. timeStart) must start with a letter or underscore, and can include a number\n\n#Now you can use that variable in different ways.... firstly print it out to the screen\nprint(\"The age of the sample is\", timeStart, \" Million years\")\n\nThe age of the sample is 145  Million years\n\n\n\n#Make a new variable called 'endtime' and add a constant to our 'timeStart' variable\nendtime=timeStart+56\n\nNothing printed out above? Good. Jupyter Notebooks won’t always do that so let’s tell Python to print it.\n\nprint(endtime)\n\n201\n\n\n\n#Make a new 'string' variable\ngeological_age='Jurassic'\n\n#Print out some useful information that includes our different variables\nprint(\"My sample is \", endtime, \" Million years old, from the\", geological_age, \" age.\")\n\nMy sample is  201  Million years old, from the Jurassic  age.\n\n\n\n#Make a Python List object, similar to an array.\ntimes=[1,4.5,5+3.2,geological_age,\"Another string\",True]\n\nprint(times)\n\n#There are many different types of data types and objects: \n#int, long, float, complex, NaN, logical, String, Lists, Tuple, Dictionary, functions, classes, etc\n\n[1, 4.5, 8.2, 'Jurassic', 'Another string', True]\n\n\n\n#indexing\nprint(times[0])\nprint(times[4])\nprint(times[4][0])\n\n1\nAnother string\nA\n\n\n\nFunctions\nThese are bits of code you want to perhaps use many times, or keep self contained, or refer to at different points. They can take values as input and give values back (or not).\n\n#Declare the name of the function\ndef add_numbers(x,y):\n    '''adds two numbers\n    usage: myaddition=addnumbers(x,y)\n    returns: z\n    inputs: x,y\n    x and y are two integers\n    z is the summation of x and y\n    '''\n    \n    z=x+y\n    \n    return(z)\n\nNote the indentation - Python forces your code to be nicely readable by using ‘whitespace’/indentation to signify what chunks of code are related. You will see this more later, but generally you should try and write readable code and follow style standards\nMany functions have a header - formatted as a multiline comment with three ’’’. This hopefully will tell you about the function\nAnyway, let’s run our function, now that we have initialised it!\n\nadd_numbers(1,2)\n\n3\n\n\n\n\nChallenge\nWrite a function to convert map scale. For example, on a 1:25,000 map (good for hiking!) the distance between two points is 15 cm. How far apart are these in real life? (3750 m).\n[Reminder: 15 cm * 25000 = 375000 cm = 3750 m]\nYour function should take as input two numbers: the distance on the map (in cm) and the second number of the scale and, i.e. calculate_distance(15, 25000) should return 375000\n\n\nSolution\n\n#Declare the name of the function\ndef calculate_distance(distance_cm,scale):\n    '''calculates distance based on map and scale\n    returns: z\n    inputs: distance_cm,scale\n    distance_cm and scale are two integers\n    returns the product of distance_cm and scale\n    '''  \n    \n    return(distance_cm * scale)\n\n\n\nLoops, operators, conditions\nPython is great for doing something a million times. It can be useful if you have many samples/data points and you want to operate or manipulate those points.\n\n#Loop through our list 'times' that we defined above\nfor mything in times:\n    print(mything)\n\n1\n4.5\n8.2\nJurassic\nAnother string\nTrue\n\n\nSometimes you need to loop through a list, but simultaneously keep track of which index you’re up to.\n\nfor myindex, mything in enumerate(times):\n    print(\"index:\",myindex,\" The thing in my 'times' list:\",mything)\n\nindex: 0  The thing in my 'times' list: 1\nindex: 1  The thing in my 'times' list: 4.5\nindex: 2  The thing in my 'times' list: 8.2\nindex: 3  The thing in my 'times' list: Jurassic\nindex: 4  The thing in my 'times' list: Another string\nindex: 5  The thing in my 'times' list: True\n\n\nYou don’t always need a pre-defined list\n\nage=140\n# What is the value of \"timeStart\" ?\n# age &lt; timeStart is a \"logical\" data-type. It's either True or False\nwhile age &lt; timeStart:\n    print(\"time:\", timeStart, \" age:\", age, \" difference:\",timeStart-age)\n    #Increment the age variable\n    age=age+1\n\ntime: 145  age: 140  difference: 5\ntime: 145  age: 141  difference: 4\ntime: 145  age: 142  difference: 3\ntime: 145  age: 143  difference: 2\ntime: 145  age: 144  difference: 1\n\n\n\n\nControl statements\nControl statements include the functions if, for, while, try.\n\n#Control statements: if, for, while, try, \nif timeStart &lt; 200:\n    print(timeStart)\n\n145\n\n\n\nif timeStart &lt;= 200:\n    print(geological_age)\nelif timeStart &gt; 200:\n    print(\"Triassic age\")\nelse:\n    pass #This option is not necessarily needed, but can be useful in some scenarios\n\nJurassic\n\n\n\n#Another function\ndef timescale(t):\n    print(t)\n    if (t &lt;= 4500) & (t &gt; 2500):\n        return(\"Archean\")\n\n    elif (t &lt;=2500) & (t &gt; 541):\n        return(\"Proterozoic\")\n\n    elif (t &lt;= 541) & (t &gt; 252):\n        return(\"Palaeozoic\")\n\n    elif (t &lt;=252) & (t &gt; 65):\n        return(\"Mesozoic\")\n\n    elif (t &lt;=65) & (t &gt;= 0):\n        return(\"Cenozoic\")\n\n    else:\n        print (\"Expect number between 0 and 4500, got:\",t)\n        return(float('nan'))\n    \ntimescale(1)\n\n1\n\n\n'Cenozoic'\n\n\nThat is the basics. Now we are going to load in some data and manipulate it.\n\n\nDealing with data\n\n#First we have to load some modules to do the work for us.\n#Modules are packages people have written so we do not have to re-invent everything!\n\n#The first is NUMerical PYthon. A very popular matrix, math, array and data manipulation library.\nimport numpy\n\n#This is a library for making figures (originally based off Matlab plotting routines)\n#We use the alias 'plt' because we don't want to type out the whole name every time we reference it!\nimport matplotlib.pyplot as plt \n\n\n#Set the variable name for the file we are loading in. \n#It is in the 'data' directory, and the file is called EarthChemCU.txt. \n#We are currently working in /notebooks.\nfilename = '../data/EarthChemCU.txt'\n\n#Now read in the data\n# loadtxt() is a function that we can now use because we loaded the library called numpy\nchemdata=numpy.loadtxt(filename, delimiter=',')\n#chemdata &lt;- the name of a variable we are making that will hold the table of data\n#filename &lt;- this is the name of the variable we declared above\n#delimiter &lt;- this is a csv file\n\n\nWant more details about a command/function we use?\n\n#Try these help commands\n#help(numpy.loadtxt)\n#?numpy.loadtxt\n\nOr really, search the function! Online documentation and discussion boards are filled with great content.\n\n\nExploring your data\nIt is often a good idea to look at the data to have some idea of what you are working with\n\n#What does the data look like. Print it out\nprint(chemdata)\n\n[[ 3.92583e+01 -1.14992e+02  1.11000e+02  1.96000e+04]\n [ 3.92583e+01 -1.14992e+02  1.11000e+02  1.57000e+04]\n [ 4.12060e+01 -1.17272e+02  1.05000e+02  3.00000e+00]\n ...\n [ 2.00530e+01  1.17419e+02  0.00000e+00  3.00000e+01]\n [ 2.00530e+01  1.17419e+02  0.00000e+00  3.30000e+01]\n [ 2.00530e+01  1.17419e+02  0.00000e+00  3.50000e+01]]\n\n\nThis data is in the style: Latitude (degrees), Longitude (degrees -180:180), Age (Ma), Copper abundance (ppm)\n\n#Print the dimensions of the data\nprint(chemdata.shape)\n\n(207431, 4)\n\n\n207431 rows!\n\n\nAccessing data from an array\nchemdata is a table of data: an array with two dimensions. So to access/look at/change parts of it, we need to specify both row and column\n\n#Print the number in the first row and third column \n#(note indexing is different in \"numpy arrays\" compared \"python lists\". \n#IMPORTANT: Python counts from 0\nprint(chemdata[0,2])\n\n111.0\n\n\n\n#Print the first row\nprint(chemdata[0,:])\n\n[   39.2583  -114.992    111.     19600.    ]\n\n\n\n#Print the third column\nprint(chemdata[:,2])\n\n[111. 111. 105. ...   0.   0.   0.]\n\n\n\n#Print the first two columns for row id 2, 5 and 6. \nprint(chemdata[[2,5,6],0:2])\n\n[[  41.206 -117.272]\n [  41.186 -117.417]\n [  41.177 -117.485]]\n\n\n\n\nChallenge\nPrint the second and third columns for row 20-30.\n\n\nSolution\n\n#The indexing counts from [start:end]\n#where \"start\" is included and \"end\" is excluded!\n#Assuming we want row 30, then you need to\n#include index 29 (i.e. set the end index to 30!)\n#Same with columns, we want column 2 (index 1) and\n#column 3 (index 2) so make our slice 1:3\n\nprint(chemdata[19:30,1:3])\n\n\n\nPlotting data\nNow to make our first plot!\n\n#Plot the lats and lons, i.e. the first column vs the second column\nplt.plot(chemdata[:,1],chemdata[:,0],'k.')\nplt.title('Copper Deposit Data')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\n\nThis does not look right… It is a messy dataset! This is not uncommon. Maybe the Lats/Lons are stored as Northings/Eastings for some samples? Maybe they are missing a decimal place?\nAnyway, Python is a great tool to clean things up! Let’s investigate further.\n\n#Plot the Latitudes\nplt.plot(chemdata[:,0])\nplt.ylabel('Latitude')\nplt.xlabel('Number')\nplt.show()\n\n#Plot the Longitudes\nplt.plot(chemdata[:,1],'r')\nplt.ylabel('Longitude')\nplt.xlabel('Number')\nplt.show()\n\n\n\n\n\n\n\nThis kind of casual data interrogation is a really handy way to exploring your data. There are definitely some outliers with latitudes and longitudes. There are quite a few ways clean the data, but let’s simply restrict our data range to -180:180 and -90:90.\n\n#Clean up the data, remove anything outside lat lon extent\n\n#Find all the \"chemdata\" column 1 (i.e. longitude) data points that are greater than -180, save it in a new variable\n#Using a succinct method in two lines\ndatamask = ((chemdata[:,0] &lt; 90)\n            & (chemdata[:,0] &gt; -90)\n            & (chemdata[:,1] &lt; 180)\n            & (chemdata[:,1] &gt; -180))\n            \ncudata4 = chemdata[datamask]\n\n\nprint(\"We have removed\", chemdata.shape[0]-cudata4.shape[0], \"samples\")\n\nWe have removed 47 samples\n\n\n\nplt.plot(cudata4[:,1],cudata4[:,0],'k.')\nplt.title('Copper Deposits from EarthChem.org')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\n\nNow make a more informative plot:\n\n#Set reasonable variable names\nlats=cudata4[:,0]\nlongs=cudata4[:,1]\nage=cudata4[:,2]\ncopper=cudata4[:,3]\n\n#lats_rich=lats[copper&gt;2]\n\nfig = plt.figure(figsize=(6,4),dpi=150)\n\n#Restrict the colour range between 0 and 100 (ppm?)\nplt.scatter(longs,lats,s=age/1000,c=copper,vmin=0, vmax=100,cmap=plt.cm.copper)\nplt.title('Copper Deposits from EarthChem.org')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\n\n\n#You could come up with a more intelligent way to reject your outliers, e.g.\nimport numpy as np\ndef reject_outliers(data):\n    m = 2\n    u = np.mean(data)\n    print(\"mean is:\", u)\n    s = np.std(data)\n    print(\"std is:\", s)\n    filtered = [e for e in data if (u - 2 * s &lt; e &lt; u + 2 * s)]\n    print(\"removed:\",np.shape(data)[0] - np.shape(filtered)[0])\n    return(filtered)\n\nfiltered_age=reject_outliers(copper)\n\nmean is: 408.55060226439844\nstd is: 6032.1541529827555\nremoved: 1163\n\n\nJust plotting the Cu content implies that better filtering could be applied (a homework exercise perhaps). Remember this is a pretty messy dataset, some Cu is reported as ppm, ppb, or %!\n\nplt.plot(copper[copper&gt;1],'k.')\nplt.show()\n\n\n\n\n\n\nLet’s make an even nicer map\n\n#Import another module called Cartopy - great for plotting things on globes\nimport cartopy.crs as ccrs\n\n#Make new variables from our array (so it is easier to see what we are doing)\nlats=cudata4[:,0]\nlongs=cudata4[:,1]\nage=cudata4[:,2]\n\n#######\n## Make the figure\n#######\n\n#Create a figure object\nfig = plt.figure(figsize=(12,8),dpi=150)\n\n#Make a map projection to plot on.\nax = plt.axes(projection=ccrs.Robinson())\n\n#Add some Earth-specific details (from the cartopy package)\nax.set_global()\nax.coastlines('50m', linewidth=0.8)\nax.stock_img()\nax.gridlines()\n\n#Make a scatter plot of the data coloured by age. \n#Restrict the colour range between 0 and 100 (Ma)\n#And also set the scatter plot as a variable 'mapscat' so we can reference it later\nmapscat=ax.scatter(longs,lats,marker=\".\",s=0.5,c=age,vmin=0,vmax=100,transform=ccrs.PlateCarree(),zorder=4,cmap=plt.cm.hsv)\n\n#Make a Colorbar\ncbar=plt.colorbar(mapscat, ax=ax, orientation=\"horizontal\", pad=0.05, fraction=0.15, shrink=0.5,extend='max')\ncbar.set_label('Age (Ma)')\n\n# Add a map title, and tell the figure to appear on screen\nplt.title('Age of Copper Deposits in the EarthChem.org database')\nplt.show()\n\n\n\n\nYou can explore the different color maps at https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html.\n\n\nChallenge\n\nPick a new element from the EarthChem data and make a similar map.\nHere is a link to download the data for indiviual elements\nCreate a new notebook and display the diagnostic steps leading up to your final map.\n\n\n\nSolution\n\n#We only need numpy and plotting libraries\nimport numpy\nimport matplotlib.pyplot as plt \nimport cartopy.crs as ccrs\n\n#Set the correct filename/filepath to where you have downloaded the data\nfilename = '../data/EarthChemAG.txt'\n\n#Add the \"skiprows\" flag, because this data has a header row\nchemdata=numpy.loadtxt(filename, delimiter=',',skiprows=1)\n    \n#Set some variable names\nlats=chemdata[:,0]\nlongs=chemdata[:,1]\nage=chemdata[:,3]\nsilver=chemdata[:,2]\n    \n#Do a quick plot\nplt.plot(longs,lats,'b.')\n\n#This set actually looks fine, no filtering necessary!\n#Just make the final plot again, with a new color bar\nplt.scatter(longs,lats,s=age/10,c=silver,vmin=0, vmax=1000,cmap=plt.cm.twilight)\nplt.title('Silver Deposits from EarthChem.org')\nplt.ylabel('Latitude')\nplt.xlabel('Longitude')\nplt.show()\n\n\n\nKey points\n\nYou can store things in Python in variables\nLists can be used to store objects of different types\nLoops with for can be used to iterate over each object in a list\nFunctions are used to write (and debug) repetitive code once\nIndexing\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html",
    "href": "notebooks/04a-SimpleSpeedUps.html",
    "title": "Efficient Python methods",
    "section": "",
    "text": "Which method for acceleration should I choose?\nHow do I utilise traditional Python approaches to multi cpu and nodes\nIn this session we will show you a few of the basic tools that we can use in Python to make our code go faster. There is no perfect method for optimising code. Efficiency gains depend on what your end goal is, what libraries are available, what method or approach you want to take when writing algorithms, what your data is like, what hardware you have. Hopefully these notes will allow you to think about your problems from different perspectives to give you the best opportunity to make your development and execution as efficient as possible.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#acceleration-parallelisation-vectorising-threading-make-python-go-fast",
    "href": "notebooks/04a-SimpleSpeedUps.html#acceleration-parallelisation-vectorising-threading-make-python-go-fast",
    "title": "Efficient Python methods",
    "section": "Acceleration, Parallelisation, Vectorising, Threading, make-Python-go-fast",
    "text": "Acceleration, Parallelisation, Vectorising, Threading, make-Python-go-fast\nWe will cover a few of the ways that you can potentially speed up Python. As we will learn there are multitudes of methods to make Python code more efficient, and also different implementations of libraries, tools, techniques that can all be utilised depending on how your code and/or data is organised. This is a rich and evolving ecosystem and there is no one perfect way to implement efficiencies.\nSome key words that might come up:\n\nVectorisation\nMPI message parsing interface\nCPU, core, node, thread, process, worker, job, task\nParallelisation\nPython decorators and functional programming.\n\n\n\nWhat does parallel mean?\nSeparate workers or processes acting in an independent or semi-dependent manner. Independent processes ship data, program files and libraries to an isolated ecosystem where computation is performed. In this case communication between workers can be achieved. Contrastingly there are also shared memory set ups where multiple computational resources are pooled together to work on the same data.\nGenerally speaking, parallel workflows fit different categories of data handling which can make you think about how to write your code and what approaches to take.\n\nEmbarrassingly parallel:\nRequires no communication between processors. Utilise shared memory spaces. For example:\n\nRunning same algorithm for a range of input parameters.\nRendering video frames in computer animation.\nOpen MP implementations.\n\n\n\nCoarse/Fine-grained parallel:\nRequires occasional or frequent communication between processors. Uses a small number of processes on large data. Fine grain uses a large number of small processes with very little communication. But can improves computationally bound problems. For example:\n\nSome examples are\nFinite difference time-stepping on parallel grid.\nFinite element methods.\nParallel tempering and MCMC.\nMPI implementations.\n\nTraditional implementations of parallelism are done on a low level. However, open source software has evolved dramatically over the last few years allowing more high level implementations and concise ‘pythonic’ syntax that wraps around low level tools."
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#profiling-your-code",
    "href": "notebooks/04a-SimpleSpeedUps.html#profiling-your-code",
    "title": "Efficient Python methods",
    "section": "Profiling your code",
    "text": "Profiling your code\nBefore you get stuck into making things fast, it is important to find out what is exactly slow in your code. Is it a particular function running slow? Or are you calling a really fast function a million times? You can save yourself a lot development time by profiling your code to give you an idea for where efficiencies can be found. Try out Jupyter’s %%timeit magic function.\n\n%%time\nimport time\ntime.sleep(1)\n\nCPU times: user 718 µs, sys: 1.02 ms, total: 1.74 ms\nWall time: 1 s\n\n\nA neat little feature to check how fast some of your cells are running. Now let’s profile a simple Python script and then think about how we could make it faster. Put this code in a script (save it as faster.py):\n\n#A test function to see how you can profile code for speedups\n\nimport time\n\ndef waithere():\n    print(\"waiting for 1 second\")\n    time.sleep(1)\n\ndef add2(a=0,b=0):\n    print(\"adding\", a, \"and\", b)\n    return(a+b)\n\ndef main():\n    print(\"Hello, try timing some parts of this code!\")\n    waithere()  \n    add2(4,7)\n    add2(3,1)\n    \nif __name__=='__main__':\n    main()\n\nHello, try timing some parts of this code!\nwaiting for 1 second\nadding 4 and 7\nadding 3 and 1\n\n\nThere are several ways to debug and profile Python, a very elegant and built in one is cProfile It analyses your code as it executes. Run it with python -m cProfile faster.py and see the output of the script and the profiling:\nHello, try timing some parts of this code!\nwaiting for 1 second\nadding 4 and 7\nadding 3 and 1\n\n12 function calls in 1.008 seconds\n\nOrdered by: standard name\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n1    0.000    0.000    1.008    1.008 faster.py:12(main)\n1    0.000    0.000    1.008    1.008 faster.py:2(&lt;module&gt;)\n1    0.000    0.000    1.002    1.002 faster.py:4(waithere)\n2    0.000    0.000    0.005    0.003 faster.py:8(add2)\n1    0.000    0.000    1.008    1.008 {built-in method builtins.exec}\n4    0.007    0.002    0.007    0.002 {built-in method builtins.print}\n1    1.001    1.001    1.001    1.001 {built-in method time.sleep}\n1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\nYou can now interrogate your code and see where you should devote your time to improving it.\nSpecial note on style: Developing python software in a modular manner assists with debugging and time profiling. This is a bit different to the sequential notebooks we have been creating. But re-writing certain snippets in self-contained functions can be a fun task.\n\nChallenge\nRevisit some of the codes we have run previously. You can do this from a Jupyter Notebook by clicking File &gt; Downlad as &gt; Python (.py). You may need to comment-out some sections, espiecially where figures are displayed, to get them to run. Run it using cProfile and look at the results. Can you identify where improvements could be made?\n\n\nSolution\n\nDiscuss together"
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#loops-and-vectorising-code-with-numpy-and-pandas",
    "href": "notebooks/04a-SimpleSpeedUps.html#loops-and-vectorising-code-with-numpy-and-pandas",
    "title": "Efficient Python methods",
    "section": "Loops and vectorising code with numpy and pandas",
    "text": "Loops and vectorising code with numpy and pandas\nYour problem might be solved by using the fast way certain packages handle certain datatypes.\nGenerally speaking, pandas and numpy libraries should be libraries you frequently use. They offer advantages in high performance computing including: 1. Efficient datastructures that under the hood are implemented in fast C code rather than python. 2. Promoting explicit use of datatype declarations - making memory management of data and functions working on this data, faster. 3. Elegant syntax promoting consice behaviour. 4. Data structures come with common built in functions that are designed to be used in a vectorised way.\nLets explore this last point on vectorisation with an example. Take this nested for loop example:\n\n#import packages\nimport numpy as np\nimport pandas as pd\nimport time \n\n#Create some fake data to work with\nSamples = pd.DataFrame(np.random.randint(0,100,size=(1000, 4)), columns=list(['Alpha','Beta','Gamma','Delta']))\nWells = pd.DataFrame(np.random.randint(0,100,size=(50, 1)), columns=list(['Alpha']))\n\n#This could perhaps be the id of a well and the list of samples found in the well. \n#You want to match up the samples with some other list, \n#perhaps the samples from some larger database like PetDB.\n\n#Create an emtpy dataframe to fill with the resulting matches\ntotalSlow=pd.DataFrame(columns=Samples.columns)\ntotalFast=pd.DataFrame(columns=Samples.columns)\n\n\n#Now compare the nested for-loop method:\n\ntic=time.time()\nfor index,samp in Samples.iterrows():\n    for index2,well in Wells.iterrows():\n        if well['Alpha']==samp['Alpha']:\n            totalSlow=pd.concat([totalSlow, samp])\n            \ntotalSlow=totalSlow.drop_duplicates()\ntoc=time.time()\nprint(\"Nested-loop Runtime:\",toc-tic, \"seconds\")\n\nNested-loop Runtime: 1.8003878593444824 seconds\n\n\n\n#Or the vectorised method:\n\ntic=time.time()\ntotalFast=Samples[Samples['Alpha'].isin(Wells.Alpha.tolist())]\ntotalFast=totalSlow.drop_duplicates()\ntoc=time.time()\nprint(\"Vectorized Runtime:\",toc-tic, \"seconds\")\n\nVectorized Runtime: 0.0013341903686523438 seconds\n\n\nWhich one is faster? Note the use of some really basic timing functions, these can help you understand the speed of your code."
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#python-multiprocessing",
    "href": "notebooks/04a-SimpleSpeedUps.html#python-multiprocessing",
    "title": "Efficient Python methods",
    "section": "Python Multiprocessing",
    "text": "Python Multiprocessing\nFrom within Python you may need a flexible way to manage computational resources. This is traditionally done with the multiprocessing library.\nWith multiprocessing, Python creates new processes. A process here can be thought of as almost a completely different program, though technically they are usually defined as a collection of resources where the resources include memory, file handles and things like that.\nOne way to think about it is that each process runs in its own Python interpreter, and multiprocessing farms out parts of your program to run on each process.\n\nSome terminology - Processes, threads and shared memory\nA process is a collection of resources including program files and memory, that operates as an independent entity. Since each process has a seperate memory space, it can operate independently from other processes. It cannot easily access shared data in other processes.\nA thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. Threads are considered lightweight because they use far less resources than processes. Threads also share the same memory space so are not independent.\n\n\n\nSegmentLocal\n\n\n\n\n\nSegmentLocal\n\n\nBack to python, the multiprocessing library was designed to break down the Global Interpreter Lock (GIL) that limits one thread to control the Python interpreter.\nIn Python, the things that are occurring simultaneously are called by different names (thread, task, process). While they all fall under the definition of concurrency (multiple things happening anaologous to different trains of thought), only multiprocessing actually runs these trains of thought at literally the same time. We will only cover multiprocessing here which assists in CPU bound operations - but keep in mind other methods exist (threading), whose implementation tends to be more low level."
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#simple-multiprocessing-example",
    "href": "notebooks/04a-SimpleSpeedUps.html#simple-multiprocessing-example",
    "title": "Efficient Python methods",
    "section": "Simple multiprocessing example",
    "text": "Simple multiprocessing example\nSome basic concepts in the multiprocessing library are: 1. the Pool(processes) object creates a pool of processes. processes is the number of worker processes to use (i.e Python interpreters). If processes is None then the number returned by os.cpu_count() is used. 2. The map(function,list) attribute of this object uses the pool to map a defined function to a list/iterator object\nTo implement multiprocessing in its basic form. You can complete this exercise in a notebook or a script.\n\n#Import the libraries we will need\nimport shapefile\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport multiprocessing\nimport time\n\n#Check how many cpus are availble on your computer\nmultiprocessing.cpu_count()\n\n#Read in the shapefile that we will use\nsf = shapefile.Reader(\"../data/platepolygons/topology_platepolygons_0.00Ma.shp\")\nrecs    = sf.records()\nshapes  = sf.shapes()\nfields  = sf.fields\nNshp    = len(shapes)\npolygons=np.arange(Nshp)\n\n\n#Get some details about the shapefile. Plot one of the polygons.\n#Let us find the areas of each of the polygons in the shapefile.\nprint(recs[10])\n\npolygonShape=shapes[10].points\npoly=np.array(polygonShape)\n\nplt.plot(poly[:,0],poly[:,1]);\n\nRecord #10: [0, 0.0, 'Global_EarthByte_230-0Ma_GK07_AREPS_PlateBoundaries.gpml', 'Global_EarthByte_230-0Ma_GK07_AREPS.rot', 911, '', 'gpml:TopologicalClosedPlateBoundary', 0.9, 0.0, 'Nazca Plate', '', 'GPlates-ef2e06a7-4086-4062-9df6-1fa3133f50b8', 0, '', 0, 0, 0.0]\n\n\n\n\n\n\n#Implement a function to calcualte the area of a polygon.\n# Area of Polygon using Shoelace formula\n# http://en.wikipedia.org/wiki/Shoelace_formula\ndef PolygonArea(nshp):\n    start_time = time.time()\n    polygonShape=shapes[nshp].points\n    corners=np.array(polygonShape)\n    n = len(corners) # of corners\n    \n    #Area calculation using Shoelace forula\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += corners[i][0] * corners[j][1]\n        area -= corners[j][0] * corners[i][1]\n    area = abs(area) / 2.0\n    time.sleep(0.2)\n    \n    endtime=time.time() - start_time\n    print(\"Process {} Finished in {:0.4f}s.\".format(nshp,endtime))\n    return(area)\n\n\n#Run the function for each polygon/plate in the shapefile:\nstart_time = time.time()\nAreas1=[]\nfor i in polygons:\n    Areas1.append(PolygonArea(i))\n\nprint(\"Final Runtime\", time.time() - start_time)\n\nProcess 0 Finished in 0.2053s.\nProcess 1 Finished in 0.2029s.\nProcess 2 Finished in 0.2059s.\nProcess 3 Finished in 0.2017s.\nProcess 4 Finished in 0.2052s.\nProcess 5 Finished in 0.2014s.\nProcess 6 Finished in 0.2039s.\nProcess 7 Finished in 0.2030s.\nProcess 8 Finished in 0.2031s.\nProcess 9 Finished in 0.2019s.\nProcess 10 Finished in 0.2019s.\nProcess 11 Finished in 0.2039s.\nProcess 12 Finished in 0.2031s.\nProcess 13 Finished in 0.2018s.\nProcess 14 Finished in 0.2034s.\nProcess 15 Finished in 0.2017s.\nProcess 16 Finished in 0.2014s.\nProcess 17 Finished in 0.2013s.\nProcess 18 Finished in 0.2018s.\nProcess 19 Finished in 0.2014s.\nProcess 20 Finished in 0.2027s.\nProcess 21 Finished in 0.2025s.\nProcess 22 Finished in 0.2017s.\nProcess 23 Finished in 0.2022s.\nProcess 24 Finished in 0.2029s.\nProcess 25 Finished in 0.2024s.\nProcess 26 Finished in 0.2117s.\nProcess 27 Finished in 0.2013s.\nProcess 28 Finished in 0.2003s.\nProcess 29 Finished in 0.2052s.\nProcess 30 Finished in 0.2029s.\nProcess 31 Finished in 0.2051s.\nProcess 32 Finished in 0.2017s.\nProcess 33 Finished in 0.2016s.\nProcess 34 Finished in 0.2012s.\nProcess 35 Finished in 0.2076s.\nProcess 36 Finished in 0.2027s.\nProcess 37 Finished in 0.2052s.\nProcess 38 Finished in 0.2026s.\nProcess 39 Finished in 0.2046s.\nProcess 40 Finished in 0.2016s.\nProcess 41 Finished in 0.2031s.\nProcess 42 Finished in 0.2022s.\nProcess 43 Finished in 0.2051s.\nProcess 44 Finished in 0.2017s.\nProcess 45 Finished in 0.2010s.\nFinal Runtime 9.354471921920776\n\n\nNow we will have to run the multiprocessing version outside of our jupyter environment. Put the following into a script or download the full version here.\n\n%%script false --no-raise-error #do not copy this line - it allows us to have this code in the notes and still\n# generate them without error\n#Put the below snippet in a code block outside of Jupyter, but this time, use the multiprocessing capabilities\n#Because of how the multiprocessing works, it does not behave nicely in Jupyter all the time\ndef make_global(shapes):\n    global gshapes\n    gshapes = shapes\n    \nstart_time = time.time()\nwith multiprocessing.Pool(initializer=make_global, initargs=(shapes,)) as pool:\n    Areas2 = pool.map(PolygonArea,polygons)\n\nprint(\"Final Runtime\", time.time() - start_time)\n\nProcess 0 Finished in 0.2148s.\nProcess 1 Finished in 0.2199s.\nProcess 30 Finished in 0.2136s.\nProcess 31 Finished in 0.2135s.\nProcess 4 Finished in 0.2138s.\nProcess 5 Finished in 0.2047s.\nProcess 24 Finished in 0.2137s.\nProcess 25 Finished in 0.2138s.\nProcess 34 Finished in 0.2116s.\nProcess 35 Finished in 0.2124s.\nProcess 6 Finished in 0.2138s.\nProcess 7 Finished in 0.2047s.\nProcess 16 Finished in 0.2137s.\nProcess 17 Finished in 0.2138s.\nProcess 38 Finished in 0.2106s.\nProcess 39 Finished in 0.2124s.\nProcess 8 Finished in 0.2138s.\nProcess 9 Finished in 0.2047s.\nProcess 26 Finished in 0.2137s.\nProcess 27 Finished in 0.2138s.\nProcess 42 Finished in 0.2106s.\nProcess 43 Finished in 0.2124s.\nProcess 12 Finished in 0.2138s.\nProcess 13 Finished in 0.2047s.\nProcess 20 Finished in 0.2137s.\nProcess 21 Finished in 0.2138s.\nProcess 32 Finished in 0.2116s.\nProcess 33 Finished in 0.2124s.\nProcess 14 Finished in 0.2138s.\nProcess 15 Finished in 0.2037s.\nProcess 22 Finished in 0.2137s.\nProcess 23 Finished in 0.2138s.\nProcess 40 Finished in 0.2106s.\nProcess 41 Finished in 0.2124s.\nProcess 2 Finished in 0.2138s.\nProcess 3 Finished in 0.2047s.\nProcess 28 Finished in 0.2137s.\nProcess 29 Finished in 0.2138s.\nProcess 44 Finished in 0.2096s.\nProcess 45 Finished in 0.2124s.\nProcess 10 Finished in 0.2138s.\nProcess 11 Finished in 0.2047s.\nProcess 18 Finished in 0.2137s.\nProcess 19 Finished in 0.2138s.\nProcess 36 Finished in 0.2116s.\nProcess 37 Finished in 0.2124s.\nFinal Runtime 5.4170615673065186\nIs there any speed up? Why are the processes not in order? Is there any overhead?\n\nChallenge.\n\nThe pyshp/shapefile library contains the function signed_area which can calculate the area of a polygon. Replace the calculation of area using the shoelace formula in the PolygonArea function with the signed_area method:\n\narea=shapefile.signed_area(corners)\n\nHow does this change the timings of your speed tests? Hint, it might not be by much.\n\n\n\nSolution\n\ndef PolygonArea(nshp):\n    start_time = time.time()\n    \n    area=shapefile.signed_area(corners)\n    \n    endtime=time.time() - start_time\n    print(\"Process {} Finished in {:0.4f}s. \\n\".format(nshp,endtime))\n    return(area)\n#Test the serial version\nstart_time = time.time()\nAreas1=[]\n\nfor i in polygons:\n    Areas1.append(PolygonArea(i))\nprint(\"Final Runtime\", time.time() - start_time)\n\n#Test the multiprocessing version\nstart_time = time.time()\nwith multiprocessing.Pool(initializer=make_global, initargs=(shapes,)) as pool:\n    Areas2 = pool.map(PolygonArea,polygons)\n\nprint(\"Final Runtime\", time.time() - start_time)\nThere is generally a sweet spot in how many processes you create to optimise the run time. A large number of python processes is generally not advisable, as it involves a large fixed cost in setting up many python interpreters and its supporting infrastructure. Play around with different numbers of processes in the pool(processes) statement to see how the runtime varies. Also, well developed libraries often have nicely optimised algorithms, you may not have to reinvent the wheel (Haha at this Python pun)."
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#useful-links",
    "href": "notebooks/04a-SimpleSpeedUps.html#useful-links",
    "title": "Efficient Python methods",
    "section": "Useful links",
    "text": "Useful links\nhttps://realpython.com/python-concurrency/\nhttps://docs.python.org/3/library/multiprocessing.html\nhttps://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/\nhttps://pawseysc.github.io/training.html"
  },
  {
    "objectID": "notebooks/04a-SimpleSpeedUps.html#mpi-message-passing-interface",
    "href": "notebooks/04a-SimpleSpeedUps.html#mpi-message-passing-interface",
    "title": "Efficient Python methods",
    "section": "MPI: Message Passing Interface",
    "text": "MPI: Message Passing Interface\nMPI is a standardized and portable message-passing system designed to function on a wide variety of parallel computers. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran. There are several well-tested and efficient implementations of MPI, many of which are open-source or in the public domain.\nMPI for Python, found in mpi4py, provides bindings of the MPI standard for the Python programming language, allowing any Python program to exploit multiple processors. This simple code demonstrates the collection of resources and how code is run on different processes:\n\n#!pip install mpi4py\n#Run with:\n#mpiexec -np 4 python mpi.py\n\nfrom mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\nprint(\"I am rank %d in group of %d processes.\" % (rank, size))\n\nI am rank 0 in group of 1 processes.\n\n\n\n#Want more help on the MPI.COMM_WORLD class object?\n#See the methods, functions, and variables associated with it.\n#help(comm)\n\n\nKey points\n\nUnderstand there are different ways to accelerate\nThe best method depends on your algorithms, code and data\nLoad multiprocessing library to execute a function in a parallel manner"
  },
  {
    "objectID": "notebooks/01b-dataframes.html",
    "href": "notebooks/01b-dataframes.html",
    "title": "Useful Python packages for different data types",
    "section": "",
    "text": "What are libraries and packages?\nHow can I load tabular data into Python?\nHow can I load shapefiles?\nHow can I load segy and las data?\nPython can deal with basically any type of data you throw at it. The open source python community has developed many packages that make things easy. Today we will look at pyshp (for dealing with shapefiles), pandas (great for tables and time series), lasio (for las format well log data) and obspy (a highly featured seismic data processing suite) packages.\nData for this exercised was downloaded from http://www.bom.gov.au/water/groundwater/explorer/map.shtml\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01b-dataframes.html#western-seismic-velf-format",
    "href": "notebooks/01b-dataframes.html#western-seismic-velf-format",
    "title": "Useful Python packages for different data types",
    "section": "Western seismic VELF format",
    "text": "Western seismic VELF format\nSometimes there are no good libraries or data-readers availble for your specific use-case. Very common if you get some specialty instrument with some unique data format. Often the documentation is a good place to start for figuring out how to interpret the data, and more-often-than not, the header of the data can give you all the information you need. Download this VELF file, containing some 3D seismic data. I could not find any good python libraries to handle this dataset, so we can just try out a few things to get the data into a format that is useful for us.\n\n#Imports for plotting\nimport matplotlib.pyplot as plt\n# Load Builtin colormaps, colormap handling utilities, and the ScalarMappable mixin.\nfrom matplotlib import cm\nimport pandas as pd\n\n\n#Open the file for reading\nf = open(\"../data/S3D_Vrms_StkVels_VELF.txt\",'r')\n\n#Read in all the lines in the file and save them to a variable\nmylist = f.readlines()\n\n#Close the file\nf.close()\nprint(\"Done reading file.\")\n\nDone reading file.\n\n\n\n#Print out the first 20 lines\nprint(mylist[0:20])\n\n['Client: XXX \\n', 'Project: YYY\\n', 'Contractor: ZZZ\\n', 'Date:\\n', '\\n', 'Velocity type: RMS Velocity in Time\\n', '\\n', '\\n', 'Datum: GDA94, UTM Zone: UTM53, Central Meridian :  \\n', 'Statics: Two way time corrected to mean sea level: No\\n', '         Gun and Cable statics applied: No\\n', '         Tidal statics applied: No\\n', '\\n', '3D Grid details:\\n', 'inline    crossline      X            Y\\n', '\\n', '1000        5000      599413.78   7382223.37\\n', '1000        5309      595633.30   7375486.63\\n', '1448        5000      609180.96   7376742.28\\n', '1448        5309      605400.48   7370005.55\\n']\n\n\n\n#Set up some empty lists to store each bit of data in\ninline=[]\ncrossline=[]\nX=[]\nY=[]\n\nvelf=[]\n\n#Loop through all the lines in the file\nfor i,line in enumerate(mylist):\n    \n    #First split the line up (by default, split by whitespace)\n    splitline=line.split()\n    \n    #If we encounter certain lines, save some data\n    if i in [16,17,18,19]:  \n        #Print out the lines (check we are doing the right thing)\n        print(splitline)\n        \n        inline.append(int(splitline[0]))\n        crossline.append(int(splitline[1]))\n        X.append(float(splitline[2]))\n        Y.append(float(splitline[3]))\n      \n\n    #This is where the actual data starts\n    #Now depending on the key word at the start of each line\n    #save the data to each particular list/array\n    #Read the data in again, this time with some thought about what we actually want to do\n    if i&gt;49:\n        if splitline[0]=='LINE':\n            LINE = int(splitline[1])\n            \n        if splitline[0]=='SPNT':\n            xline3d=int(splitline[1])\n            binx=float(splitline[2])\n            biny=float(splitline[3])\n            inline3d=int(splitline[4])\n            \n        if splitline[0]=='VELF':\n            \n            for j,val in enumerate(splitline[1:]):\n                #print(j,val)\n                #Counting from the 0th index of splitline[1:end]\n                if j%2==0:\n                    t=int(val)\n                else:\n                    vt=int(val)\n                    velf.append([LINE,xline3d,binx,biny,inline3d,t,vt]) \n\n['1000', '5000', '599413.78', '7382223.37']\n['1000', '5309', '595633.30', '7375486.63']\n['1448', '5000', '609180.96', '7376742.28']\n['1448', '5309', '605400.48', '7370005.55']\n\n\n\n#Convert the python \"list\" type to Pandas dataframe\ndf=pd.DataFrame(velf)\n#Set the names of the columns\ndf.columns=['LINE','xline3d','binx','biny','inline3d','t','vt']\n\ndf\n\n\n\n\n\n\n\n\nLINE\nxline3d\nbinx\nbiny\ninline3d\nt\nvt\n\n\n\n\n0\n1000\n5080\n598435.0\n7380479.0\n1000\n0\n3200\n\n\n1\n1000\n5080\n598435.0\n7380479.0\n1000\n295\n3300\n\n\n2\n1000\n5080\n598435.0\n7380479.0\n1000\n598\n4137\n\n\n3\n1000\n5080\n598435.0\n7380479.0\n1000\n738\n4537\n\n\n4\n1000\n5080\n598435.0\n7380479.0\n1000\n1152\n4500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3082\n1440\n5280\n605580.0\n7370735.0\n1440\n2216\n5259\n\n\n3083\n1440\n5280\n605580.0\n7370735.0\n1440\n2861\n5791\n\n\n3084\n1440\n5280\n605580.0\n7370735.0\n1440\n3526\n6294\n\n\n3085\n1440\n5280\n605580.0\n7370735.0\n1440\n4697\n7077\n\n\n3086\n1440\n5280\n605580.0\n7370735.0\n1440\n5988\n7748\n\n\n\n\n3087 rows × 7 columns\n\n\n\n\n#Plot the target area\nplt.scatter(df.binx,df.biny,c=df.xline3d)\nplt.colorbar()\nplt.show()\n\n#Plot it in 3d just becasue we can\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(df.binx,df.biny,df.inline3d,c=df.xline3d)\nax.view_init(30, 70)\nplt.show()\n\n\n\n\n\n\n\n\n#Now, make some plots...\n#One way to do this, is to\n#make a 'group' for each unique seismic line\ngroups=df.groupby(['LINE','xline3d','inline3d'])\n\n\n#Make plots by certain groupings\n\n#Add a value to spread out the data nicely\ni=0\nfor name,grp in groups:\n    \n    if name[2]==1280:\n        print(name)\n        plt.plot(grp.vt+i,-grp.t)\n        i+=500\n\n(1280, 5040, 1280)\n(1280, 5060, 1280)\n(1280, 5080, 1280)\n(1280, 5100, 1280)\n(1280, 5120, 1280)\n(1280, 5140, 1280)\n(1280, 5160, 1280)\n(1280, 5180, 1280)\n(1280, 5200, 1280)\n(1280, 5220, 1280)\n(1280, 5240, 1280)\n(1280, 5260, 1280)\n(1280, 5280, 1280)\n\n\n\n\n\n\nfrom scipy.interpolate import interp1d\nimport numpy as np\n\n\n#Normal plots\n%matplotlib inline\n\n#Fancy intereactive plots\n#%matplotlib notebook\n\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\nfor name,grp in groups:\n\n    ##Plot all the data\n    ax.plot(grp.binx+grp.vt,grp.biny,grp.t,'b-')\n    \n    ##Plot all the data with colors\n#     colors=cm.seismic(grp.vt/grp.vt.max())\n#     ax.scatter(grp.binx+grp.vt,grp.biny,grp.t,c=grp.vt/grp.vt.max(),cmap=cm.seismic)\n    \n    #Interpolate the data and plot with colors\n#     x = grp.t\n#     y = grp.vt\n#     f = interp1d(x, y, kind='linear')\n\n#     num=50\n#     xnew = np.linspace(0,max(x),num)\n#     ynew = f(xnew)\n#     binx = np.linspace(min(grp.binx),max(grp.binx),num)\n#     biny = np.linspace(min(grp.biny),max(grp.biny),num)\n#     colours = cm.seismic(ynew/grp.vt.max())\n\n#     ax.scatter(binx+xnew,biny,xnew,c=colours)\n    \nplt.show()"
  },
  {
    "objectID": "notebooks/01b-dataframes.html#bonus-convert-text-to-segy-format",
    "href": "notebooks/01b-dataframes.html#bonus-convert-text-to-segy-format",
    "title": "Useful Python packages for different data types",
    "section": "Bonus: Convert Text to SegY format",
    "text": "Bonus: Convert Text to SegY format\nA work in progress… but the strategy is to simply match up the correct SEGY values with those in the text file. Grouping everything by each x-y (i-j, lat-lon) surface point, and then the z-axis will be the trace (down to a depth or a TWT etc).\n\nfrom obspy.core.stream import Stream, Trace\nfrom obspy.core.utcdatetime import UTCDateTime\nfrom obspy.io.segy.segy import SEGYTraceHeader\nfrom obspy.core.util.attribdict import AttribDict\nfrom obspy.io.segy.segy import SEGYBinaryFileHeader\nimport sys\n\n# Group all the text traces by their the i-j coordinates\ngroups=df.groupby(['LINE','xline3d'])\nprint(len(groups))\n\n#Make a stream object (flush it out to begin because we have used this variable names for demos)\nstream_out = None \nstream_out = Stream()\n\n#not sure how to group the trace ensembles but can use a counter to keep track of them\nensemble_number = 0\n       \nfor ids,df_trace in groups:\n    #ids are the LINE, xline3d coordinate locations\n    #trc is the subset of the full dataframe for just that i-j location\n\n    #For each LINE-xline3d location, a trace is impdence at all the depth values, i.e.\n    data = df_trace.vt.values\n\n    # Enforce correct byte number and set to the Trace object\n    data = np.require(data, dtype=np.float32)\n    trace = Trace(data=data)\n\n    # Set all the segy header information\n    # Attributes in trace.stats will overwrite everything in trace.stats.segy.trace_header\n    trace.stats.delta = 0.01\n    trace.stats.starttime = UTCDateTime(1970,1,1,0,0,0)\n\n    # If you want to set some additional attributes in the trace header,\n    # add one and only set the attributes you want to be set. Otherwise the\n    # header will be created for you with default values.\n    if not hasattr(trace.stats, 'segy.trace_header'):\n        trace.stats.segy = {}\n\n    trace.stats.segy.trace_header = SEGYTraceHeader()\n\n#         trace.stats.segy.trace_header.trace_sequence_number_within_line = index + 1\n#         trace.stats.segy.trace_header.receiver_group_elevation = 0\n    trace.stats.segy.trace_header.source_coordinate_x = int(df_trace.binx.values[0])\n    trace.stats.segy.trace_header.source_coordinate_y = int(df_trace.biny.values[0])\n    trace.stats.segy.trace_header.ensemble_number = ensemble_number #Not sure how this is actually determined\n    trace.stats.segy.trace_header.lag_time_A = 2400\n    trace.stats.segy.trace_header.lag_time_B = 3000\n    trace.stats.segy.trace_header.number_of_samples_in_this_trace = len(data)\n\n    ensemble_number +=1\n\n    # Add trace to stream\n    stream_out.append(trace)\n\n# A SEGY file has file wide headers. This can be attached to the stream\n# object.  If these are not set, they will be autocreated with default\n# values.\nstream_out.stats = AttribDict()\nstream_out.stats.textual_file_header = 'Textual Header!'\nstream_out.stats.binary_file_header = SEGYBinaryFileHeader()\nstream_out.stats.binary_file_header.trace_sorting_code = 5\n# stream.stats.binary_file_header.number_of_data_traces_per_ensemble=1\n\nprint(\"Stream object before writing...\")\nprint(stream_out)\n\nstream_out.write(\"TEST.sgy\", format=\"SEGY\", data_encoding=1, byteorder=sys.byteorder)\n\nprint(\"Stream object after writing. Will have some segy attributes...\")\nprint(stream_out)\n\n217\nStream object before writing...\n217 Trace(s) in Stream:\n\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.060000Z | 100.0 Hz, 7 samples\n...\n(215 other traces)\n...\n... | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.100000Z | 100.0 Hz, 11 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\nStream object after writing. Will have some segy attributes...\n217 Trace(s) in Stream:\n\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.060000Z | 100.0 Hz, 7 samples\n...\n(215 other traces)\n...\nSeq. No. in line:    0 | 1970-01-01T00:00:00.000000Z - 1970-01-01T00:00:00.100000Z | 100.0 Hz, 11 samples\n\n[Use \"print(Stream.__str__(extended=True))\" to print all Traces]"
  },
  {
    "objectID": "notebooks/PathsEnvironments.html",
    "href": "notebooks/PathsEnvironments.html",
    "title": "Path and Environments",
    "section": "",
    "text": "Core ideas:\n\n\nDirectory structure navigation.\n\n\nConda environment(s).\n\n\n\n# You can run system commands from a Jupyter notebook with '!'\n\n# Unix system\n!pwd\n\n# Windows system\n!cd\n\n\nTo open a data file hard drive it’s ESSENTIAL to know it’s exact path.\nCore ideas: 1. Relative paths. 2. Absolute paths. 3. Error Messages.\n\nimport numpy as np\nrel_path = np.loadtxt(\"./data/EarthChemCU.txt\", delimiter=',')\nabs_path = np.loadtxt(\"/home/julian/Desktop/geopy/data/EarthChemCU.txt\", delimiter=',')\n\n\n# ERROR, read error message\nbad_path = np.loadtxt(\"../data/EarthChemCU.txt\", delimiter=',')\n\n\n## For windows: FICTIONAL paths to illustrate my point\n\nrel_path = './Download/geo course/data/EarthChemCU.txt'\nabs_path = 'C:/Users/jgio5642/Downloads/geo course/data/EarthChemCU.txt'\nwin_path = r'C:\\Users\\jgio5642\\Download\\geo course\\data\\EarthChemCU.txt'\n\n# The 'r' make a raw string - important because '\\U' is a special python reserved character.\n\n\n# programming method: to make paths useable\nimport glob\nfile = glob.glob('**/*.txt', recursive=True)\nfile\n\n\n# the initial file we wanted\nfile[0]\n\n\n# A full proof way of find files is to use \"tab completion\"\n\npath = './data/EarthChemCU.txt'\n\n\n\n\n\nConda environments\n\nConda is a “python” environment manager. Managing packages (python / R / C / Fortran, etc.) and dependencies.\nDifferent styles of conda exist: anaconda, miniconda, mamba, etc, but interface is the same. (Difference is default packages and internal dependency calculations).\nConda can be available system-wide, or just per user.\n\nIn session 1 we downloaded our own i.e. per user)\nI suspect some are using older or system installs from debugging.  \n\nConda is designed to have many envrionments that can be switch in and out.\n\n\n# Find what conda environments you have available\n!conda info --env\n\n\n# Find infomation about your conda installation - important if you change computers\n!conda info\n\n\n# Find what python packages are `import'-able\n!conda list\n\n# Very similar to the pip list. The underlying python package manager\n#!pip list\n\n\n# Very similar to the pip list if you're familiar with pip\n!conda env export --from-history\n\n# This information could be copied into a file and used to download the same packages - this is how we created the initial environment in session 1.\n\n\n# To download other packages\n!conda search emoji\n\n# if a package doesn't exist in the regular package channels, use '-c'\n#!conda search -c conda-forge emoji\n\n\n# To install a package by name, (required \"-y\")\n!conda install emoji -y\n\n\nimport emoji\n\n\nprint(emoji.emojize(\"Python is fun :thumbs_up: :red_heart: :winking_face:\"))\n\n\n# To remove a package - take a LONG time\n!conda remove emoji -y\n\n\n!conda list | grep emoji\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04b-DaskDataframes.html",
    "href": "notebooks/04b-DaskDataframes.html",
    "title": "Working with Big Data using Dask",
    "section": "",
    "text": "Use a modern python library and elegant syntax for performance benefits\nHow do I deal with large irregular data and show me some real world examples of Dask?\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04b-DaskDataframes.html#dask",
    "href": "notebooks/04b-DaskDataframes.html#dask",
    "title": "Working with Big Data using Dask",
    "section": "DASK",
    "text": "DASK\nDask is a flexible library for parallel computing in Python.\nDask is composed of two parts: Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads. “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.\nDask emphasizes the following virtues:\n\nFamiliar: Provides parallelized NumPy array and Pandas DataFrame objects\nFlexible: Provides a task scheduling interface for more custom workloads and integration with other projects.\nNative: Enables distributed computing in pure Python with access to the PyData stack.\nFast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\nScales up: Runs resiliently on clusters with 1000s of cores\nScales down: Trivial to set up and run on a laptop in a single process\nResponsive: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans\n\n\n\n\nDask High Level Schema https://docs.dask.org/en/latest/\n\n\n\nDask provides high level collections - these are Dask Dataframes, bags, and arrays. On a low level, dask dynamic task schedulers to scale up or down processes, and presents parallel computations by implementing task graphs. It provides an alternative to scaling out tasks instead of threading (IO Bound) and multiprocessing (cpu bound).\nA Dask DataFrame is a large parallel DataFrame composed of many smaller Pandas DataFrames, split along the index. These Pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames.\n\n\n\nDask High Level Schema https://docs.dask.org/en/latest/dataframe.html/\n\n\n\nCommon Use Cases: Dask DataFrame is used in situations where Pandas is commonly needed, usually when Pandas fails due to data size or computation speed: - Manipulating large datasets, even when those datasets don’t fit in memory - Accelerating long computations by using many cores - Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\nDask Dataframes may not be the best choice if: your data fits comfortable in RAM - Use pandas only! If you need a proper database. You need functions not implemented by dask dataframes - see Dask Delayed."
  },
  {
    "objectID": "notebooks/04b-DaskDataframes.html#dask-dataframes",
    "href": "notebooks/04b-DaskDataframes.html#dask-dataframes",
    "title": "Working with Big Data using Dask",
    "section": "Dask Dataframes",
    "text": "Dask Dataframes\nWe will load in some data to explore.\n\n#Import dask dataframe modules\nimport dask.dataframe as dd\n\n#import dask\n#dask.config.set({\"visualization.engine\": \"cytoscape\"})\n#NOTE: to run this example (with diagrams) you will need to \"pip install graphviz\" and donwload graphviz\n#https://graphviz.org/download/\nimport os\n#os.environ[\"PATH\"] += os.pathsep + 'C:/APPS/Graphviz/bin'\n\n\n# Setup a parlalle LocalCluster that makes use of all the cores and RAM we have on a single machine\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster()\n# explicitly connect to the cluster we just created\nclient = Client(cluster)\nclient\n\n/Users/darya/miniconda3/envs/geopy/lib/python3.9/site-packages/distributed/node.py:183: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 54995 instead\n  warnings.warn(\n\n\n\n     \n    \n        Client\n        Client-eac15e50-22ac-11ed-9bb4-fe453513c759\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:54995/status\n\n\n\n\n\nCluster Info\n\n\n\n\n\n\nLocalCluster\nfbc990ef\n\n\n\nDashboard: http://127.0.0.1:54995/status\nWorkers: 5\n\n\nTotal threads: 10\nTotal memory: 32.00 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\nScheduler Info\n\n\n\n\n\n\nScheduler\nScheduler-0247a9eb-a609-475c-b8e5-1e5c3e496e65\n\n\n\nComm: tcp://127.0.0.1:54998\nWorkers: 5\n\n\nDashboard: http://127.0.0.1:54995/status\nTotal threads: 10\n\n\nStarted: Just now\nTotal memory: 32.00 GiB\n\n\n\n\n\n\nWorkers\n\n\n\n\n\nWorker: 0\n\n\n\nComm: tcp://127.0.0.1:55037\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:55039/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:55002\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-g_vn8opz\n\n\n\n\n\n\n\n\n\n\nWorker: 1\n\n\n\nComm: tcp://127.0.0.1:55044\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:55053/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:55004\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-uxx2_0xo\n\n\n\n\n\n\n\n\n\n\nWorker: 2\n\n\n\nComm: tcp://127.0.0.1:55043\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:55045/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:55001\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-6v08k6vd\n\n\n\n\n\n\n\n\n\n\nWorker: 3\n\n\n\nComm: tcp://127.0.0.1:55051\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:55055/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:55005\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-fd9fzs7b\n\n\n\n\n\n\n\n\n\n\nWorker: 4\n\n\n\nComm: tcp://127.0.0.1:55057\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:55058/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:55003\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-wsov5c3n"
  },
  {
    "objectID": "notebooks/04b-DaskDataframes.html#helpful-links",
    "href": "notebooks/04b-DaskDataframes.html#helpful-links",
    "title": "Working with Big Data using Dask",
    "section": "Helpful Links:",
    "text": "Helpful Links:\nDask bag fundamentals https://docs.dask.org/en/latest/bag.html\nBag API’s: https://docs.dask.org/en/latest/bag-api.html\nDask bag limitations: https://docs.dask.org/en/latest/shared.html\nPangeo info: https://pangeo.io/#what-is-pangeo\nXarray: http://xarray.pydata.org/en/stable/\nXarray API: http://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html\nDask Dataframe intro https://docs.dask.org/en/latest/dataframe.html\nAPI list for Dask Dataframes https://docs.dask.org/en/latest/dataframe.html\nWhat are decorators https://realpython.com/primer-on-python-decorators/\n\nKey Points\n\nDask builds on numpy and pandas APIs but operates in a parallel manner\nComputations are by default lazy and must be triggered - this reduces unneccessary computation time\nDask Bag uses map filter and group by operations on python objects or semi/unstrucutred data\ndask.multiprocessing is under the hood\nXarray is another option for holding netcdf data"
  },
  {
    "objectID": "notebooks/merge.html",
    "href": "notebooks/merge.html",
    "title": "Merging two csv files",
    "section": "",
    "text": "Questions\n\nWhat is the best way to merge two csv files?\n\n\n\nObjectives\n\nUse Pandas to merge two csv files?\n\n\nMerging two files with tabular data in each is one of the most common data processing tasks that occurs that at the start of a data science project. Below, we merge two files:\n\nOne with geochemistry data, which shows at different depths the concentration of specific minerals\nOne with lithology data, which provides annotation around which lithology group is observed at different depths at specific sites\n\n\n# load in pandas \nimport pandas as pd\n\n#Load in the two files as dataframes\ndfgeo = pd.read_csv(\"../userdata/Geochemistry.csv\")\ndflit = pd.read_csv(\"../userdata/Lithology.csv\")\n\n\ndfgeo\n\n\n\n\n\n\n\n\nCollarId\nFromDepth\nToDepth\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\n\n\n\n\n0\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n\n\n1\n224202\n4.5\n4.6\n3\n93000\n-9999\n-9999\n-9999\n3.5\n\n\n2\n224202\n4.6\n5.0\n2\n94800\n-9999\n-9999\n-9999\n4.5\n\n\n3\n224202\n5.0\n6.0\n3\n93000\n-9999\n-9999\n-9999\n3.5\n\n\n4\n224202\n6.0\n7.0\n2\n94800\n-9999\n-9999\n-9999\n4.5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n72\n975622\n37.0\n38.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n73\n975622\n38.0\n39.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n74\n975622\n39.0\n40.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n75\n975622\n40.0\n41.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n76\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n\n\n77 rows × 9 columns\n\n\n\n\ndflit\n\n\n\n\n\n\n\n\nCollarId\nFromDepth\nToDepth\nlithology_group\n\n\n\n\n0\n224202\n3.0\n3.1\nsediment\n\n\n1\n224202\n3.1\n4.0\nsediment\n\n\n2\n224202\n4.0\n10.0\nclay\n\n\n3\n224202\n10.0\n15.0\nbasalt\n\n\n4\n224202\n15.0\n20.0\ngranite\n\n\n5\n224202\n20.0\n22.0\npegmatite\n\n\n6\n224202\n22.0\n35.0\nbasalt\n\n\n7\n224202\n35.0\n37.0\npegmatite\n\n\n8\n975622\n0.0\n5.0\nsediment\n\n\n9\n975622\n5.0\n10.0\nclay\n\n\n10\n975622\n10.0\n15.0\ndiorite\n\n\n11\n975622\n15.0\n30.0\nbasalt\n\n\n12\n975622\n30.0\n40.0\npegmatite\n\n\n13\n975622\n40.0\n41.0\nquartz\n\n\n14\n975622\n41.0\n42.0\ngranite\n\n\n\n\n\n\n\nThe CollarId is the column that is common between the two datasets, so can be used as a so-called “join key”.\nThe FromDepth and ToDepth are also common, but if we merge using the default settings, only columns where both values perfectly match will be joined - this isn’t quite what we want:\n\ndfbadmerge = pd.merge(dfgeo,dflit)\ndfbadmerge\n\n\n\n\n\n\n\n\nCollarId\nFromDepth\nToDepth\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nlithology_group\n\n\n\n\n0\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\nsediment\n\n\n1\n975622\n40.0\n41.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\nquartz\n\n\n2\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\ngranite\n\n\n\n\n\n\n\nThe next step merges the two data frames, joining all columns based only on the CollarID. The _x and _y denote the values that were in the original columns FromDepth and ToDepth in dfgeo and dflit, respectively.\n\n#Merge them based on CollarId\ndfmerge=pd.merge(dfgeo,dflit,how=\"outer\",on=[\"CollarId\"])\ndfmerge\n\n\n\n\n\n\n\n\nCollarId\nFromDepth_x\nToDepth_x\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nFromDepth_y\nToDepth_y\nlithology_group\n\n\n\n\n0\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n3.0\n3.1\nsediment\n\n\n1\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n3.1\n4.0\nsediment\n\n\n2\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n4.0\n10.0\nclay\n\n\n3\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n10.0\n15.0\nbasalt\n\n\n4\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n15.0\n20.0\ngranite\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n569\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n10.0\n15.0\ndiorite\n\n\n570\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n15.0\n30.0\nbasalt\n\n\n571\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n\n\n572\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n40.0\n41.0\nquartz\n\n\n573\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n41.0\n42.0\ngranite\n\n\n\n\n574 rows × 12 columns\n\n\n\nNow drop any rows outside the From/To depth of the Lithology:\n\ndf = dfmerge[(dfmerge.FromDepth_x &gt;= dfmerge.FromDepth_y) & (dfmerge.ToDepth_x &lt;= dfmerge.ToDepth_y)].reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\nCollarId\nFromDepth_x\nToDepth_x\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nFromDepth_y\nToDepth_y\nlithology_group\n\n\n\n\n0\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n3.0\n3.1\nsediment\n\n\n1\n224202\n4.5\n4.6\n3\n93000\n-9999\n-9999\n-9999\n3.5\n4.0\n10.0\nclay\n\n\n2\n224202\n4.6\n5.0\n2\n94800\n-9999\n-9999\n-9999\n4.5\n4.0\n10.0\nclay\n\n\n3\n224202\n5.0\n6.0\n3\n93000\n-9999\n-9999\n-9999\n3.5\n4.0\n10.0\nclay\n\n\n4\n224202\n6.0\n7.0\n2\n94800\n-9999\n-9999\n-9999\n4.5\n4.0\n10.0\nclay\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n71\n975622\n37.0\n38.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n\n\n72\n975622\n38.0\n39.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n\n\n73\n975622\n39.0\n40.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n\n\n74\n975622\n40.0\n41.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n40.0\n41.0\nquartz\n\n\n75\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n41.0\n42.0\ngranite\n\n\n\n\n76 rows × 12 columns\n\n\n\nI would expect that to equal 77 (the original number of Geochemistry points)\nWhy is one missing? (TODO show how you found out which one was missing!)\n\n#After some digging the missing value is line 50 from the dfgeo dataframe\ndfgeo.loc[49:51]\n\n\n\n\n\n\n\n\nCollarId\nFromDepth\nToDepth\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\n\n\n\n\n49\n975622\n14.2\n14.8\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n50\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n51\n975622\n16.0\n17.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n\n\n\n\n\n\n\n\n#Compared with the final dataframe\ndf.loc[49:51]\n\n\n\n\n\n\n\n\nCollarId\nFromDepth_x\nToDepth_x\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nFromDepth_y\nToDepth_y\nlithology_group\n\n\n\n\n49\n975622\n14.2\n14.8\n2\n111000\n-9999\n-9999\n-9999\n13.5\n10.0\n15.0\ndiorite\n\n\n50\n975622\n16.0\n17.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n15.0\n30.0\nbasalt\n\n\n51\n975622\n17.0\n18.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n15.0\n30.0\nbasalt\n\n\n\n\n\n\n\n\n#Finding that row in the complete merged dataframe\ndfmerge.loc[(dfmerge['FromDepth_x'] == 14.8) & (dfmerge['ToDepth_x'] == 16.0)]\n\n\n\n\n\n\n\n\nCollarId\nFromDepth_x\nToDepth_x\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nFromDepth_y\nToDepth_y\nlithology_group\n\n\n\n\n385\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n0.0\n5.0\nsediment\n\n\n386\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n5.0\n10.0\nclay\n\n\n387\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n10.0\n15.0\ndiorite\n\n\n388\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n15.0\n30.0\nbasalt\n\n\n389\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n\n\n390\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n40.0\n41.0\nquartz\n\n\n391\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n41.0\n42.0\ngranite\n\n\n\n\n\n\n\nWe see that the ACTUAL lithology group should be diorite AND basalt. So based on our criteria it actually chose neither.\nIt will take a bit more hacking to deal with an edge cases like this… but I will leave that up to you.\nNext, we do TODO FIXME.\n\ndfmerge['depthInterval'] = pd.arrays.IntervalArray.from_arrays(dfmerge.FromDepth_x, dfmerge.ToDepth_x,closed='left')\ndfmerge['lithInterval'] = pd.arrays.IntervalArray.from_arrays(dfmerge.FromDepth_y, dfmerge.ToDepth_y,closed='left')\n\nAnd then we do TODO FIXME.\n\ndf = dfmerge[dfmerge.apply(lambda row: row['depthInterval'].overlaps(row['lithInterval']),axis=1)].reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\nCollarId\nFromDepth_x\nToDepth_x\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nFromDepth_y\nToDepth_y\nlithology_group\ndepthInterval\nlithInterval\n\n\n\n\n0\n224202\n3.0\n3.1\n2\n94800\n-9999\n-9999\n-9999\n4.5\n3.0\n3.1\nsediment\n[3.0, 3.1)\n[3.0, 3.1)\n\n\n1\n224202\n4.5\n4.6\n3\n93000\n-9999\n-9999\n-9999\n3.5\n4.0\n10.0\nclay\n[4.5, 4.6)\n[4.0, 10.0)\n\n\n2\n224202\n4.6\n5.0\n2\n94800\n-9999\n-9999\n-9999\n4.5\n4.0\n10.0\nclay\n[4.6, 5.0)\n[4.0, 10.0)\n\n\n3\n224202\n5.0\n6.0\n3\n93000\n-9999\n-9999\n-9999\n3.5\n4.0\n10.0\nclay\n[5.0, 6.0)\n[4.0, 10.0)\n\n\n4\n224202\n6.0\n7.0\n2\n94800\n-9999\n-9999\n-9999\n4.5\n4.0\n10.0\nclay\n[6.0, 7.0)\n[4.0, 10.0)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n73\n975622\n37.0\n38.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n[37.0, 38.0)\n[30.0, 40.0)\n\n\n74\n975622\n38.0\n39.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n[38.0, 39.0)\n[30.0, 40.0)\n\n\n75\n975622\n39.0\n40.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n30.0\n40.0\npegmatite\n[39.0, 40.0)\n[30.0, 40.0)\n\n\n76\n975622\n40.0\n41.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n40.0\n41.0\nquartz\n[40.0, 41.0)\n[40.0, 41.0)\n\n\n77\n975622\n41.0\n42.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n41.0\n42.0\ngranite\n[41.0, 42.0)\n[41.0, 42.0)\n\n\n\n\n78 rows × 14 columns\n\n\n\n\ndf.loc[49:51]\n\n\n\n\n\n\n\n\nCollarId\nFromDepth_x\nToDepth_x\nAg_PPM\nAl_PPM\nAl2O_PPM\nAl2O3_PPM\nAlO_PPM\nAs_PPM\nFromDepth_y\nToDepth_y\nlithology_group\ndepthInterval\nlithInterval\n\n\n\n\n49\n975622\n14.2\n14.8\n2\n111000\n-9999\n-9999\n-9999\n13.5\n10.0\n15.0\ndiorite\n[14.2, 14.8)\n[10.0, 15.0)\n\n\n50\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n10.0\n15.0\ndiorite\n[14.8, 16.0)\n[10.0, 15.0)\n\n\n51\n975622\n14.8\n16.0\n2\n111000\n-9999\n-9999\n-9999\n13.5\n15.0\n30.0\nbasalt\n[14.8, 16.0)\n[15.0, 30.0)\n\n\n\n\n\n\n\n\nKey points\n\nPandas default merge command will force all columns names that match to join on all of the common columns.\nWe demonstrated how to merge based on a human-sensible range.\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML_workflow.html",
    "href": "notebooks/03-ML_workflow.html",
    "title": "Machine Learning (ML) From Scratch",
    "section": "",
    "text": "How can I use Python for Machine learning?\nHow to I wrange my data to work within an ML context?\nHow do I assess whether my models fit well?\nMost machine learning problems begin with a dataset, but before we can perform any kind of inference on that dataset we must create/wrangle/build it. This is often the most time-consuming and hard part of a successful machine learning workflow. There is no set procedure here, as all data is different, although there are a few simple methods we can take to make a useful dataset.\nMachine learning can be split into:\nWe will be using data from a submitted Manuscript (Butterworth and Barnett-Moore 2020) which was a finalist in the Unearthed, ExploreSA: Gawler Challenge. You can visit the original repo here.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-1---determine-our-target-variable",
    "href": "notebooks/03-ML_workflow.html#step-1---determine-our-target-variable",
    "title": "Machine Learning (ML) From Scratch",
    "section": "Step 1 - Determine our target variable",
    "text": "Step 1 - Determine our target variable\nLet’s explore our our main dataset.\n\nDeposit locations - mine and mineral occurrences\nThe most important dataset for this workflow is the currently known locations of mineral occurrences. Using the data we already know about these mineral deposits we will build a model to predict where future occurrences will be.\n\n# For working with shapefiles (packaged is called pyshp)\nimport shapefile\n# For working with dataframes\nimport pandas as pd\n\n\n# Set the filename\nmineshape=\"../data/MinesMinerals/mines_and_mineral_occurrences_all.shp\"\n\n# Set shapefile attributes and assign\nsf = shapefile.Reader(mineshape)\nfields = [x[0] for x in sf.fields][1:]\nrecords = sf.records()\nshps = [s.points for s in sf.shapes()]\n\n# Write into a dataframe for easy use\ndf = pd.DataFrame(columns=fields, data=records)\n\nView the metadata of the South Australian all mines and mineral deposits to get a better understanding for what features we could use as a target.\n\n#See what the dataframe looks like\nprint(df.columns)\n\n#For clean printing to html drop columns that contains annoying / and \\ chars.\n#And set max columns\npd.options.display.max_columns = 8\ndf.drop(columns=['REFERENCE','O_MAP_SYMB'])\n\nIndex(['MINDEP_NO', 'DEP_NAME', 'REFERENCE', 'COMM_CODE', 'COMMODS',\n       'COMMOD_MAJ', 'COMM_SPECS', 'GCHEM_ASSC', 'DISC_YEAR', 'CLASS_CODE',\n       'OPER_TYPE', 'MAP_SYMB', 'STATUS_VAL', 'SIZE_VAL', 'GEOL_PROV',\n       'DB_RES_RVE', 'DB_PROD', 'DB_DOC_IMG', 'DB_EXV_IMG', 'DB_DEP_IMG',\n       'DB_DEP_FLE', 'COX_CLASS', 'REG_O_CTRL', 'LOC_O_CTRL', 'LOC_O_COM',\n       'O_LITH_CDE', 'O_LITH01', 'O_STRAT_NM', 'H_LITH_CDE', 'H_LITH02',\n       'H_STRAT_NM', 'H_MAP_SYMB', 'EASTING', 'NORTHING', 'ZONE', 'LONGITUDE',\n       'LATITUDE', 'SVY_METHOD', 'HORZ_ACC', 'SRCE_MAP', 'SRCE_CNTRE',\n       'COMMENTS', 'O_MAP_SYMB'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nMINDEP_NO\nDEP_NAME\nCOMM_CODE\nCOMMODS\n...\nHORZ_ACC\nSRCE_MAP\nSRCE_CNTRE\nCOMMENTS\n\n\n\n\n0\n5219\nMOUNT DAVIES NO.2A\nNi\nNickel\n...\n2000.0\n500k meis\n\n\n\n\n1\n52\nONE STONE\nNi\nNickel\n...\n500.0\n71-385\n\n\n\n\n2\n8314\nHINCKLEY RANGE\nFe\nIron\n...\n500.0\n\n\n\n\n\n3\n69\nKALKA\nV, ILM\nVanadium, Ilmenite\n...\n100.0\n1 MILE\nmgt polygon on digital map\n\n\n\n4\n65\nECHIDNA\nNi\nNickel\n...\n20.0\n50K GEOL\nDH ECHIDNA PROSPECT\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8672\n6937\nYARINGA\nQTZE\nQuartzite\n...\n200.0\n50k moc\nfenced yard\n\n\n\n8673\n4729\nWELCHS\nSCHT\nSchist\n...\n20.0\n50k topo\n\n\n\n\n8674\n4718\nARCADIAN\nCLAY\nClay\n...\n5.0\nPlan 1951-0327\nPit\n\n\n\n8675\n1436\nMCDONALD\nAu\nGold\n...\n200.0\n50k moc\nqz float\n\n\n\n8676\n8934\nFAIRFIELD FARM\nSAND\nSand\n...\n20.0\n\npit\n\n\n\n\n\n8677 rows × 41 columns\n\n\n\n\n#We are building a model to target South Australia, so load in a map of it.\ngawlshape=\"../data/SA/SA_STATE_POLYGON_shp\"\nshapeRead = shapefile.Reader(gawlshape)\nshapes  = shapeRead.shapes()\n\n#Save the boundary xy pairs in arrays we will use throughout the workflow\nxval = [x[0] for x in shapes[1].points]\nyval = [x[1] for x in shapes[1].points]\n\n\n# Subset the data, for a single Mineral target\ncommname='Mn'\n\n#Pull out all the occurences of the commodity and go from there\ncomm=df[df['COMM_CODE'].str.contains(commname)]\ncomm=comm.reset_index(drop=True)\nprint(\"Shape of \"+ commname, comm.shape)\n\n# Can make further subsets of the data here if needed\n#commsig=comm[comm.SIZE_VAL!=\"Low Significance\"]\n#comm=comm[comm.SIZE_VAL!=\"Low Significance\"]\n#comm=comm[comm.COX_CLASS == \"Olympic Dam Cu-U-Au\"]\n#comm=comm[(comm.lon&lt;max(xval)) & (comm.lon&gt;min(xval)) & (comm.lat&gt;min(yval)) & (comm.lat&lt;max(yval))]\n\nShape of Mn (115, 43)\n\n\n\n# For plotting\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nax.plot(df.LONGITUDE,df.LATITUDE,'b.',label=\"All Mineral Deposits\")\nax.plot(comm.LONGITUDE,comm.LATITUDE,'yx',label=commname+\" Deposits\")\n\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\n#ax.plot(comm.LONGITUDE, comm.LATITUDE, marker='o', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\nplt.show()"
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-2---wrangle-the-geophysical-and-geological-datasets-predictor-variables",
    "href": "notebooks/03-ML_workflow.html#step-2---wrangle-the-geophysical-and-geological-datasets-predictor-variables",
    "title": "Machine Learning (ML) From Scratch",
    "section": "Step 2 - Wrangle the geophysical and geological datasets (predictor variables)",
    "text": "Step 2 - Wrangle the geophysical and geological datasets (predictor variables)\nMany geophysical data are available for South Australia overlapping our target mineral locations. We may presume that certain mineral occurrences express a combination of geology and geophysics. We can train an algorithm to learn these associations and then use the same algorithm to make predictions for where unknown occurrences may be found.\nHere we load in the (slightly) pre-processed geophysical datasets and prepare them for further manipulations, data-mining, and machine learning. All of the full/raw datasets are available from https://map.sarig.sa.gov.au/. For this exercise we have simplified the datasets by reducing complexity and resolution. Grab additional processed datasets from https://github.com/natbutter/gawler-exploration/tree/master/ML-DATA\n\nResistivity xyz data\n\n#Read in the data\ndata_res=pd.read_csv(\"../data/AusLAMP_MT_Gawler_25.xyzr\",\n                     sep=',',header=0,names=['lat','lon','depth','resistivity'])\ndata_res\n\n\n\n\n\n\n\n\nlat\nlon\ndepth\nresistivity\n\n\n\n\n0\n-27.363931\n128.680796\n-25.0\n2.0007\n\n\n1\n-27.659362\n128.662322\n-25.0\n1.9979\n\n\n2\n-27.886602\n128.647965\n-25.0\n1.9948\n\n\n3\n-28.061394\n128.636833\n-25.0\n1.9918\n\n\n4\n-28.195844\n128.628217\n-25.0\n1.9885\n\n\n...\n...\n...\n...\n...\n\n\n11003\n-35.127716\n142.399588\n-25.0\n2.0079\n\n\n11004\n-35.230939\n142.408396\n-25.0\n2.0084\n\n\n11005\n-35.365124\n142.419903\n-25.0\n2.0085\n\n\n11006\n-35.539556\n142.434958\n-25.0\n2.0076\n\n\n11007\n-35.766303\n142.454694\n-25.0\n2.0049\n\n\n\n\n11008 rows × 4 columns\n\n\n\nThis data is the Lat-Lon spatial location and the value of the feature at that location.\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nim=ax.scatter(data_res.lon,data_res.lat,s=4,c=data_res.resistivity,cmap=\"jet\")\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nax.plot(comm.LONGITUDE, comm.LATITUDE, marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\ncbaxes = fig.add_axes([0.40, 0.18, 0.2, 0.015])\ncbar = plt.colorbar(im, cax = cbaxes,orientation=\"horizontal\",extend='both')\ncbar.set_label('Resistivity $\\Omega$.m', labelpad=10)\ncbar.ax.xaxis.set_label_position('top')\n\nplt.show()\n\n\n\n\n\n\nFaults and dykes vector polylines\n\n# For dealing with arrays \nimport numpy as np\n\n\n#Get fault data neo\nfaultshape=\"../data/Faults/Faults.shp\"\nshapeRead = shapefile.Reader(faultshape)\nshapes  = shapeRead.shapes()\nNshp    = len(shapes)\n\nfaultsNeo=[]\nfor i in range(0,Nshp):\n    for j in shapes[i].points:\n        faultsNeo.append([j[0],j[1]])\nfaultsNeo=np.array(faultsNeo)\nfaultsNeo\n\narray([[133.46269605, -27.41825034],\n       [133.46770683, -27.42062991],\n       [133.4723624 , -27.42259841],\n       ...,\n       [138.44613353, -35.36560605],\n       [138.44160669, -35.36672662],\n       [138.43805501, -35.36793484]])\n\n\nThis data is just a Lat-Lon location. Think how we can use this in a model.\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nplt.plot(faultsNeo[:,0],faultsNeo[:,1],'.',markersize=0.1,label=\"Neoproterozoic-Faults\")\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nax.plot(comm.LONGITUDE, comm.LATITUDE, marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\nplt.show()\n\n\n\n\n\n\nNetcdf formatted raster grids - geophysics\n\n# For timing events\nimport time\n# For making grids and reading netcdf data\nimport scipy\nimport scipy.io\n\n\n#Define a function to read the netcdf files\ndef readnc(filename):\n    tic=time.time()\n    rasterfile=filename\n    data = scipy.io.netcdf_file(rasterfile,'r',mmap=False)\n    xdata=data.variables['lon'][:]\n    ydata=data.variables['lat'][:]\n    zdata=np.array(data.variables['Band1'][:])\n    data.close()\n    \n    toc=time.time()\n    print(\"Loaded\", rasterfile, \"in\", f'{toc-tic:.2f}s')\n    print(\"Spacing x\", f'{xdata[2]-xdata[1]:.2f}', \n          \"y\", f'{ydata[2]-ydata[1]:.2f}', \n          \"Shape:\", np.shape(zdata), \"Min x:\", np.min(xdata), \"Max x:\", np.max(xdata),\n          \"Min y:\", np.min(ydata), f'Max y {np.max(ydata):.2f}')\n\n    return(xdata,ydata,zdata,np.min(xdata),np.min(ydata),xdata[2]-xdata[1],ydata[2]-ydata[1])\n\n\n# Digital Elevation Model\nx1,y1,z1,originx1,originy1,pixelx1,pixely1 = readnc(\"../data/sa-dem.nc\")\n# Total Magnetic Intensity\nx2,y2,z2,originx2,originy2,pixelx2,pixely2 = readnc(\"../data/sa-mag-tmi.nc\")\n# Gravity\nx3,y3,z3,originx3,originy3,pixelx3,pixely3 = readnc(\"../data/sa-grav.nc\")\n\nLoaded ../data/sa-dem.nc in 0.00s\nSpacing x 0.01 y 0.01 Shape: (1208, 1201) Min x: 129.005 Max x: 141.005 Min y: -38.065 Max y -25.99\nLoaded ../data/sa-mag-tmi.nc in 0.00s\nSpacing x 0.01 y 0.01 Shape: (1208, 1201) Min x: 129.005 Max x: 141.005 Min y: -38.065 Max y -25.99\nLoaded ../data/sa-grav.nc in 0.00s\nSpacing x 0.01 y 0.01 Shape: (1208, 1201) Min x: 129.005 Max x: 141.005 Min y: -38.065 Max y -25.99\n\n\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nim=plt.pcolormesh(x1,y1,z1,cmap='Greys',shading='auto')\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nax.plot(comm.LONGITUDE, comm.LATITUDE, marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\ncbaxes = fig.add_axes([0.40, 0.18, 0.2, 0.015])\ncbar = plt.colorbar(im, cax = cbaxes,orientation=\"horizontal\",extend='both')\ncbar.set_label('DEM (m)', labelpad=10)\ncbar.ax.xaxis.set_label_position('top')\n\nplt.show()\n\n\n\n\nThese data are raster grids. Essentially Lat-Lon-Value like the XYZ data, but represented in a different format.\n\n\nCategorical Geology in vector polygons\n\n#Archean basement geology\ngeolshape=shapefile.Reader(\"../data/Archaean_Early_Mesoprterzoic_polygons_shp/geology_archaean.shp\")\n\nrecsArch   = geolshape.records()\nshapesArch  = geolshape.shapes()\n\n\n# Print the field names in the shapefile\nfor i,field in enumerate(geolshape.fields):\n    print(i-1,field[0]) \n\n-1 DeletionFlag\n0 MAJORSTRAT\n1 SG_DESCRIP\n2 MAPUNIT\n3 SG_PROVINC\n4 DOMAIN\n5 AGE\n6 SEQUSET\n7 PRIMARYAGE\n8 OROGENYAGE\n9 INHERITAGE\n10 STRATNO\n11 STRATNAME\n12 STRATDESC\n13 GISCODE\n14 SUBDIVNAME\n15 SUBDIVSYMB\n16 PROVINCE\n17 MAXAGE\n18 MAXMOD\n19 MAXMETH\n20 MINAGE\n21 MINMOD\n22 MINMETH\n23 GLCODE\n\n\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\n\n#index of the geology unit #4 #10 #12\ngeoindex = 4\n#Gather all the unique Major Geology unit numbers\nlabs=[]\nfor i in recsArch:\n    labs.append(i[geoindex])\n\ngeols = list(set(labs))\n\n# Create a unique color for each geological unit label\ncolor = plt.cm.tab20(np.linspace(0, 1, len(geols)))\ncdict={}\nfor i, geol in enumerate(geols):\n    cdict.update({geol:color[i]})\n    \n#Plot each of the geology polygons\nlegend1=[]\nfor i in range(len(shapesArch)):\n    boundary = shapesArch[i].points\n    xs = [x for x, y in shapesArch[i].points]\n    ys = [y for x, y in shapesArch[i].points]\n    c = cdict[recsArch[i][geoindex]]\n    l1 = ax.fill(xs,ys,c=c,label=recsArch[i][geoindex])\n    legend1.append(l1)\n      \n#Plot the extra stuff\nl2 = ax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\nl3 = ax.plot(comm.LONGITUDE, comm.LATITUDE, \n        marker='s', markeredgecolor='k', linestyle='',markersize=4, color='y',\n        label=commname+\" Deposits\")\n\n#Todo: Split the legends\n#ax.legend([l2,l3],['SA',commname+\" Deposits\"],loc=3)\n\n#Legend without duplicate values\nhandles, labels = ax.get_legend_handles_labels()\nunique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\nax.legend(*zip(*unique), bbox_to_anchor = (1.02, 1.01), ncol=3)\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\n#plt.legend(loc=3) #bbox_to_anchor = (1.05, 0.6))\n\nplt.show()\n\n\n\n\nTake a moment to appreciate the various methods you have used just to load the data!\nNow we need to think about what we actually want to achieve? What is our goal here? This will determine what kind of data analysis/manipulation we need to make here. Consider the flow diagram for choosing the right machine learning method."
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-3---assign-geophys-values-to-target-locations",
    "href": "notebooks/03-ML_workflow.html#step-3---assign-geophys-values-to-target-locations",
    "title": "Machine Learning (ML) From Scratch",
    "section": "Step 3 - Assign geophys values to target locations",
    "text": "Step 3 - Assign geophys values to target locations\nWe need to assign the values of each of these geophysical datasets (predictor variables) to the target class (i.e. mineral deposit locations). The assumption being that the occurrence of some mineral deposit (e.g. Cu) is a function of x1, x2, x3, x4, x5, x6. Where the Resistivity is x1, the distance to a Neoprotezoic fault is x2, the value of DEM, magnetic TMI, and Gravity is x3, x4, and x5, and the geological basement unit is x6.\n\n# Make a Target DataFrame of the points we want to interrogate the features for\ntd1 = comm[['LONGITUDE', 'LATITUDE']].copy()\n\n\nResistivity\n\n# For making KD Trees\nimport scipy.spatial\n\n\n# Define a function which \"coregisters\" a point from a bunch of other points.\ndef coregPoint(tree,point,region,retval='index'):\n    '''\n    Finds the nearest neighbour to a point from a bunch of other points\n    tree - a scipy CKTree to search for the point over\n    point - array([longitude,latitude])\n    region - integer, same units as data\n    '''\n    dists, indexes = tree.query(point,k=1,distance_upper_bound=region) \n\n    if retval=='index':\n        return (indexes)\n    elif retval=='dists':\n        return(dists)\n    \n\n\n# Find the values of the resetivity grid for each lat/lon deposit location.\n\n# Make a search-tree of the point-pairs for fast lookup of nearest matches\ntreeres = scipy.spatial.cKDTree(np.c_[data_res.lon,data_res.lat])\n\n# Perform the search for each point\nindexes = td1.apply(\n    lambda x: coregPoint(treeres,np.array([x.LONGITUDE, x.LATITUDE]),1,retval='index'), axis=1)\n\n\ntd1['res'] = data_res.loc[indexes].resistivity.values\ntd1\n\n\n\n\n\n\n\n\nLONGITUDE\nLATITUDE\nres\n\n\n\n\n0\n139.179436\n-29.877637\n2.2135\n\n\n1\n138.808767\n-30.086296\n2.3643\n\n\n2\n138.752281\n-30.445684\n2.1141\n\n\n3\n138.530506\n-30.533225\n2.2234\n\n\n4\n138.887019\n-30.565479\n2.1982\n\n\n...\n...\n...\n...\n\n\n110\n136.059715\n-34.327929\n3.4926\n\n\n111\n138.016821\n-35.733084\n2.0868\n\n\n112\n139.250036\n-34.250155\n1.9811\n\n\n113\n135.905480\n-34.425866\n2.7108\n\n\n114\n135.835578\n-34.509779\n3.1224\n\n\n\n\n115 rows × 3 columns\n\n\n\n\n\nFaults\n\n#Same for the fault data \n# but this time we get the \"distance to the point\", rather than the value at that point.\ntreefaults = scipy.spatial.cKDTree(faultsNeo)\n\ndists = td1.apply(\n    lambda x: coregPoint(treefaults,np.array([x.LONGITUDE, x.LATITUDE]),100,retval='dists'), axis=1)\n\n\ntd1['faults'] = dists\ntd1\n\n\n\n\n\n\n\n\nLONGITUDE\nLATITUDE\nres\nfaults\n\n\n\n\n0\n139.179436\n-29.877637\n2.2135\n0.010691\n\n\n1\n138.808767\n-30.086296\n2.3643\n0.103741\n\n\n2\n138.752281\n-30.445684\n2.1141\n0.006659\n\n\n3\n138.530506\n-30.533225\n2.2234\n0.013925\n\n\n4\n138.887019\n-30.565479\n2.1982\n0.007356\n\n\n...\n...\n...\n...\n...\n\n\n110\n136.059715\n-34.327929\n3.4926\n0.526835\n\n\n111\n138.016821\n-35.733084\n2.0868\n0.002451\n\n\n112\n139.250036\n-34.250155\n1.9811\n0.027837\n\n\n113\n135.905480\n-34.425866\n2.7108\n0.670323\n\n\n114\n135.835578\n-34.509779\n3.1224\n0.776152\n\n\n\n\n115 rows × 4 columns\n\n\n\n\n\nGeophysics\n\n# Define a function which \"coregisters\" a point within a raster.\ndef get_coords_at_point(originx,originy,pixelx,pixely,lon,lat):\n    '''\n    Given a point in some coordinate reference (e.g. lat/lon)\n    Find the closest point to that in an array (e.g. a raster)\n    and return the index location of that point in the raster.\n    INPUTS\n        \"output from \"gdal_data.GetGeoTransform()\"\n    originx: first point in first axis\n    originy: first point in second axis\n    pixelx: difference between x points\n    pixely: difference between y points\n    \n    lon: x/row-coordinate of interest\n    lat: y/column-coordinate of interest\n    \n    RETURNS\n    col: x index value from the raster\n    row: y index value from the raster\n    '''\n    row = int((lon - originx)/pixelx)\n    col = int((lat - originy)/pixely)\n\n    return (col, row)\n\n\n# Pass entire array of latlon and raster info to us in get_coords_at_point\ndef rastersearch(latlon,raster,originx,originy,pixelx,pixely):\n    zlist=[]\n    for lon,lat in zip(latlon.LONGITUDE,latlon.LATITUDE):\n        try:\n            zlist.append(raster[get_coords_at_point(originx,originy,pixelx,pixely,lon,lat)])\n        except:\n            zlist.append(np.nan)\n            \n    return(zlist)\n\n\ntd1['dem'] = rastersearch(td1,z1,originx1,originy1,pixelx1,pixely1)\ntd1['mag'] = rastersearch(td1,z2,originx2,originy2,pixelx2,pixely2)\ntd1['grav'] = rastersearch(td1,z3,originx3,originy3,pixelx3,pixely3)\n\n\ntd1\n\n\n\n\n\n\n\n\nLONGITUDE\nLATITUDE\nres\nfaults\ndem\nmag\ngrav\n\n\n\n\n0\n139.179436\n-29.877637\n2.2135\n0.010691\n187.297424\n-118.074890\n1.852599\n\n\n1\n138.808767\n-30.086296\n2.3643\n0.103741\n179.499237\n-209.410507\n-12.722121\n\n\n2\n138.752281\n-30.445684\n2.1141\n0.006659\n398.336823\n-159.566422\n-6.249788\n\n\n3\n138.530506\n-30.533225\n2.2234\n0.013925\n335.983429\n-131.176437\n-11.665316\n\n\n4\n138.887019\n-30.565479\n2.1982\n0.007356\n554.278198\n-192.363297\n-1.025702\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n110\n136.059715\n-34.327929\n3.4926\n0.526835\n45.866119\n-244.067841\n11.410070\n\n\n111\n138.016821\n-35.733084\n2.0868\n0.002451\n145.452789\n-203.566940\n18.458364\n\n\n112\n139.250036\n-34.250155\n1.9811\n0.027837\n276.489319\n-172.889587\n-1.714886\n\n\n113\n135.905480\n-34.425866\n2.7108\n0.670323\n162.431747\n569.713684\n15.066316\n\n\n114\n135.835578\n-34.509779\n3.1224\n0.776152\n89.274399\n64.385925\n24.267015\n\n\n\n\n115 rows × 7 columns\n\n\n\n\n# Check we got it right.\n# Plot a grid, and our interrogated points\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\nim=plt.pcolormesh(x3,y3,z3,cmap='jet',shading='auto',vmin=min(td1.grav),vmax=max(td1.grav))\n#ax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\n#ax.plot(comm.LONGITUDE, comm.LATITUDE, marker='o', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nax.scatter(td1.LONGITUDE, td1.LATITUDE, s=20, c=td1.grav,\n           label=commname+\" Gravity\",cmap='jet',vmin=min(td1.grav),vmax=max(td1.grav),edgecolors='white')\n\nplt.xlim(138,140)\nplt.ylim(-32,-30)\nplt.legend(loc=3)\n\ncbaxes = fig.add_axes([0.40, 0.18, 0.2, 0.015])\ncbar = plt.colorbar(im, cax = cbaxes,orientation=\"horizontal\",extend='both')\ncbar.set_label('Gravity (gal)', labelpad=10)\ncbar.ax.xaxis.set_label_position('top')\n\nplt.show()\n\n\n\n\n\n\nGeology\n\n# For dealing with shapefile components\nfrom shapely.geometry import Point\nfrom shapely.geometry import shape\n\n#Define a function to find what polygon a point lives inside (speed imporivements can be made here)\ndef shapeExplore(lon,lat,shapes,recs,record):\n    #'record' is the column index you want returned\n    for i in range(len(shapes)):\n        boundary = shapes[i]\n        if Point((lon,lat)).within(shape(boundary)):\n            return(recs[i][record])\n    #if you have been through the loop with no result\n    return('-9999')\n\n\n%%time\ngeoindex = 4\ntd1['geol']=td1.apply(lambda x: shapeExplore(x.LONGITUDE, x.LATITUDE, shapesArch,recsArch,geoindex), axis=1)\n\nCPU times: user 4.57 s, sys: 75.1 ms, total: 4.65 s\nWall time: 4.52 s\n\n\n\ntd1\n\n\n\n\n\n\n\n\nLONGITUDE\nLATITUDE\nres\nfaults\ndem\nmag\ngrav\ngeol\n\n\n\n\n0\n139.179436\n-29.877637\n2.2135\n0.010691\n187.297424\n-118.074890\n1.852599\nCrustal element Muloorina\n\n\n1\n138.808767\n-30.086296\n2.3643\n0.103741\n179.499237\n-209.410507\n-12.722121\nCrustal element Adelaide\n\n\n2\n138.752281\n-30.445684\n2.1141\n0.006659\n398.336823\n-159.566422\n-6.249788\nCrustal element Adelaide\n\n\n3\n138.530506\n-30.533225\n2.2234\n0.013925\n335.983429\n-131.176437\n-11.665316\nCrustal element Adelaide\n\n\n4\n138.887019\n-30.565479\n2.1982\n0.007356\n554.278198\n-192.363297\n-1.025702\nCrustal element Adelaide\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n110\n136.059715\n-34.327929\n3.4926\n0.526835\n45.866119\n-244.067841\n11.410070\nCleve, Spencer, Olympic Domains\n\n\n111\n138.016821\n-35.733084\n2.0868\n0.002451\n145.452789\n-203.566940\n18.458364\nCrustal element Kanmantoo SW\n\n\n112\n139.250036\n-34.250155\n1.9811\n0.027837\n276.489319\n-172.889587\n-1.714886\nCrustal element Kanmantoo Main\n\n\n113\n135.905480\n-34.425866\n2.7108\n0.670323\n162.431747\n569.713684\n15.066316\nCleve Domain\n\n\n114\n135.835578\n-34.509779\n3.1224\n0.776152\n89.274399\n64.385925\n24.267015\nCleve Domain\n\n\n\n\n115 rows × 8 columns\n\n\n\nCongrats, you now have an ML dataset ready to go!\nAlmost… but what is the target? Let’s make a binary classifier."
  },
  {
    "objectID": "notebooks/03-ML_workflow.html#step-4---generate-a-non-deposit-dataset",
    "href": "notebooks/03-ML_workflow.html#step-4---generate-a-non-deposit-dataset",
    "title": "Machine Learning (ML) From Scratch",
    "section": "Step 4 - Generate a “non-deposit” dataset",
    "text": "Step 4 - Generate a “non-deposit” dataset\nWe have a set of locations where a certain mineral deposit occurs along with the values of various geophysical parameters at those locations. To identify what values of the geophysics are associated with a mineral deposit then we need a representation of the “background noise” of those parameters, i.e. what the values are when there is no mineral deposit.\nThis step is important. There are numerous ways to generate our non-deposit set, each with different benefits and trade-offs. The randomisation of points throughout some domain appears to be robust. But you must think, is this domain a reasonable estimation of “background” geophysics/geology? Why are you picking these locations as non-deposits? Will they be over/under-representing actual deposits? Will they be over/under-representing actual non-deposits?\n\n#Now make a set of \"non-deposits\" using a random location within our exploration area\nlats_rand=np.random.uniform(low=min(df.LATITUDE), high=max(df.LATITUDE), size=len(comm.LATITUDE))\nlons_rand=np.random.uniform(low=min(df.LONGITUDE), high=max(df.LONGITUDE), size=len(comm.LONGITUDE))\n\nprint(\"Produced\", len(lats_rand),len(lons_rand), \"latitude-longitude pairs for non-deposits.\")\n\nProduced 115 115 latitude-longitude pairs for non-deposits.\n\n\n\n# Where are these randomised \"non deposits\"\nfig = plt.figure(figsize=(8,8))\nax = plt.axes()\n\nax.plot(xval,yval,'grey',linestyle='--',linewidth=1,label='SA')\n\nax.plot(lons_rand, lats_rand, \n        marker='.', linestyle='',markersize=1, color='b',label=\"Random Samples\")\n\nax.plot(td1.LONGITUDE, td1.LATITUDE, \n        marker='x', linestyle='',markersize=5, color='y',label=commname+\" Deposits\")\n\nplt.xlim(128.5,141.5)\nplt.ylim(-38.5,-25.5)\nplt.legend(loc=3)\n\nplt.show()\n\n\n\n\nWe must do the same coregistration/interrogation of the different data layers for our randomised “non-deposit” data.\n\n%%time\n\ntd2 = pd.DataFrame({'LONGITUDE': lons_rand, 'LATITUDE': lats_rand})\n                   \n# Res\nindexes = td2.apply(\n    lambda x: coregPoint(treeres,np.array([x.LONGITUDE, x.LATITUDE]),10,retval='index'), axis=1)\n    \ntd2['res'] = data_res.loc[indexes].resistivity.values\n\n# Faults\ntd2['faults'] = td2.apply(\n    lambda x: coregPoint(treefaults,np.array([x.LONGITUDE, x.LATITUDE]),100,retval='dists'), axis=1)\n\n# Geophys\ntd2['dem'] = rastersearch(td2,z1,originx1,originy1,pixelx1,pixely1)\ntd2['mag'] = rastersearch(td2,z2,originx2,originy2,pixelx2,pixely2)\ntd2['grav'] = rastersearch(td2,z3,originx3,originy3,pixelx3,pixely3)\n\n#Geology\ntd2['geol']=td2.apply(lambda x: shapeExplore(x.LONGITUDE, x.LATITUDE, shapesArch,recsArch,geoindex), axis=1)\n\nCPU times: user 10.5 s, sys: 122 ms, total: 10.6 s\nWall time: 10.4 s\n\n\n\n#Add flag indicating classification label\ntd1['deposit']=1\ntd2['deposit']=0\n\n\nfv = pd.concat([td1,td2],axis=0,ignore_index=True)\nfv\n\n\n\n\n\n\n\n\nLONGITUDE\nLATITUDE\nres\nfaults\n...\nmag\ngrav\ngeol\ndeposit\n\n\n\n\n0\n139.179436\n-29.877637\n2.2135\n0.010691\n...\n-118.074890\n1.852599\nCrustal element Muloorina\n1\n\n\n1\n138.808767\n-30.086296\n2.3643\n0.103741\n...\n-209.410507\n-12.722121\nCrustal element Adelaide\n1\n\n\n2\n138.752281\n-30.445684\n2.1141\n0.006659\n...\n-159.566422\n-6.249788\nCrustal element Adelaide\n1\n\n\n3\n138.530506\n-30.533225\n2.2234\n0.013925\n...\n-131.176437\n-11.665316\nCrustal element Adelaide\n1\n\n\n4\n138.887019\n-30.565479\n2.1982\n0.007356\n...\n-192.363297\n-1.025702\nCrustal element Adelaide\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\n140.264586\n-36.024709\n1.9738\n0.109419\n...\n82.679138\n3.195437\nCrustal element Kanmantoo Main\n0\n\n\n226\n132.210526\n-26.808665\n2.0358\n0.445198\n...\n-311.140411\n-32.652912\nFregon Subdomain\n0\n\n\n227\n137.838001\n-33.729123\n-0.6225\n0.333648\n...\n1048.547852\n-20.456957\nNawa, Mount Woods, Fowler, Wilgena, Harris Gr\n0\n\n\n228\n140.016574\n-32.481088\n2.0752\n0.234001\n...\n-259.214233\n-16.506145\nCrustal element Adelaide\n0\n\n\n229\n131.226528\n-30.922765\n1.9347\n0.020291\n...\n15.309577\n-38.666344\nFisher Domain\n0\n\n\n\n\n230 rows × 9 columns\n\n\n\n\n# Save all our hard work to a csv file for more hacking to come!\nfv.to_csv('../data/fv.csv',index=False)"
  }
]