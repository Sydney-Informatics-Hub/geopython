<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>01b-dataframes.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/simplex.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="lesson.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 41px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h2 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h3 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h4 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h5 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h6 {
  padding-top: 46px;
  margin-top: -46px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Setup
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="setup.html">Setup</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 1
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="00-intro.html">Introduction</a>
    </li>
    <li>
      <a href="01a-fundamentals.html">Fundamentals</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 2
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01b-dataframes.html">Useful Python packages for different data types</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 3
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="02b-Clustering.html">Numerical Approaches</a>
    </li>
    <li>
      <a href="03-ML_workflow.html">Machine Learning Workflow</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Session 4
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="04a-SimpleSpeedUps.html">Efficient Python Methods</a>
    </li>
    <li>
      <a href="02a-mapping.html">Bonus: Matplotlib and Mapping with Cartopy</a>
    </li>
    <li>
      <a href="03b-DeepLearningTS.html">Bonus: Deep Learning &amp; Time Series</a>
    </li>
    <li>
      <a href="04b-DaskDataframes.html">Bonus: Dask data frames</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="useful-python-packages-for-different-data-types" class="section level1">
<h1>Useful Python packages for different data types</h1>
<div id="questions" class="section level3 questions">
<h3>Questions</h3>
<ul>
<li>What are libraries and packages?</li>
<li>How can I load tabular data into Python?</li>
<li>How can I load shapefiles?</li>
<li>How can I load segy and las data?</li>
</ul>
</div>
<div id="objectives" class="section level3 objectives">
<h3>Objectives</h3>
<ul>
<li>Learn how to deal with specialty data types.</li>
<li>Learn about pandas, pyshp, lasio, obspy.</li>
</ul>
</div>
<p>Python can deal with basically any type of data you throw at it. The open source python community has developed many packages that make things easy. Today we will look at <code>pyshp</code> (for dealing with shapefiles), <code>pandas</code> (great for tables and time series), <code>lasio</code> (for las format well log data) and <code>obspy</code> (a highly featured seismic data processing suite) packages.</p>
<p>Data for this exercised was downloaded from <a href="http://www.bom.gov.au/water/groundwater/explorer/map.shtml" class="uri">http://www.bom.gov.au/water/groundwater/explorer/map.shtml</a></p>
</div>
<div id="shapefiles" class="section level1">
<h1>Shapefiles</h1>
<p>Shapefiles are a very common file format for GIS data, the standard for which is developed and maintained by ESRI, the makers of the ArcGIS software. Shapefiles collect vectors of features, such as points, lines, polygons. The “file” is actually a misnomer - if you look at a single “shapefile” on your machine using a file explorer, you can see that it’s actually made up of several files, three of which are mandatory, and others which may/may not be there.</p>
<pre class="python"><code>#Load the required modules
import shapefile

#NOTE: Weirdly and confusingly, this package is called &quot;pyshp&quot; but you call it via the name &quot;shapefile&quot;</code></pre>
<pre class="python"><code>#help(shapefile)
#Or check out the help pages https://github.com/GeospatialPython/pyshp</code></pre>
<pre class="python"><code>#Set the filename
boreshape=&#39;../data/shp_torrens_river/NGIS_BoreLine.shp&#39;

#read in the file
shapeRead = shapefile.Reader(boreshape)

#And save out some of the shape file attributes
recs    = shapeRead.records()
shapes  = shapeRead.shapes()
fields  = shapeRead.fields
Nshp    = len(shapes)</code></pre>
<pre class="python"><code>print(Nshp) #print the Number of items in the shapefile</code></pre>
<pre><code>7635</code></pre>
<pre class="python"><code>fields #print the fields</code></pre>
<pre><code>[(&#39;DeletionFlag&#39;, &#39;C&#39;, 1, 0),
 [&#39;HydroID&#39;, &#39;N&#39;, 10, 0],
 [&#39;HydroCode&#39;, &#39;C&#39;, 30, 0],
 [&#39;BoreID&#39;, &#39;N&#39;, 10, 0],
 [&#39;TopElev&#39;, &#39;F&#39;, 19, 11],
 [&#39;BottomElev&#39;, &#39;F&#39;, 19, 11],
 [&#39;HGUID&#39;, &#39;N&#39;, 10, 0],
 [&#39;HGUNumber&#39;, &#39;N&#39;, 10, 0],
 [&#39;NafHGUNumb&#39;, &#39;N&#39;, 10, 0],
 [&#39;SHAPE_Leng&#39;, &#39;F&#39;, 19, 11]]</code></pre>
<pre class="python"><code>recs[3] #print the first record, then this is a list that can be subscripted further</code></pre>
<pre><code>Record #3: [32002002, &#39;652800645&#39;, 30027773, -147.26, -154.26, 31000045, 1044, 125005, 0.0]</code></pre>
<pre class="python"><code>shapes[1].points #print the point values of the first shape</code></pre>
<pre><code>[(591975.5150000006, -3816141.8817), (591975.5150000006, -3816141.8817)]</code></pre>
<pre class="python"><code>shapeRead.shapeTypeName </code></pre>
<pre><code>&#39;POLYLINEZ&#39;</code></pre>
<pre class="python"><code>rec= shapeRead.record(0)
rec[&#39;TopElev&#39;]</code></pre>
<pre><code>6.74</code></pre>
<div id="challenge." class="section level3 challenge">
<h3>Challenge.</h3>
<ul>
<li>Look at the data above. It provides the coordinates of the wells as points.</li>
<li>How many coordinates are provided for each well? Why do you think this is?</li>
<li>What is the Bottom Elevation of the 300th record?</li>
</ul>
<details>
<summary>
Solution
</summary>
<p>There are two coordinates. But they are duplicated.</p>
<pre class="python"><code>    
rec= shapeRead.record(299)
rec[&#39;BottomElev&#39;]
    
#or
    
recs[299][4]</code></pre>
</details>
</div>
<pre class="python"><code>#Here is a slightly neater way to read in the data, but it looks confusing at first.
#But we will need it in this form for our next exercise.

#This type of assignment is known as &quot;list comprehension&quot;
#fields = [x[0] for x in shapeRead.fields][1:]

#Break this down line by line
#for x in shapeRead.fields:
#    print(x)

#Now just print the 1st (0th) column of each list variable
#for x in shapeRead.fields:
#    print(x[0])

#But we want to save these values in a list (not just print them out).
#fields=[]
#for x in shapeRead.fields:
#    fields.append(x[0])

#And we don&#39;t want the DeletionFlag field, so we need to just get all the values except the first
#fields=fields[1:]

#fields = [x[0] for x in shapeRead.fields][1:]

#Do a list comprehnsion for the the other variable too
#shps = [s.points for s in shapeRead.shapes()]</code></pre>
<p>Shapefiles are not a native python format, but the community have developed tools for exploring them. The package we have used “pyshp” imported with the name “shapefile” (for some non-consistent weird reason), is one example of working with shapefiles. Alternatives exist.</p>
</div>
<div id="dataframes-and-table-manipulation" class="section level1">
<h1>Dataframes and table manipulation</h1>
<p>Pandas is one of the most useful packages (along with probably numpy and matplotlib). We will use it several times throughout the course for data handling and manipulation.</p>
<pre class="python"><code>#As before, read in the shapefile
boreshape=&#39;../data/shp_torrens_river/NGIS_BoreLine.shp&#39;

#Read the shapefile attributes to variables
shapeRead = shapefile.Reader(boreshape)
fields = [x[0] for x in shapeRead.fields][1:]
shps = [s.points for s in shapeRead.shapes()]
recs= shapeRead.records()</code></pre>
<pre class="python"><code>import pandas
#Now convert the variables to a pandas dataframe
df = pandas.DataFrame(columns=fields, data=recs)

#See more details at the docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html</code></pre>
<pre class="python"><code>df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
HydroID
</th>
<th>
HydroCode
</th>
<th>
BoreID
</th>
<th>
TopElev
</th>
<th>
BottomElev
</th>
<th>
HGUID
</th>
<th>
HGUNumber
</th>
<th>
NafHGUNumb
</th>
<th>
SHAPE_Leng
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
32001999
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
6.74
</td>
<td>
-74.26
</td>
<td>
31000043
</td>
<td>
1042
</td>
<td>
104005
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
32002000
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-74.26
</td>
<td>
-125.26
</td>
<td>
31000109
</td>
<td>
1108
</td>
<td>
110002
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
32002001
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-125.26
</td>
<td>
-147.26
</td>
<td>
31000045
</td>
<td>
1044
</td>
<td>
125005
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
32002002
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-147.26
</td>
<td>
-154.26
</td>
<td>
31000045
</td>
<td>
1044
</td>
<td>
125005
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
32002003
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-154.26
</td>
<td>
-168.26
</td>
<td>
31000045
</td>
<td>
1044
</td>
<td>
125005
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
7630
</th>
<td>
32145557
</td>
<td>
662810075
</td>
<td>
30057044
</td>
<td>
102.62
</td>
<td>
90.89
</td>
<td>
31000139
</td>
<td>
1138
</td>
<td>
100001
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
7631
</th>
<td>
32145558
</td>
<td>
662810075
</td>
<td>
30057044
</td>
<td>
103.08
</td>
<td>
102.62
</td>
<td>
31000139
</td>
<td>
1138
</td>
<td>
100001
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
7632
</th>
<td>
32145559
</td>
<td>
662813065
</td>
<td>
30060034
</td>
<td>
535.08
</td>
<td>
451.08
</td>
<td>
31000026
</td>
<td>
1025
</td>
<td>
134001
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
7633
</th>
<td>
32145560
</td>
<td>
662813065
</td>
<td>
30060034
</td>
<td>
451.08
</td>
<td>
171.08
</td>
<td>
31000014
</td>
<td>
1013
</td>
<td>
134001
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
7634
</th>
<td>
32145561
</td>
<td>
662814687
</td>
<td>
30061656
</td>
<td>
444.30
</td>
<td>
432.30
</td>
<td>
31000014
</td>
<td>
1013
</td>
<td>
134001
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
<p>
7635 rows × 9 columns
</p>
</div>
<pre class="python"><code>#Add a new column called &quot;coords&quot; to the DataFrame, fill it with what is in &quot;shps&quot;
df[&#39;coords&#39;] = shps

#Alternatively, you can use the &quot;assign&quot; method
#df = df.assign(coords=shps)</code></pre>
<pre class="python"><code>df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
HydroID
</th>
<th>
HydroCode
</th>
<th>
BoreID
</th>
<th>
TopElev
</th>
<th>
BottomElev
</th>
<th>
HGUID
</th>
<th>
HGUNumber
</th>
<th>
NafHGUNumb
</th>
<th>
SHAPE_Leng
</th>
<th>
coords
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
32001999
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
6.74
</td>
<td>
-74.26
</td>
<td>
31000043
</td>
<td>
1042
</td>
<td>
104005
</td>
<td>
0.0
</td>
<td>
[(591975.5150000006, -3816141.8817), (591975.5…
</td>
</tr>
<tr>
<th>
1
</th>
<td>
32002000
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-74.26
</td>
<td>
-125.26
</td>
<td>
31000109
</td>
<td>
1108
</td>
<td>
110002
</td>
<td>
0.0
</td>
<td>
[(591975.5150000006, -3816141.8817), (591975.5…
</td>
</tr>
<tr>
<th>
2
</th>
<td>
32002001
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-125.26
</td>
<td>
-147.26
</td>
<td>
31000045
</td>
<td>
1044
</td>
<td>
125005
</td>
<td>
0.0
</td>
<td>
[(591975.5150000006, -3816141.8817), (591975.5…
</td>
</tr>
<tr>
<th>
3
</th>
<td>
32002002
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-147.26
</td>
<td>
-154.26
</td>
<td>
31000045
</td>
<td>
1044
</td>
<td>
125005
</td>
<td>
0.0
</td>
<td>
[(591975.5150000006, -3816141.8817), (591975.5…
</td>
</tr>
<tr>
<th>
4
</th>
<td>
32002003
</td>
<td>
652800645
</td>
<td>
30027773
</td>
<td>
-154.26
</td>
<td>
-168.26
</td>
<td>
31000045
</td>
<td>
1044
</td>
<td>
125005
</td>
<td>
0.0
</td>
<td>
[(591975.5150000006, -3816141.8817), (591975.5…
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
7630
</th>
<td>
32145557
</td>
<td>
662810075
</td>
<td>
30057044
</td>
<td>
102.62
</td>
<td>
90.89
</td>
<td>
31000139
</td>
<td>
1138
</td>
<td>
100001
</td>
<td>
0.0
</td>
<td>
[(605865.9246000014, -3830429.3729999997), (60…
</td>
</tr>
<tr>
<th>
7631
</th>
<td>
32145558
</td>
<td>
662810075
</td>
<td>
30057044
</td>
<td>
103.08
</td>
<td>
102.62
</td>
<td>
31000139
</td>
<td>
1138
</td>
<td>
100001
</td>
<td>
0.0
</td>
<td>
[(605865.9246000014, -3830429.3729999997), (60…
</td>
</tr>
<tr>
<th>
7632
</th>
<td>
32145559
</td>
<td>
662813065
</td>
<td>
30060034
</td>
<td>
535.08
</td>
<td>
451.08
</td>
<td>
31000026
</td>
<td>
1025
</td>
<td>
134001
</td>
<td>
0.0
</td>
<td>
[(612545.5916999988, -3832402.8148999996), (61…
</td>
</tr>
<tr>
<th>
7633
</th>
<td>
32145560
</td>
<td>
662813065
</td>
<td>
30060034
</td>
<td>
451.08
</td>
<td>
171.08
</td>
<td>
31000014
</td>
<td>
1013
</td>
<td>
134001
</td>
<td>
0.0
</td>
<td>
[(612545.5916999988, -3832402.8148999996), (61…
</td>
</tr>
<tr>
<th>
7634
</th>
<td>
32145561
</td>
<td>
662814687
</td>
<td>
30061656
</td>
<td>
444.30
</td>
<td>
432.30
</td>
<td>
31000014
</td>
<td>
1013
</td>
<td>
134001
</td>
<td>
0.0
</td>
<td>
[(635716.0604999997, -3816751.5364999995), (63…
</td>
</tr>
</tbody>
</table>
<p>
7635 rows × 10 columns
</p>
</div>
<p>Pandas more frequently is used to directly read in tables. So let’s read in the csv data that came with shapefile (as this gives us some additional fields not stored in the shapefile that we can explore.</p>
<pre class="python"><code>#read in the data
log_data=pandas.read_csv(&quot;../data/shp_torrens_river/NGIS_LithologyLog.csv&quot;,usecols=list(range(0,13))) 

#What is the &quot;usecols&quot; variable equal to?
#Try reading the data without using the usecols option, can you solve the error?</code></pre>
<pre class="python"><code>log_data           # print the first 30 and last 30 rows</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
OBJECTID
</th>
<th>
BoreID
</th>
<th>
HydroCode
</th>
<th>
RefElev
</th>
<th>
RefElevDesc
</th>
<th>
FromDepth
</th>
<th>
ToDepth
</th>
<th>
TopElev
</th>
<th>
BottomElev
</th>
<th>
MajorLithCode
</th>
<th>
MinorLithCode
</th>
<th>
Description
</th>
<th>
Source
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1769789
</td>
<td>
30062892
</td>
<td>
662815923
</td>
<td>
57.25
</td>
<td>
NGS
</td>
<td>
18.0
</td>
<td>
19.5
</td>
<td>
39.25
</td>
<td>
37.75
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Clay
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1769790
</td>
<td>
30062892
</td>
<td>
662815923
</td>
<td>
57.25
</td>
<td>
NGS
</td>
<td>
19.5
</td>
<td>
22.0
</td>
<td>
37.75
</td>
<td>
35.25
</td>
<td>
ROCK
</td>
<td>
None
</td>
<td>
Rocks and sand
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1769791
</td>
<td>
30062892
</td>
<td>
662815923
</td>
<td>
57.25
</td>
<td>
NGS
</td>
<td>
22.0
</td>
<td>
24.0
</td>
<td>
35.25
</td>
<td>
33.25
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Clay
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1770725
</td>
<td>
30141910
</td>
<td>
662816624
</td>
<td>
4.0
</td>
<td>
NGS
</td>
<td>
0.0
</td>
<td>
6.0
</td>
<td>
4.0
</td>
<td>
-2.0
</td>
<td>
None
</td>
<td>
None
</td>
<td>
No sample
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1770726
</td>
<td>
30141910
</td>
<td>
662816624
</td>
<td>
4.0
</td>
<td>
NGS
</td>
<td>
6.0
</td>
<td>
15.0
</td>
<td>
-2.0
</td>
<td>
-11.0
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Clay - orange-red grey, mottled; stiff-sticky….
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
70168
</th>
<td>
4120345
</td>
<td>
30304039
</td>
<td>
662829228
</td>
<td>
None
</td>
<td>
UNK
</td>
<td>
9.0
</td>
<td>
10.0
</td>
<td>
None
</td>
<td>
None
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Sandy clay
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
70169
</th>
<td>
4120346
</td>
<td>
30304039
</td>
<td>
662829228
</td>
<td>
None
</td>
<td>
UNK
</td>
<td>
10.0
</td>
<td>
12.5
</td>
<td>
None
</td>
<td>
None
</td>
<td>
SAND
</td>
<td>
None
</td>
<td>
Clay sand
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
70170
</th>
<td>
4120347
</td>
<td>
30304050
</td>
<td>
652802882
</td>
<td>
None
</td>
<td>
UNK
</td>
<td>
0.0
</td>
<td>
0.3
</td>
<td>
None
</td>
<td>
None
</td>
<td>
FILL
</td>
<td>
None
</td>
<td>
Fill
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
70171
</th>
<td>
4120348
</td>
<td>
30304050
</td>
<td>
652802882
</td>
<td>
None
</td>
<td>
UNK
</td>
<td>
0.3
</td>
<td>
0.8
</td>
<td>
None
</td>
<td>
None
</td>
<td>
SAND
</td>
<td>
None
</td>
<td>
Clayey sand
</td>
<td>
SAGeodata
</td>
</tr>
<tr>
<th>
70172
</th>
<td>
4120349
</td>
<td>
30304050
</td>
<td>
652802882
</td>
<td>
None
</td>
<td>
UNK
</td>
<td>
0.8
</td>
<td>
3.5
</td>
<td>
None
</td>
<td>
None
</td>
<td>
SAND
</td>
<td>
None
</td>
<td>
Sand
</td>
<td>
SAGeodata
</td>
</tr>
</tbody>
</table>
<p>
70173 rows × 13 columns
</p>
</div>
<pre class="python"><code># add a new column as a function of existing columns
log_data[&#39;Thickness&#39;] = log_data.ToDepth - log_data.FromDepth</code></pre>
<pre class="python"><code>type(log_data)     # see what Python type the DataFrame is</code></pre>
<pre><code>pandas.core.frame.DataFrame</code></pre>
<pre class="python"><code>log_data.head(3)    # print the first 3 rows</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
OBJECTID
</th>
<th>
BoreID
</th>
<th>
HydroCode
</th>
<th>
RefElev
</th>
<th>
RefElevDesc
</th>
<th>
FromDepth
</th>
<th>
ToDepth
</th>
<th>
TopElev
</th>
<th>
BottomElev
</th>
<th>
MajorLithCode
</th>
<th>
MinorLithCode
</th>
<th>
Description
</th>
<th>
Source
</th>
<th>
Thickness
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1769789
</td>
<td>
30062892
</td>
<td>
662815923
</td>
<td>
57.25
</td>
<td>
NGS
</td>
<td>
18.0
</td>
<td>
19.5
</td>
<td>
39.25
</td>
<td>
37.75
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Clay
</td>
<td>
SAGeodata
</td>
<td>
1.5
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1769790
</td>
<td>
30062892
</td>
<td>
662815923
</td>
<td>
57.25
</td>
<td>
NGS
</td>
<td>
19.5
</td>
<td>
22.0
</td>
<td>
37.75
</td>
<td>
35.25
</td>
<td>
ROCK
</td>
<td>
None
</td>
<td>
Rocks and sand
</td>
<td>
SAGeodata
</td>
<td>
2.5
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1769791
</td>
<td>
30062892
</td>
<td>
662815923
</td>
<td>
57.25
</td>
<td>
NGS
</td>
<td>
22.0
</td>
<td>
24.0
</td>
<td>
35.25
</td>
<td>
33.25
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Clay
</td>
<td>
SAGeodata
</td>
<td>
2.0
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>log_data.index     # “the index” (aka “the labels”). 
#Pandas is great for using timeseries data, where the index can be the timestamps</code></pre>
<pre><code>RangeIndex(start=0, stop=70173, step=1)</code></pre>
<pre class="python"><code>log_data.columns  # column names (which is “an index”)</code></pre>
<pre><code>Index([&#39;OBJECTID&#39;, &#39;BoreID&#39;, &#39;HydroCode&#39;, &#39;RefElev&#39;, &#39;RefElevDesc&#39;,
       &#39;FromDepth&#39;, &#39;ToDepth&#39;, &#39;TopElev&#39;, &#39;BottomElev&#39;, &#39;MajorLithCode&#39;,
       &#39;MinorLithCode&#39;, &#39;Description&#39;, &#39;Source&#39;, &#39;Thickness&#39;],
      dtype=&#39;object&#39;)</code></pre>
<pre class="python"><code>log_data.dtypes    # data types of each column</code></pre>
<pre><code>OBJECTID           int64
BoreID             int64
HydroCode          int64
RefElev           object
RefElevDesc       object
FromDepth        float64
ToDepth          float64
TopElev           object
BottomElev        object
MajorLithCode     object
MinorLithCode     object
Description       object
Source            object
Thickness        float64
dtype: object</code></pre>
<pre class="python"><code>log_data.shape     # number of rows and columns</code></pre>
<pre><code>(70173, 14)</code></pre>
<pre class="python"><code>log_data.values    # underlying numpy array — df are stored as numpy arrays for efficiencies.</code></pre>
<pre><code>array([[1769789, 30062892, 662815923, ..., &#39;Clay&#39;, &#39;SAGeodata&#39;, 1.5],
       [1769790, 30062892, 662815923, ..., &#39;Rocks and sand&#39;, &#39;SAGeodata&#39;,
        2.5],
       [1769791, 30062892, 662815923, ..., &#39;Clay&#39;, &#39;SAGeodata&#39;, 2.0],
       ...,
       [4120347, 30304050, 652802882, ..., &#39;Fill&#39;, &#39;SAGeodata&#39;, 0.3],
       [4120348, 30304050, 652802882, ..., &#39;Clayey sand&#39;, &#39;SAGeodata&#39;,
        0.5],
       [4120349, 30304050, 652802882, ..., &#39;Sand&#39;, &#39;SAGeodata&#39;, 2.7]],
      dtype=object)</code></pre>
<pre class="python"><code>#log_data[&#39;MajorLithCode&#39;]         # select one column
##Equivalent to 
#log_data.MajorLithCode 
##and
#log_data.iloc[:,9]
##and
#log_data.loc[:,&#39;MajorLithCode&#39;]</code></pre>
<pre class="python"><code>type(log_data[&#39;MajorLithCode&#39;])   # determine datatype of column (e.g., Series)</code></pre>
<pre><code>pandas.core.series.Series</code></pre>
<pre class="python"><code>#describe the data frame
log_data.describe(include=&#39;all&#39;)     </code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
OBJECTID
</th>
<th>
BoreID
</th>
<th>
HydroCode
</th>
<th>
RefElev
</th>
<th>
RefElevDesc
</th>
<th>
FromDepth
</th>
<th>
ToDepth
</th>
<th>
TopElev
</th>
<th>
BottomElev
</th>
<th>
MajorLithCode
</th>
<th>
MinorLithCode
</th>
<th>
Description
</th>
<th>
Source
</th>
<th>
Thickness
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
7.017300e+04
</td>
<td>
7.017300e+04
</td>
<td>
7.017300e+04
</td>
<td>
70173
</td>
<td>
70173
</td>
<td>
70173.000000
</td>
<td>
70173.000000
</td>
<td>
70173
</td>
<td>
70173
</td>
<td>
70173
</td>
<td>
70173
</td>
<td>
70173
</td>
<td>
70173
</td>
<td>
70173.000000
</td>
</tr>
<tr>
<th>
unique
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
5068
</td>
<td>
4
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
27784
</td>
<td>
27883
</td>
<td>
81
</td>
<td>
42
</td>
<td>
33614
</td>
<td>
54
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
top
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
None
</td>
<td>
NGS
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
None
</td>
<td>
None
</td>
<td>
CLYU
</td>
<td>
None
</td>
<td>
Clay
</td>
<td>
SAGeodata
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
freq
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
18514
</td>
<td>
44947
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
18514
</td>
<td>
18514
</td>
<td>
25861
</td>
<td>
62812
</td>
<td>
4603
</td>
<td>
70119
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
2.505677e+06
</td>
<td>
3.018198e+07
</td>
<td>
6.624491e+08
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
24.938020
</td>
<td>
30.621160
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
5.683139
</td>
</tr>
<tr>
<th>
std
</th>
<td>
9.276182e+05
</td>
<td>
8.069609e+04
</td>
<td>
2.130226e+06
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
45.431762
</td>
<td>
48.605931
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
9.942400
</td>
</tr>
<tr>
<th>
min
</th>
<td>
1.769789e+06
</td>
<td>
3.002715e+07
</td>
<td>
6.528000e+08
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
0.000000
</td>
<td>
0.010000
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
0.000000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
1.932741e+06
</td>
<td>
3.014557e+07
</td>
<td>
6.628129e+08
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
0.800000
</td>
<td>
3.960000
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
1.000000
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
1.999028e+06
</td>
<td>
3.018487e+07
</td>
<td>
6.628196e+08
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
7.000000
</td>
<td>
11.500000
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
2.800000
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
3.967146e+06
</td>
<td>
3.025487e+07
</td>
<td>
6.628248e+08
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
25.800000
</td>
<td>
34.700000
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
7.000000
</td>
</tr>
<tr>
<th>
max
</th>
<td>
4.120349e+06
</td>
<td>
3.030405e+07
</td>
<td>
6.728042e+08
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
610.300000
</td>
<td>
620.160000
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
300.500000
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code># summarise a pandas Series
log_data.FromDepth.describe()   # describe a single column</code></pre>
<pre><code>count    70173.000000
mean        24.938020
std         45.431762
min          0.000000
25%          0.800000
50%          7.000000
75%         25.800000
max        610.300000
Name: FromDepth, dtype: float64</code></pre>
<pre class="python"><code>#calculate mean of 6th column (&quot;FromDepth&quot;)
log_data.iloc[:,5].mean()      </code></pre>
<pre><code>24.93802017870121</code></pre>
<pre class="python"><code>#alternate method to calculate mean of FromDepth column (the 6th one)
log_data[&quot;FromDepth&quot;].mean()    </code></pre>
<pre><code>24.93802017870121</code></pre>
<pre class="python"><code>#Count how many Lith Codes there are
lithCounts=log_data.MajorLithCode.value_counts()</code></pre>
<pre class="python"><code>#Print the lithcodes, use .index or .values 
lithCounts</code></pre>
<pre><code>CLYU    25861
SAND    12772
SLAT     4071
FILL     4020
SDST     3209
        ...  
DIOR        1
SANN        1
HFLS        1
CALU        1
REGO        1
Name: MajorLithCode, Length: 81, dtype: int64</code></pre>
<pre class="python"><code>#plot a bar chart of the lith codes
lithCounts.plot.bar(figsize=(15,5))</code></pre>
<pre><code>&lt;AxesSubplot:&gt;</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_38_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#Plot a bar chart of the lith codes for the rarer lithologies
lithCounts[(lithCounts &lt; 50)].plot.bar(rot=45,figsize=(15,5))</code></pre>
<pre><code>&lt;AxesSubplot:&gt;</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_39_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>import numpy as np
import matplotlib.pyplot as plt
 
#Pandas data
x = log_data.Thickness
mu = log_data.Thickness.mean()
sigma = log_data.Thickness.std()

# An Equivalent Numpy version
#x = log_data.Thickness.values
#mu = np.mean(x) # mean of distribution
#sigma = np.std(x) # standard deviation of distribution

# the histogram of the data
plt.hist(x, bins=np.arange(0,20,1), alpha=0.5)
plt.xlabel(&#39;Thickness (m)&#39;)
plt.ylabel(&#39;Count&#39;)
mystring=&quot;Histogram with a mean of &quot;+ str(np.round(mu,2)) + &quot; &amp; SD &quot; + str(np.round(sigma,2))
plt.title(mystring)
 
# Tweak spacing to prevent clipping of ylabel
#plt.subplots_adjust(left=0.15)
plt.show()</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_40_0.png" alt="" />
<p class="caption">png</p>
</div>
<div id="challenge" class="section level3 challenge">
<h3>Challenge</h3>
<p>Plot a histogram of the thickness where the “MajorLithCode” is “FILL”.</p>
<p>Hint: to filter a pandas data frame by value use the following syntax:</p>
<p><code>df[df['Variable'] == "value"]</code></p>
<details>
<summary>
Solution
</summary>
<pre class="python"><code>x = log_data[log_data[&#39;MajorLithCode&#39;] == &quot;FILL&quot;][&#39;Thickness&#39;].values
#OR
#x = log_data[log_data.MajorLithCode == &quot;FILL&quot;].Thickness.values

plt.hist(x, bins=np.arange(0,20,1), alpha=0.5)
plt.show()</code></pre>
</details>
</div>
<pre class="python"><code># import numpy as np
# cmap = plt.get_cmap(&#39;viridis&#39;)
# colors = cmap(np.linspace(0, 1, len(lithCounts.index)))
# colors

# for row in log_data.itertuples():
#     boreid=row[3]
#     for ind,value in enumerate(recs):  
#         try:
#             value.index(boreid)
#             print(recs)
#         except:
#             continue
#     #(row[3])


#You can plot the location of the bores slowly
# for ind, value in enumerate(recs):
#     #Get the lat lon value
#     lon=df.coords[ind][0][0]
#     lat=df.coords[ind][0][1]
#     #Get the Lithology unit
#     #value[]
    
#     #Now add the point to the plot
#     plt.plot(lon,lat,&quot;|&quot;)
    
# plt.show()

# #or fast
# lons= [df.coords[i][0][0] for i in range(1,len(recs))] 
# lats= [df.coords[i][0][1] for i in range(1,len(recs))] 
# plt.plot(lons,lats,&quot;|&quot;)
# plt.show()</code></pre>
<div id="extra-credit-challenge" class="section level3 challenge">
<h3>Extra credit challenge</h3>
<p>Go to <a href="http://www.bom.gov.au/water/groundwater/explorer/map.shtml">http://www.bom.gov.au/water/groundwater/explorer/map.shtml</a> and pick another River Region. Download the dataset in “Shapefile” format (this will download the csv also). Once you have the data, follow the same routines as above and see what you can find out about the river region.</p>
<details>
<summary>
Solution
</summary>
<p>TODO Nate</p>
</details>
</div>
</div>
<div id="log-ascii-files" class="section level1">
<h1>Log ASCII Files</h1>
<p>Python has a wide range of packages/libraries to do specific tasks. You can often create your own tools for doing niche tasks, but often you will find that many already exist to make things simpler for you. We will explore libraries that work with borehole data (in .las format) with the <a href="https://lasio.readthedocs.io/en/latest/">lasio</a> library.</p>
<p>This tutorial based off <a href="https://towardsdatascience.com/handling-big-volume-of-well-log-data-with-a-boosted-time-efficiency-with-python-dfe0319daf26" class="uri">https://towardsdatascience.com/handling-big-volume-of-well-log-data-with-a-boosted-time-efficiency-with-python-dfe0319daf26</a></p>
<p>Original Data from: <a href="https://sarigbasis.pir.sa.gov.au/WebtopEw/ws/samref/sarig1/image/DDD/PEDP013LOGS.zip" class="uri">https://sarigbasis.pir.sa.gov.au/WebtopEw/ws/samref/sarig1/image/DDD/PEDP013LOGS.zip</a></p>
<p>Title: Cooper Basin selected well logs in LAS format.<br />
Publication Date: November 20<br />
Prepared by: Energy Resources Division, Department of the Premier and Cabinet<br />
This Record URL: <a href="https://sarigbasis.pir.sa.gov.au/WebtopEw/ws/samref/sarig1/wci/Record?r=0&amp;m=1&amp;w=catno=2040037" class="uri">https://sarigbasis.pir.sa.gov.au/WebtopEw/ws/samref/sarig1/wci/Record?r=0&amp;m=1&amp;w=catno=2040037</a></p>
<pre class="python"><code>#For plotting
import matplotlib.pyplot as plt

#Library specifically for &quot;well data&quot;
import lasio

#To read files
import glob

#For &quot;regular expression manipulation&quot;
import re</code></pre>
<pre class="python"><code>#Build a list of filenames to read
read_files = glob.glob(&quot;../data/WELL/*.las&quot;)
read_files</code></pre>
<pre><code>[&#39;../data/WELL/Burrungule1.las&#39;,
 &#39;../data/WELL/BoolLagoon1.las&#39;,
 &#39;../data/WELL/BeachportEast1.las&#39;,
 &#39;../data/WELL/Balnaves.las&#39;,
 &#39;../data/WELL/BiscuitFlat1.las&#39;,
 &#39;../data/WELL/Bungaloo1.las&#39;,
 &#39;../data/WELL/Beachport1.las&#39;,
 &#39;../data/WELL/Banyula.las&#39;]</code></pre>
<p><strong>Note:</strong> the possibility of Windows VS Unix character interpretations.</p>
<pre class="python"><code>#Cut out just the name of the well from the filenames
well_names = []
for file in read_files:
    print(&quot;FILE:&quot;, file)
    #Split the filepath at a &quot;/&quot; OR a &quot;.las&quot; OR a &quot;\&quot;
    well=re.split(r&#39;/|\\|.las&#39;,file)
    print(&quot;SPLIT:&quot;, well, &quot;\n&quot;)
    well_names.append(well[-2])

print(&quot;There are &quot;, len(well_names), &quot;wells.&quot;)
print(well_names)</code></pre>
<pre><code>FILE: ../data/WELL/Burrungule1.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;Burrungule1&#39;, &#39;&#39;] 

FILE: ../data/WELL/BoolLagoon1.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;BoolLagoon1&#39;, &#39;&#39;] 

FILE: ../data/WELL/BeachportEast1.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;BeachportEast1&#39;, &#39;&#39;] 

FILE: ../data/WELL/Balnaves.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;Balnaves&#39;, &#39;&#39;] 

FILE: ../data/WELL/BiscuitFlat1.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;BiscuitFlat1&#39;, &#39;&#39;] 

FILE: ../data/WELL/Bungaloo1.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;Bungaloo1&#39;, &#39;&#39;] 

FILE: ../data/WELL/Beachport1.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;Beachport1&#39;, &#39;&#39;] 

FILE: ../data/WELL/Banyula.las
SPLIT: [&#39;..&#39;, &#39;data&#39;, &#39;WELL&#39;, &#39;Banyula&#39;, &#39;&#39;] 

There are  8 wells.
[&#39;Burrungule1&#39;, &#39;BoolLagoon1&#39;, &#39;BeachportEast1&#39;, &#39;Balnaves&#39;, &#39;BiscuitFlat1&#39;, &#39;Bungaloo1&#39;, &#39;Beachport1&#39;, &#39;Banyula&#39;]</code></pre>
<pre class="python"><code>#Now actually read in the log files to lasio
#The last cell was just to automatically make a nicely formatted list of well names!
lases = []
for files in read_files:
    las = lasio.read(files)
    lases.append(las)</code></pre>
<pre class="python"><code>#You can get an idea of what you can interogate using the help function
#help(lases)</code></pre>
<pre class="python"><code>#This is just a regular Python list! But the list contains
#in this case, special objects known as &quot;LasFile(s)&quot; or lasio.las object.
#Get some details using help again
#help(lases[1])</code></pre>
<pre class="python"><code>#From there we can get some info from each of the wells
j=0
for well in lases:
    #e.g. pull out the varaibles availble from the wells
    print(&quot;Wellid:&quot;, j, well_names[j])
    j+=1
    print(well.keys())</code></pre>
<pre><code>Wellid: 0 Burrungule1
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;DT&#39;, &#39;GR&#39;, &#39;RDEP&#39;, &#39;RMED&#39;, &#39;SP&#39;]
Wellid: 1 BoolLagoon1
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;DRHO&#39;, &#39;DT&#39;, &#39;GR&#39;, &#39;NPHI&#39;, &#39;PEF&#39;, &#39;RDEP&#39;, &#39;RHOB&#39;, &#39;RMED&#39;, &#39;SP&#39;]
Wellid: 2 BeachportEast1
[&#39;DEPTH&#39;, &#39;GR&#39;, &#39;RDEP&#39;, &#39;RMED&#39;, &#39;SP&#39;]
Wellid: 3 Balnaves
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;DRHO&#39;, &#39;DT&#39;, &#39;GR&#39;, &#39;MINV&#39;, &#39;MNOR&#39;, &#39;NPHI&#39;, &#39;PEF&#39;, &#39;RDEP&#39;, &#39;RHOB&#39;, &#39;RMED&#39;, &#39;RMIC&#39;, &#39;SP&#39;]
Wellid: 4 BiscuitFlat1
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;DRHO&#39;, &#39;DT&#39;, &#39;GR&#39;, &#39;MINV&#39;, &#39;MNOR&#39;, &#39;NPHI&#39;, &#39;PEF&#39;, &#39;RDEP&#39;, &#39;RHOB&#39;, &#39;RMED&#39;, &#39;RMIC&#39;, &#39;SP&#39;]
Wellid: 5 Bungaloo1
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;DRHO&#39;, &#39;DT&#39;, &#39;DTS&#39;, &#39;GR&#39;, &#39;NPHI&#39;, &#39;PEF&#39;, &#39;RDEP&#39;, &#39;RHOB&#39;, &#39;RMED&#39;, &#39;RMIC&#39;, &#39;SP&#39;]
Wellid: 6 Beachport1
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;MINV&#39;, &#39;MNOR&#39;, &#39;RDEP&#39;, &#39;RMED&#39;, &#39;SP&#39;]
Wellid: 7 Banyula
[&#39;DEPTH&#39;, &#39;CALI&#39;, &#39;DRHO&#39;, &#39;DT&#39;, &#39;GR&#39;, &#39;NPHI&#39;, &#39;RDEP&#39;, &#39;RHOB&#39;, &#39;RMED&#39;, &#39;SP&#39;]</code></pre>
<pre class="python"><code>#Set a wellid you want to explore more
wellid=1</code></pre>
<pre class="python"><code>#Make a plot of one of the wells
plt.plot(lases[wellid][&#39;DRHO&#39;],lases[wellid][&#39;DEPTH&#39;])</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f3b59bf3910&gt;]</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_54_1.png" alt="" />
<p class="caption">png</p>
</div>
<p>You have just plotted the density (DRHO) at each measured depth point. You can clean this up and present it better in the next few cells.</p>
<pre class="python"><code>#Get some more info out of the well data
print(lases[wellid].curves)</code></pre>
<pre><code>Mnemonic  Unit   Value  Description                                         
--------  ----   -----  -----------                                         
DEPTH     M             Depth                                               
CALI      in            Caliper     CALI Edited, bool_lagoon_1.lis          
DRHO      g/cm3         DenCorr     DRHO Edited, bool_lagoon_1.lis          
DT        us/ft         Sonic       DT Edited, bool_lagoon_1.lis            
GR        gAPI          GammaRay    GR , bool_lagoon_1.lis                  
NPHI      dec           Neutron     NPHI Edited, bool_lagoon_1.lis          
PEF       b/e           PEFactor    PEF Edited, bool_lagoon_1.lis           
RDEP      ohmm          DeepRes     LLD Edited, Shifted, bool_lagoon_1.lis  
RHOB      g/cm3         Density     RHOB Edited, bool_lagoon_1.lis          
RMED      ohmm          MedRes      LLS Edited, Shifted, bool_lagoon_1.lis  
SP        MV            SP          SP Edited, Shifted, bool_lagoon_1.lis   </code></pre>
<div class="challenge">

<div id="challenge-1" class="section level3">
<h3>Challenge</h3>
<p>Run this bit of code. Then add additional mnemonic plots to the figure.</p>
<pre class="python"><code>#Import additional packages we will need
import numpy as np
import pandas as pd

#Convert a data array to a pandas dataframe
#and find significant spikes in the data
#Return the spikes as a binary 1 or 0 array
def find_unc(data):
    #Convert data to pandas
    df=pd.DataFrame(data)
    #Caluclate the rolling average 
    #(200 is somewhat arbitray value to take the rolling average over)
    df_mean = df.rolling(200).mean()
    #Calculate the percent change (i.e any points of change) of the data
    df_change = df_mean.pct_change(periods=200)
    #Convert large percent changes to 1 or 0
    #0.5 (50%) is a somewhat arbitray number 
    #to set as the amount of change in the data
    dfbin = ((df_change &lt; -0.5) | (df_change &gt; 0.5)).astype(int)
    #Return the binaray array
    return(dfbin)

#Define a function to make the plot and set parameters
def make_plot(i,var,colour):
    #Set the data to a variable
    data=lases[wellid][var]
    #Find the spikes in the data
    dfbin=find_unc(data)
    #Now perform the plotting
    top=min(lases[wellid][&#39;DEPTH&#39;])
    bot=max(lases[wellid][&#39;DEPTH&#39;])
    ax[i].plot(dfbin*np.nanmax(data), lases[wellid][&#39;DEPTH&#39;], color = &#39;black&#39;, linewidth = 0.5)
    ax[i].plot(data, lases[wellid][&#39;DEPTH&#39;], color = colour, linewidth = 0.5)
    ax[i].set_xlabel(var)
    ax[i].xaxis.label.set_color(colour)
    ax[i].set_xlim(np.nanpercentile(lases[wellid][var],0.5), np.nanpercentile(lases[wellid][var],99.5))
    ax[i].tick_params(axis=&#39;x&#39;, colors=colour)
    ax[i].title.set_color(colour)
    ax[i].set_ylim(top,bot)
    ax[i].invert_yaxis()
    ax[i].tick_params(left=False,
                bottom=True,
                labelleft=False,
                labelbottom=True)

#Make the figure
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,6))

#Add a plot of each mnemoic to your figure
make_plot(0,&quot;GR&quot;,&quot;green&quot;)
make_plot(1,&quot;RDEP&quot;,&quot;red&quot;)

#Quick way to get the list of keys
#lases[wellid].keys()

#Fix the details on the figure
plt.subplots_adjust(wspace=0.01)
ax[0].set_ylabel(&quot;Depth (m)&quot;)
ax[0].tick_params(left=True,
            bottom=True,
            labelleft=True,
            labelbottom=True)</code></pre>
<details>
<summary>
Solution
</summary>
<p>To solve the challenge you can change the <code>ncols</code> varibale and then add new calls to the <code>make_plot</code> function.</p>
<pre class="python"><code>#Change the number of columns
fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(16,6))
    
#Add additional calls to the make plot
make_plot(0,&quot;CALI&quot;,&quot;green&quot;)
make_plot(1,&quot;DRHO&quot;,&quot;red&quot;)
make_plot(2,&quot;DT&quot;,&quot;blue&quot;)
make_plot(3,&quot;GR&quot;,&quot;purple&quot;)
make_plot(4,&quot;RDEP&quot;,&quot;cyan&quot;)
make_plot(5,&quot;RHOB&quot;,&quot;pink&quot;)
make_plot(6,&quot;RMED&quot;,&quot;brown&quot;)
make_plot(7,&quot;SP&quot;,&quot;orange&quot;)</code></pre>
<p>To learn more about the smoothing steps make a diagnostic plots at each step.</p>
<pre class="python"><code>#Set an example dataset, i.e.
#wellid 1 and var is &quot;GR&quot;
data=lases[1][&quot;GR&quot;]

#Convert data to pandas
df=pd.DataFrame(data)

plt.plot(df)
plt.title(&quot;Raw data&quot;)
plt.show()

#Caluclate the rolling average 
df_mean = df.rolling(200).mean()

plt.plot(df_mean)
plt.title(&quot;Smoothed data&quot;)
plt.show()

#Calculate the percent change (i.e any points of change) of the data
df_change = df_mean.pct_change(periods=200)

plt.plot(df_change)
plt.title(&quot;Percentage changes along the smoothed data&quot;)
plt.show()

#Convert large percent changes to 1 or 0
dfbin = ((df_change &lt; -0.2) | (df_change &gt; 0.2)).astype(int)

plt.plot(dfbin)
plt.title(&quot;&#39;Binarised&#39; version of large percentage changes&quot;)
plt.show()</code></pre>
</div>
</div>
<div id="segy-seismic-data-processing" class="section level1">
<h1>SEGY Seismic data processing</h1>
<pre class="python"><code>from obspy.io.segy.segy import _read_segy
import matplotlib.pyplot as plt
import numpy as np

#Adapted from https://agilescientific.com/blog/2016/9/21/x-lines-of-python-read-and-write-seg-y
#See the notebooks here for more good examples
#https://github.com/agile-geoscience/xlines</code></pre>
<pre class="python"><code>#Set the filename of the segy data

filename=&quot;../data/james/james_1959_pstm_tvfk_gain.sgy&quot;

#Data randomly chosen from here:
#Title: 2006 James 3D Seismic Survey.
#Author: White, A.
#Prepared by: Terrex Seismic Pty Ltd; Pioneer Surveys Pty Ltd; WestenGeco
#Tenement: PPL00182
#Operator: Santos Ltd
#https://sarigbasis.pir.sa.gov.au/WebtopEw/ws/samref/sarig1/wci/Record?r=0&amp;m=1&amp;w=catno=2035790</code></pre>
<pre class="python"><code>#This will take about 1 minute. 
#When the [*] changes to [52] and the circle in the top right is clear, it has completed
stream = _read_segy(filename, headonly=True)
print(np.shape(stream.traces))
stream</code></pre>
<pre><code>(48832,)





48832 traces in the SEG Y structure.</code></pre>
<pre class="python"><code>#Look at a single trace
one_trace = stream.traces[10000]

#Print out details single trace
print(one_trace)

plt.figure(figsize=(16,2))
plt.plot(one_trace.data)
plt.show()</code></pre>
<pre><code>Trace sequence number within line: 10001
1001 samples, dtype=float32, 250.00 Hz</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_62_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#Stack multiple traces into a single numpy array
data = np.stack([t.data for t in stream.traces[12320:12320+500]])</code></pre>
<pre class="python"><code>#What does the stacked data look like
data.shape</code></pre>
<pre><code>(500, 1001)</code></pre>
<pre class="python"><code>#Have a look at the data
plt.imshow(data.T, cmap=&quot;Greys&quot;, aspect=&#39;auto&#39;)</code></pre>
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f3bab243dc0&gt;</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_65_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#Make a more informative plot

#Restrict the data to the 95th percentile
vm = np.percentile(data, 95)

print(&quot;The 95th percentile is {:.0f}; the max amplitude is {:.0f}&quot;.format(vm, data.max()))

#Make the plot
plt.figure(figsize=(16,8))
plt.imshow(data.T, cmap=&quot;RdBu&quot;, vmin=-vm, vmax=vm, aspect=&#39;auto&#39;)
plt.colorbar()
plt.show()</code></pre>
<pre><code>The 95th percentile is 4365; the max amplitude is 34148</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_66_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#What else is in the data?

#Print out the segy headers
print(stream.textual_file_header.decode())</code></pre>
<pre><code>C 1 CLIENT SANTOS                 COMPANY                       CREW NO         C 2 LINE    2000.00 AREA JAMES3D                                                C 3 REEL NO           DAY-START OF REEL     YEAR      OBSERVER                  C 4 INSTRUMENT  MFG            MODEL            SERIAL NO                       C 5 DATA TRACES/RECORD 24569  AUXILIARY TRACES/RECORD       0 CDP FOLD    40    C 6 SAMPLE INTERVAL  4.00   SAMPLES/TRACE  1001 BITS/IN      BYTES/SAMPLE  4    C 7 RECORDING FORMAT        FORMAT THIS REEL SEG-Y  MEASUREMENT SYSTEM METERS   C 8 SAMPLE CODE FLOATING PT                                                     C09 JAMES 3D                                                                    C10 WESTERNGECO                                                                 C11 MARCH 2007                                                                  C12 VERSION : James3D_pstm_tvfk_gain                                            C13 FILTERED TRIM PSTM STACK                                                    C14                                                                             C15 GEOMETRY APPLY-TAR-MINP-                                                    C16 NOISE REDUCTION - SWATT                                                     C17  SC DECON - SCAC                                                            C18 RESIDUAL_STATICS                                                            C19  TRIM_STATICS - INVERSE_TAR - SORT                                          C20 PSTM  - SORT  - GAIN                                                        C21 TRIM_STATICS - STACK                                                        C22 SPECW_10-70HZ -TVF_10-75HZ-TRACE_BALANCE                                    C23                                                                             C24                                                                             C25                                                                             C26                                                                             C27                                                                             C28                                                                             C29                                                                             C30                                                                             C31                                                                             C32                                                                             C33                                                                             C34                                                                             C35                                                                             C36                                                                             C37                                                                             C38                                                                             C39                                                                             C40 END EBCDIC                                                                  </code></pre>
<pre class="python"><code>#And the header information for a particular trace
print(stream.traces[1013].header)</code></pre>
<pre><code>trace_sequence_number_within_line: 1014
trace_sequence_number_within_segy_file: 1014
original_field_record_number: 2004
trace_number_within_the_original_field_record: 1
energy_source_point_number: 10026
ensemble_number: 10026
trace_number_within_the_ensemble: 28
trace_identification_code: 1
number_of_vertically_summed_traces_yielding_this_trace: 1
number_of_horizontally_stacked_traces_yielding_this_trace: 13
data_use: 1
distance_from_center_of_the_source_point_to_the_center_of_the_receiver_group: 0
receiver_group_elevation: 0
surface_elevation_at_source: 0
source_depth_below_surface: 0
datum_elevation_at_receiver_group: 0
datum_elevation_at_source: 0
water_depth_at_source: 0
water_depth_at_group: 0
scalar_to_be_applied_to_all_elevations_and_depths: 1
scalar_to_be_applied_to_all_coordinates: 1
source_coordinate_x: 482760
source_coordinate_y: 7035836
group_coordinate_x: 482760
group_coordinate_y: 7035836
coordinate_units: 1
weathering_velocity: 0
subweathering_velocity: 0
uphole_time_at_source_in_ms: 0
uphole_time_at_group_in_ms: 0
source_static_correction_in_ms: 0
group_static_correction_in_ms: 0
total_static_applied_in_ms: -70
lag_time_A: 0
lag_time_B: 0
delay_recording_time: 0
mute_time_start_time_in_ms: 0
mute_time_end_time_in_ms: 20
number_of_samples_in_this_trace: 1001
sample_interval_in_ms_for_this_trace: 4000
gain_type_of_field_instruments: 0
instrument_gain_constant: 0
instrument_early_or_initial_gain: 0
correlated: 0
sweep_frequency_at_start: 0
sweep_frequency_at_end: 0
sweep_length_in_ms: 0
sweep_type: 0
sweep_trace_taper_length_at_start_in_ms: 0
sweep_trace_taper_length_at_end_in_ms: 0
taper_type: 0
alias_filter_frequency: 0
alias_filter_slope: 0
notch_filter_frequency: 0
notch_filter_slope: 0
low_cut_frequency: 0
high_cut_frequency: 0
low_cut_slope: 0
high_cut_slope: 0
year_data_recorded: 0
day_of_year: 0
hour_of_day: 0
minute_of_hour: 0
second_of_minute: 0
time_basis_code: 0
trace_weighting_factor: 0
geophone_group_number_of_roll_switch_position_one: 0
geophone_group_number_of_trace_number_one: 0
geophone_group_number_of_last_trace: 0
gap_size: 0
over_travel_associated_with_taper: 0
x_coordinate_of_ensemble_position_of_this_trace: 0
y_coordinate_of_ensemble_position_of_this_trace: 0
for_3d_poststack_data_this_field_is_for_in_line_number: 0
for_3d_poststack_data_this_field_is_for_cross_line_number: -4587520
shotpoint_number: 2004
scalar_to_be_applied_to_the_shotpoint_number: 0
trace_value_measurement_unit: 10026
transduction_constant_mantissa: 0
transduction_constant_exponent: 0
transduction_units: 0
device_trace_identifier: 0
scalar_to_be_applied_to_times: 1052
source_type_orientation: 0
source_energy_direction_mantissa: 0
source_energy_direction_exponent: 1607
source_measurement_mantissa: 0
source_measurement_exponent: 0
source_measurement_unit: 0</code></pre>
<pre class="python"><code>#You can automatically extract data you might need from the header
#Get the sample interval from the header info
dt = stream.traces[0].header.sample_interval_in_ms_for_this_trace / 1e6
dt</code></pre>
<pre><code>0.004</code></pre>
<div class="challenge">

<div id="challenge-2" class="section level3">
<h3>Challenge</h3>
<p>A single seismic section can be viewed with this snippet of code:</p>
<pre class="python"><code>#Set number of xlines
n=262
#Set start iline
m=0

print(m,m*n,m*n+n)
data = np.stack(t.data for t in stream.traces[m*n:m*n+n])
vm = np.percentile(data, 95)
plt.figure(figsize=(14,4))
plt.imshow(data.T,cmap=&quot;RdBu&quot;, vmin=-vm, vmax=vm, aspect=&#39;auto&#39;)
plt.show()</code></pre>
<p>Can you put this in a loop to show multiple sections at once?</p>
<details>
<summary>
Solution
</summary>
<p>…</p>
<pre class="python"><code>#Set number of xlines
n=262
#Set start iline
m=0

while m &lt; 10:
    print(m,m*n,m*n+n)
    data = np.stack(t.data for t in stream.traces[m*n:m*n+n])
    vm = np.percentile(data, 95)
    plt.figure(figsize=(14,4))
    plt.imshow(data.T,cmap=&quot;RdBu&quot;, vmin=-vm, vmax=vm, aspect=&#39;auto&#39;)
    plt.show()
    m=m+1</code></pre>
</div>
</div>
<div id="roll-your-own-data-reader" class="section level1">
<h1>Roll-your-own data reader</h1>
<div id="western-seismic-velf-format" class="section level2">
<h2>Western seismic VELF format</h2>
<p>Sometimes there are no good libraries or data-readers availble for your specific use-case. Very common if you get some specialty instrument with some unique data format. Often the documentation is a good place to start for figuring out how to interpret the data, and more-often-than not, the <em>header</em> of the data can give you all the information you need. Download <a href="data/S3D_Vrms_StkVels_VELF.txt">this VELF file</a>, containing some 3D seismic data. I could not find any <em>good</em> python libraries to handle this dataset, so we can just try out a few things to get the data into a format that is useful for us.</p>
<pre class="python"><code>#Imports for plotting
import matplotlib.pyplot as plt
from matplotlib import cm
import pandas as pd</code></pre>
<pre class="python"><code>#Open the file for reading
f = open(&quot;../data/S3D_Vrms_StkVels_VELF.txt&quot;,&#39;r&#39;)

#Read in all the lines in the file and save them to a variable
mylist = f.readlines()

#Close the file
f.close()
print(&quot;Done reading file.&quot;)</code></pre>
<pre><code>Done reading file.</code></pre>
<pre class="python"><code>#Print out the first 20 lines
print(mylist[0:20])</code></pre>
<pre><code>[&#39;Client: XXX \n&#39;, &#39;Project: YYY\n&#39;, &#39;Contractor: ZZZ\n&#39;, &#39;Date:\n&#39;, &#39;\n&#39;, &#39;Velocity type: RMS Velocity in Time\n&#39;, &#39;\n&#39;, &#39;\n&#39;, &#39;Datum: GDA94, UTM Zone: UTM53, Central Meridian :  \n&#39;, &#39;Statics: Two way time corrected to mean sea level: No\n&#39;, &#39;         Gun and Cable statics applied: No\n&#39;, &#39;         Tidal statics applied: No\n&#39;, &#39;\n&#39;, &#39;3D Grid details:\n&#39;, &#39;inline    crossline      X            Y\n&#39;, &#39;\n&#39;, &#39;1000        5000      599413.78   7382223.37\n&#39;, &#39;1000        5309      595633.30   7375486.63\n&#39;, &#39;1448        5000      609180.96   7376742.28\n&#39;, &#39;1448        5309      605400.48   7370005.55\n&#39;]</code></pre>
<pre class="python"><code>#Set up some empty lists to store each bit of data in
inline=[]
crossline=[]
X=[]
Y=[]

velf=[]

#Loop through all the lines in the file
for i,line in enumerate(mylist):
    
    #First split the line up (by default, split by whitespace)
    splitline=line.split()
    
    #If we encounter certain lines, save some data
    if i in [16,17,18,19]:  
        #Print out the lines (check we are doing the right thing)
        print(splitline)
        
        inline.append(int(splitline[0]))
        crossline.append(int(splitline[1]))
        X.append(float(splitline[2]))
        Y.append(float(splitline[3]))
      

    #This is where the actual data starts
    #Now depending on the key word at the start of each line
    #save the data to each particular list/array
    #Read the data in again, this time with some thought about what we actually want to do
    if i&gt;49:
        if splitline[0]==&#39;LINE&#39;:
            LINE = int(splitline[1])
            
        if splitline[0]==&#39;SPNT&#39;:
            xline3d=int(splitline[1])
            binx=float(splitline[2])
            biny=float(splitline[3])
            inline3d=int(splitline[4])
            
        if splitline[0]==&#39;VELF&#39;:
            
            for j,val in enumerate(splitline[1:]):
                #print(j,val)
                #Counting from the 0th index of splitline[1:end]
                if j%2==0:
                    t=int(val)
                else:
                    vt=int(val)
                    velf.append([LINE,xline3d,binx,biny,inline3d,t,vt]) 
</code></pre>
<pre><code>[&#39;1000&#39;, &#39;5000&#39;, &#39;599413.78&#39;, &#39;7382223.37&#39;]
[&#39;1000&#39;, &#39;5309&#39;, &#39;595633.30&#39;, &#39;7375486.63&#39;]
[&#39;1448&#39;, &#39;5000&#39;, &#39;609180.96&#39;, &#39;7376742.28&#39;]
[&#39;1448&#39;, &#39;5309&#39;, &#39;605400.48&#39;, &#39;7370005.55&#39;]</code></pre>
<pre class="python"><code>#Convert the python &quot;list&quot; type to Pandas dataframe
df=pd.DataFrame(velf)
#Set the names of the columns
df.columns=[&#39;LINE&#39;,&#39;xline3d&#39;,&#39;binx&#39;,&#39;biny&#39;,&#39;inline3d&#39;,&#39;t&#39;,&#39;vt&#39;]

df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
LINE
</th>
<th>
xline3d
</th>
<th>
binx
</th>
<th>
biny
</th>
<th>
inline3d
</th>
<th>
t
</th>
<th>
vt
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1000
</td>
<td>
5080
</td>
<td>
598435.0
</td>
<td>
7380479.0
</td>
<td>
1000
</td>
<td>
0
</td>
<td>
3200
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1000
</td>
<td>
5080
</td>
<td>
598435.0
</td>
<td>
7380479.0
</td>
<td>
1000
</td>
<td>
295
</td>
<td>
3300
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1000
</td>
<td>
5080
</td>
<td>
598435.0
</td>
<td>
7380479.0
</td>
<td>
1000
</td>
<td>
598
</td>
<td>
4137
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1000
</td>
<td>
5080
</td>
<td>
598435.0
</td>
<td>
7380479.0
</td>
<td>
1000
</td>
<td>
738
</td>
<td>
4537
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1000
</td>
<td>
5080
</td>
<td>
598435.0
</td>
<td>
7380479.0
</td>
<td>
1000
</td>
<td>
1152
</td>
<td>
4500
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
3082
</th>
<td>
1440
</td>
<td>
5280
</td>
<td>
605580.0
</td>
<td>
7370735.0
</td>
<td>
1440
</td>
<td>
2216
</td>
<td>
5259
</td>
</tr>
<tr>
<th>
3083
</th>
<td>
1440
</td>
<td>
5280
</td>
<td>
605580.0
</td>
<td>
7370735.0
</td>
<td>
1440
</td>
<td>
2861
</td>
<td>
5791
</td>
</tr>
<tr>
<th>
3084
</th>
<td>
1440
</td>
<td>
5280
</td>
<td>
605580.0
</td>
<td>
7370735.0
</td>
<td>
1440
</td>
<td>
3526
</td>
<td>
6294
</td>
</tr>
<tr>
<th>
3085
</th>
<td>
1440
</td>
<td>
5280
</td>
<td>
605580.0
</td>
<td>
7370735.0
</td>
<td>
1440
</td>
<td>
4697
</td>
<td>
7077
</td>
</tr>
<tr>
<th>
3086
</th>
<td>
1440
</td>
<td>
5280
</td>
<td>
605580.0
</td>
<td>
7370735.0
</td>
<td>
1440
</td>
<td>
5988
</td>
<td>
7748
</td>
</tr>
</tbody>
</table>
<p>
3087 rows × 7 columns
</p>
</div>
<pre class="python"><code>#Plot the target area
plt.scatter(df.binx,df.biny,c=df.xline3d)
plt.colorbar()
plt.show()

#Plot it in 3d just becasue we can
fig = plt.figure()
ax = fig.add_subplot(projection=&#39;3d&#39;)
ax.scatter(df.binx,df.biny,df.inline3d,c=df.xline3d)
ax.view_init(30, 70)
plt.show()</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_77_0.png" alt="" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_77_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#Now, make some plots...
#One way to do this, is to
#make a &#39;group&#39; for each unique seismic line
groups=df.groupby([&#39;LINE&#39;,&#39;xline3d&#39;,&#39;inline3d&#39;])</code></pre>
<pre class="python"><code>#Make plots by certain groupings

#Add a value to spread out the data nicely
i=0
for name,grp in groups:
    
    if name[2]==1280:
        print(name)
        plt.plot(grp.vt+i,-grp.t)
        i+=500</code></pre>
<pre><code>(1280, 5040, 1280)
(1280, 5060, 1280)
(1280, 5080, 1280)
(1280, 5100, 1280)
(1280, 5120, 1280)
(1280, 5140, 1280)
(1280, 5160, 1280)
(1280, 5180, 1280)
(1280, 5200, 1280)
(1280, 5220, 1280)
(1280, 5240, 1280)
(1280, 5260, 1280)
(1280, 5280, 1280)</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_79_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>from scipy.interpolate import interp1d
import numpy as np</code></pre>
<pre class="python"><code>#Normal plots
%matplotlib inline

#Fancy intereactive plots
#%matplotlib notebook</code></pre>
<pre class="python"><code>fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot(projection=&#39;3d&#39;)
for name,grp in groups:

    ##Plot all the data
    ax.plot(grp.binx+grp.vt,grp.biny,grp.t,&#39;b-&#39;)
    
    ##Plot all the data with colors
#     colors=cm.seismic(grp.vt/grp.vt.max())
#     ax.scatter(grp.binx+grp.vt,grp.biny,grp.t,c=grp.vt/grp.vt.max(),cmap=cm.seismic)
    
    #Interpolate the data and plot with colors
#     x = grp.t
#     y = grp.vt
#     f = interp1d(x, y, kind=&#39;linear&#39;)

#     num=50
#     xnew = np.linspace(0,max(x),num)
#     ynew = f(xnew)
#     binx = np.linspace(min(grp.binx),max(grp.binx),num)
#     biny = np.linspace(min(grp.biny),max(grp.biny),num)
#     colours = cm.seismic(ynew/grp.vt.max())

#     ax.scatter(binx+xnew,biny,xnew,c=colours)
    
plt.show()</code></pre>
<div class="figure">
<img src="01b-dataframes_files/01b-dataframes_82_0.png" alt="" />
<p class="caption">png</p>
</div>
</div>
</div>
<div id="api-application-programming-interface-calls" class="section level1">
<h1>API (application programming interface) calls</h1>
<p>For example: <a href="https://github.com/geological-survey-of-queensland/open-data-api" class="uri">https://github.com/geological-survey-of-queensland/open-data-api</a></p>
<p>This is a general API, and can be interfaced with many types of software. Python can do it too! You can write an entire application around APIs provided by developers.</p>
<pre class="python"><code>import requests
import json

# set the API &#39;endpoint&#39;
api = &#39;https://geoscience.data.qld.gov.au/api/action/&#39;

# construct our query using the rules set out in the documentation
query = api + &#39;package_search?q=quamby&#39;

# make the get request and store it in the response object
response = requests.get(query)

# view the payload as JSON
json_response = response.json()
</code></pre>
<pre class="python"><code>#What kind of data does the server return us?
type(json_response)
#json_response</code></pre>
<pre><code>dict</code></pre>
<pre class="python"><code>#Dig into the returned data and just pull out certain items we want
jr=json_response[&#39;result&#39;][&#39;results&#39;][1][&#39;GeoJSONextent&#39;]</code></pre>
<pre class="python"><code>#Clean up the returned values
mysplit=jr.split(&#39;:&#39;)[-1].split(&#39;}&#39;)[0]</code></pre>
<pre class="python"><code>#And convert them into something more reasonable.
import ast
xy=ast.literal_eval(mysplit)[0]</code></pre>
<pre class="python"><code>#Now you have the data you need in a useful format you can do whatever you like!
xy</code></pre>
<pre><code>[[[139.901170853999, -20.4651656329993],
  [139.884504220999, -20.4651658769991],
  [139.867837588999, -20.4651661309991],
  [139.851170957999, -20.4651663759992],
  [139.834504324999, -20.4651666519991],
  [139.834504414999, -20.4484997749993],
  [139.834504493999, -20.4318328969991],
  [139.834504583999, -20.4151660189993],
  [139.834504673999, -20.3984991289993],
  [139.834504817999, -20.3818322399992],
  [139.834504974999, -20.3651653509993],
  [139.834505130999, -20.3484984619992],
  [139.834505276999, -20.3318315619991],
  [139.817838621999, -20.3318318499993],
  [139.801171968999, -20.3318321489993],
  [139.784505313999, -20.3318324379992],
  [139.784505458999, -20.3151655479993],
  [139.784505604999, -20.2984986709991],
  [139.801172269999, -20.2984983719992],
  [139.817838923999, -20.2984980729993],
  [139.834505589999, -20.2984977849992],
  [139.851172242999, -20.2984974849992],
  [139.867838875999, -20.2984972409993],
  [139.884505496999, -20.2984970089993],
  [139.901172127999, -20.2984967649993],
  [139.901171993999, -20.3151636539992],
  [139.901171860999, -20.3318305429993],
  [139.901171725999, -20.3484974319992],
  [139.901171591999, -20.3651643209993],
  [139.901171445999, -20.3818312099993],
  [139.901171311999, -20.3984980879992],
  [139.901171199999, -20.4151649779992],
  [139.901171087999, -20.4318318659992],
  [139.901170965999, -20.4484987439991],
  [139.901170853999, -20.4651656329993]]]</code></pre>
<div id="key-points" class="section level3 keypoints">
<h3>Key points</h3>
<ul>
<li>Obspy can be used to work with segy seismic data</li>
<li>Lasio can be used to work with well log data</li>
<li>Pyshp can be used to work with Shapefiles</li>
<li>Pandas dataframes are the best format for working with tabular data of mixed numeric/character types</li>
<li>Numpy arrays are faster when working with purely numeric data</li>
<li>Make your own “data-reader” with native Python</li>
<li>API calls can be made to build your own workflows and applications with Python</li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
