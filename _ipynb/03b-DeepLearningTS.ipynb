{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"03b. Deep Learning with Time Series data\"\n",
    "teaching: 30\n",
    "exercises: 30\n",
    "questions:\n",
    "- \"What is deep learning?\"\n",
    "- \"What is a GPU and why do I care?\"\n",
    "objectives:\n",
    "- \"Set up your own Python environemnt\"\n",
    "- \"Run a tensorflow job\"\n",
    "keypoints:\n",
    "- \"Roll-your-own software stack\"\n",
    "- \"Getting the correct balance of versions for software stacks is imperative\"\n",
    "- \"Not all GPUs are compliant with all software\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python offers many ways to make use of the compute capability in your GPU. A very common application is deep learning using the tensorflow and keras packages. In this example we are going to look at forecasting a timeseries using recurrent neural netowrks based on the history of the time series itself.\n",
    "\n",
    "### You can run through the steps on you local machine using the Jupyter notebook example\n",
    "\n",
    "We are looking at temperature data in Sydney for the last 150 years with daily measurements (Based on an example from \n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/). We want to predict what the future is going to look like. Note: the default values in the notebook restrict the length of dataset used in the analysis purely for time constraints. But feel free to adjust the numbers as you like. Using a GPU trained deep-learning framework to predict time series data. Specifically we are using a Long Short-Term Memory (LSTM) deep learning network\n",
    "\n",
    "The data is from the Australian Bureau of Meteorology (BOM) representing the [daily maximum temperatures for the last 150 years from the Sydney Observatory](http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=122&p_display_type=dailyDataFile&p_startYear=&p_c=&p_stn_num=066062)\n",
    "\n",
    "A problem might be, given the last few decades of temperature cycles, what will tomorrow's weather be? Let's try and predict the future!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#import all the libraries we need\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MaxAbsScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Load in the dataset\n",
    "filename='../data/sydney_temperature.csv'\n",
    "dataframe = pd.read_csv(filename, usecols=[5], engine='python')\n",
    "dataset = dataframe.dropna()\n",
    "dataset = dataset.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset to be betwenn 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "datasetScaled = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some stats about the data\n",
    "print(dataframe.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           Maximum temperature (Degree C)\n",
    "    count                    58316.000000\n",
    "    mean                        21.731120\n",
    "    std                          4.669517\n",
    "    min                          7.700000\n",
    "    25%                         18.200000\n",
    "    50%                         21.600000\n",
    "    75%                         24.900000\n",
    "    max                         45.800000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Look at some of the data set\n",
    "#This is the temperature throughout the year.\n",
    "#The summer and winter cycles are obvious\n",
    "#But there is a fair bit of variablity day-to-day\n",
    "plt.plot(dataset[50000:])\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Temperature (degrees Celsius)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    Text(0, 0.5, 'Temperature (degrees Celsius)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![png](../fig/fig-03DL-temperature.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "#Use the first 58000 days as training\n",
    "train=datasetScaled[0:58000,:]\n",
    "#Use from 55000 to 58316 as testing set, \n",
    "#that means we will test on 3000 days we know the answer for, \n",
    "#leaving 316 that the algorithm has never seen!\n",
    "test=datasetScaled[55000:,:]\n",
    "\n",
    "print(\"Traing set is: \", train.shape)\n",
    "print(\"Test set is: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Traing set is:  (58000, 1)\n",
    "    Test set is:  (3316, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# previous time steps to use as input variables to predict the next time period\n",
    "look_back = 30 \n",
    "\n",
    "# reshape into X=t and Y=t+look_back\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the LSTM network\n",
    "#The network has a visible layer with 1 input, \n",
    "#a hidden layer with 4 LSTM blocks or neurons, \n",
    "#and an output layer that makes a single value prediction. \n",
    "#The default sigmoid activation function is used for the LSTM blocks. \n",
    "#The network is trained for 4 epochs and a batch size of 1 is used.\n",
    "\n",
    "print(\"Running model...\")\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "print(\"Adding LSTM.\")\n",
    "model.add(tf.keras.layers.LSTM(4, input_shape=(look_back, 1)))\n",
    "\n",
    "print(\"Adding dense.\")\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "print(\"Compiling.\")\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Running model...\n",
    "    Adding LSTM.\n",
    "    Adding dense.\n",
    "    Compiling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model, this takes the longest time\n",
    "print(\"fitting...\")\n",
    "startT=time.time()\n",
    "model.fit(trainX, trainY, epochs=4, batch_size=30, verbose=1)\n",
    "endT=time.time()\n",
    "\n",
    "print(\"Time taken: \", endT-startT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    fitting...\n",
    "    Train on 57969 samples\n",
    "    Epoch 1/4\n",
    "    57969/57969 [==============================] - 41s 699us/sample - loss: 0.0066\n",
    "    Epoch 2/4\n",
    "    57969/57969 [==============================] - 43s 741us/sample - loss: 0.0062\n",
    "    Epoch 3/4\n",
    "    57969/57969 [==============================] - 43s 745us/sample - loss: 0.0060\n",
    "    Epoch 4/4\n",
    "    57969/57969 [==============================] - 42s 731us/sample - loss: 0.0059\n",
    "    Time taken:  169.15610480308533\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Save or load the model\n",
    "#model.save('kerasmodel.hdf5')\n",
    "#model = tf.keras.models.load_model('kerasmodel.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert and rescale predictions\n",
    "trainPredicti = scaler.inverse_transform(trainPredict)\n",
    "trainYi = scaler.inverse_transform([trainY])\n",
    "testPredicti = scaler.inverse_transform(testPredict)\n",
    "testYi = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainYi[0], trainPredicti[:,0]))\n",
    "print('Train Score: %.4f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testYi[0], testPredicti[:,0]))\n",
    "print('Test Score: %.4f RMSE' % (testScore))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train Score: 2.9142 RMSE\n",
    "    Test Score: 2.9692 RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT the result\n",
    "\n",
    "#Create a dataset that is the same size as the testing/training set \n",
    "dummyfull=numpy.ones((datasetScaled.shape[0]-test.shape[0],1))*numpy.mean(testPredict)\n",
    "print(dummyfull.shape,testPredicti.shape,datasetScaled.shape)\n",
    "testvec = numpy.concatenate((dummyfull,testPredict))\n",
    "\n",
    "#Scale the data\n",
    "transformer = MaxAbsScaler().fit(train[:])\n",
    "testScale= transformer.transform(testvec)\n",
    "\n",
    "print(trainPredict.shape,testPredict.shape,testvec.shape)\n",
    "\n",
    "train=datasetScaled[50000:58000,:]\n",
    "train=datasetScaled[0:58000,:]\n",
    "#Use from 50000 to 58316 as testing set, \n",
    "#that means we will test on 8000 days we know the answer for, \n",
    "#leaving 316 that the algorithm has never seen!\n",
    "test=datasetScaled[55000:,:]\n",
    "\n",
    "plt.plot(datasetScaled[:])\n",
    "plt.plot(train[:],'r')\n",
    "plt.plot(testScale[:],'k')\n",
    "\n",
    "plt.legend([\"All Data\",\"Training Data\",\"Predicted data\"])\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Scaled Temperature\")\n",
    "plt.xlim([54000,58500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (55000, 1) (3285, 1) (58316, 1)\n",
    "    (57969, 1) (3285, 1) (58285, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](../fig/fig-03DL-prediction.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the environment you are working in, you will have to use a different combo of python/cuda/tensorflow/keras versions and GPU hardware. Check compatability for [NVIDIA Drivers/CUDA](https://docs.nvidia.com/deploy/cuda-compatibility/index.html), [CUDA/Python/Tensorflow](https://www.tensorflow.org/install/source#tested_build_configurations).\n",
    "\n",
    "In Deep Learning, *training* the model can take a seriously long time, so we often only want to do this once and then tweak our model. In which case we can do that by saving out our data as as a *\\*.hdf5* file. \n",
    "\n",
    "# Exercise\n",
    "Pick another site at [http://www.bom.gov.au/climate/data/](http://www.bom.gov.au/climate/data/) and re-run the analysis. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md,ipynb",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
